{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi Dataset Generator for Grammar Error Correction\n",
    "\n",
    "This notebook:\n",
    "1. Fetches 10,000 Hindi sentences from Hugging Face IndicCorpV2 dataset\n",
    "2. Creates 5,000 identity pairs (input = output)\n",
    "3. Creates 5,000 corrupted pairs with word/character level errors\n",
    "4. Saves the generated dataset and combines with existing test.csv if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    'requests',\n",
    "    'pandas', \n",
    "    'tqdm',\n",
    "    'datasets'  # Added datasets library for Hugging Face datasets\n",
    "]\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import random\n",
    "    import re\n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Try to import datasets library\n",
    "    try:\n",
    "        import datasets\n",
    "        print(\"Datasets library is available\")\n",
    "    except ImportError:\n",
    "        print(\"Installing datasets library...\")\n",
    "        install_package('datasets')\n",
    "        import datasets\n",
    "        \n",
    "except ImportError as e:\n",
    "    missing_package = str(e).split(\"'\")[1]\n",
    "    print(f\"Installing {missing_package}...\")\n",
    "    install_package(missing_package)\n",
    "    \n",
    "    # Re-import after installation\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import random\n",
    "    import re\n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    try:\n",
    "        import datasets\n",
    "    except ImportError:\n",
    "        print(\"Installing datasets library...\")\n",
    "        install_package('datasets')\n",
    "        import datasets\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TARGET_SENTENCES = 10000\n",
    "IDENTITY_PAIRS = 5000\n",
    "CORRUPTED_PAIRS = 5000\n",
    "\n",
    "# API endpoint for Hugging Face datasets-server\n",
    "BASE_URL = \"https://datasets-server.huggingface.co/rows\"\n",
    "DATASET = \"ai4bharat/IndicCorpV2\"\n",
    "CONFIG = \"indiccorp_v2\"\n",
    "SPLIT = \"hin_Deva\"\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_hindi_sentences(target_count=10000, batch_size=100):\n",
    "    \"\"\"\n",
    "    Fetch Hindi sentences using datasets library directly\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        print(f\"Fetching {target_count} Hindi sentences using datasets library...\")\n",
    "        \n",
    "        # Load the IndicCorpV2 dataset directly\n",
    "        print(\"Loading IndicCorpV2 dataset...\")\n",
    "        dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=\"hin_Deva\", streaming=True)\n",
    "        \n",
    "        sentences = []\n",
    "        \n",
    "        with tqdm(total=target_count, desc=\"Fetching sentences\") as pbar:\n",
    "            for example in dataset:\n",
    "                if 'text' in example:\n",
    "                    text = example['text'].strip()\n",
    "                    if text and len(text) > 10:  # Filter out very short texts\n",
    "                        sentences.append(text)\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                        if len(sentences) >= target_count:\n",
    "                            break\n",
    "        \n",
    "        print(f\"Successfully fetched {len(sentences)} sentences\")\n",
    "        return sentences\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise Exception(f\"Failed to load Hindi sentences from IndicCorpV2 dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_word_level_errors(sentence, error_rate=0.3):\n",
    "    \"\"\"\n",
    "    Introduce word-level errors: deletion, insertion, swapping\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "    \n",
    "    corrupted_words = words.copy()\n",
    "    \n",
    "    # Determine number of errors to introduce\n",
    "    num_errors = max(1, int(len(words) * error_rate))\n",
    "    \n",
    "    for _ in range(num_errors):\n",
    "        error_type = random.choice(['delete', 'insert', 'swap'])\n",
    "        \n",
    "        if error_type == 'delete' and len(corrupted_words) > 1:\n",
    "            # Delete a random word\n",
    "            idx = random.randint(0, len(corrupted_words) - 1)\n",
    "            corrupted_words.pop(idx)\n",
    "            \n",
    "        elif error_type == 'insert':\n",
    "            # Insert a duplicate word at random position\n",
    "            if corrupted_words:\n",
    "                word_to_duplicate = random.choice(corrupted_words)\n",
    "                idx = random.randint(0, len(corrupted_words))\n",
    "                corrupted_words.insert(idx, word_to_duplicate)\n",
    "                \n",
    "        elif error_type == 'swap' and len(corrupted_words) > 1:\n",
    "            # Swap two adjacent words\n",
    "            idx = random.randint(0, len(corrupted_words) - 2)\n",
    "            corrupted_words[idx], corrupted_words[idx + 1] = corrupted_words[idx + 1], corrupted_words[idx]\n",
    "    \n",
    "    return ' '.join(corrupted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_character_level_errors(sentence, error_rate=0.1):\n",
    "    \"\"\"\n",
    "    Introduce character-level errors: deletion, insertion, swapping\n",
    "    \"\"\"\n",
    "    if len(sentence) < 2:\n",
    "        return sentence\n",
    "    \n",
    "    chars = list(sentence)\n",
    "    \n",
    "    # Determine number of errors to introduce\n",
    "    num_errors = max(1, int(len(chars) * error_rate))\n",
    "    \n",
    "    for _ in range(num_errors):\n",
    "        if len(chars) < 2:\n",
    "            break\n",
    "            \n",
    "        error_type = random.choice(['delete', 'insert', 'swap'])\n",
    "        \n",
    "        if error_type == 'delete' and len(chars) > 1:\n",
    "            # Delete a random character (but not spaces at word boundaries)\n",
    "            non_space_indices = [i for i, c in enumerate(chars) if c != ' ']\n",
    "            if non_space_indices:\n",
    "                idx = random.choice(non_space_indices)\n",
    "                chars.pop(idx)\n",
    "                \n",
    "        elif error_type == 'insert':\n",
    "            # Insert a duplicate character at random position\n",
    "            if chars:\n",
    "                char_to_duplicate = random.choice([c for c in chars if c != ' '])\n",
    "                if char_to_duplicate:\n",
    "                    idx = random.randint(0, len(chars))\n",
    "                    chars.insert(idx, char_to_duplicate)\n",
    "                \n",
    "        elif error_type == 'swap' and len(chars) > 1:\n",
    "            # Swap two adjacent characters\n",
    "            idx = random.randint(0, len(chars) - 2)\n",
    "            chars[idx], chars[idx + 1] = chars[idx + 1], chars[idx]\n",
    "    \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrupted_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Apply both word-level and character-level errors to a sentence\n",
    "    \"\"\"\n",
    "    # Randomly choose to apply word-level or character-level errors (or both)\n",
    "    error_types = random.choice([\n",
    "        ['word'],\n",
    "        ['character'],\n",
    "        ['word', 'character']\n",
    "    ])\n",
    "    \n",
    "    corrupted = sentence\n",
    "    \n",
    "    if 'word' in error_types:\n",
    "        corrupted = introduce_word_level_errors(corrupted)\n",
    "    \n",
    "    if 'character' in error_types:\n",
    "        corrupted = introduce_character_level_errors(corrupted)\n",
    "    \n",
    "    return corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 10000 Hindi sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching sentences:   0%|          | 0/10000 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data at offset 0: 501 Server Error: Not Implemented for url: https://datasets-server.huggingface.co/rows?dataset=ai4bharat%2FIndicCorpV2&config=indiccorp_v2&split=hin_Deva&offset=0&limit=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "501 Server Error: Not Implemented for url: https://datasets-server.huggingface.co/rows?dataset=ai4bharat%2FIndicCorpV2&config=indiccorp_v2&split=hin_Deva&offset=0&limit=100",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fetch Hindi sentences\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m hindi_sentences = \u001b[43mfetch_hindi_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTARGET_SENTENCES\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mfetch_hindi_sentences\u001b[39m\u001b[34m(target_count, batch_size)\u001b[39m\n\u001b[32m     18\u001b[39m params = {\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m: DATASET,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: CONFIG,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlimit\u001b[39m\u001b[33m\"\u001b[39m: current_batch_size\n\u001b[32m     24\u001b[39m }\n\u001b[32m     26\u001b[39m response = requests.get(BASE_URL, params=params, timeout=\u001b[32m30\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m data = response.json()\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mrows\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\CODING\\IndicGEC2025\\.venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 501 Server Error: Not Implemented for url: https://datasets-server.huggingface.co/rows?dataset=ai4bharat%2FIndicCorpV2&config=indiccorp_v2&split=hin_Deva&offset=0&limit=100"
     ]
    }
   ],
   "source": [
    "# Fetch Hindi sentences\n",
    "hindi_sentences = fetch_hindi_sentences(TARGET_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data pairs\n",
    "print(\"Creating training data pairs...\")\n",
    "\n",
    "# Shuffle sentences for random selection\n",
    "random.shuffle(hindi_sentences)\n",
    "\n",
    "data_pairs = []\n",
    "\n",
    "# Create identity pairs (first 5000 sentences)\n",
    "print(f\"Creating {IDENTITY_PAIRS} identity pairs...\")\n",
    "for i in tqdm(range(IDENTITY_PAIRS), desc=\"Identity pairs\"):\n",
    "    sentence = hindi_sentences[i]\n",
    "    data_pairs.append({\n",
    "        'input': sentence,\n",
    "        'output': sentence,\n",
    "        'type': 'identity'\n",
    "    })\n",
    "\n",
    "# Create corrupted pairs (next 5000 sentences)\n",
    "print(f\"Creating {CORRUPTED_PAIRS} corrupted pairs...\")\n",
    "for i in tqdm(range(IDENTITY_PAIRS, IDENTITY_PAIRS + CORRUPTED_PAIRS), desc=\"Corrupted pairs\"):\n",
    "    original_sentence = hindi_sentences[i]\n",
    "    corrupted_sentence = create_corrupted_sentence(original_sentence)\n",
    "    \n",
    "    data_pairs.append({\n",
    "        'input': corrupted_sentence,\n",
    "        'output': original_sentence,\n",
    "        'type': 'corrupted'\n",
    "    })\n",
    "\n",
    "print(f\"Total pairs created: {len(data_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(data_pairs)\n",
    "\n",
    "# Display some examples\n",
    "print(\"\\nSample identity pairs:\")\n",
    "identity_samples = df[df['type'] == 'identity'].head(3)\n",
    "for idx, row in identity_samples.iterrows():\n",
    "    print(f\"Input:  {row['input'][:100]}...\")\n",
    "    print(f\"Output: {row['output'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nSample corrupted pairs:\")\n",
    "corrupted_samples = df[df['type'] == 'corrupted'].head(3)\n",
    "for idx, row in corrupted_samples.iterrows():\n",
    "    print(f\"Input:  {row['input'][:100]}...\")\n",
    "    print(f\"Output: {row['output'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Remove the 'type' column as it's just for our reference\n",
    "df_final = df[['input', 'output']]\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df_final.shape}\")\n",
    "print(f\"Columns: {list(df_final.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated dataset\n",
    "generated_filename = 'generated_hindi_dataset.csv'\n",
    "df_final.to_csv(generated_filename, index=False, encoding='utf-8')\n",
    "print(f\"Generated dataset saved as: {generated_filename}\")\n",
    "\n",
    "# Check if test.csv exists and combine if it does\n",
    "import os\n",
    "\n",
    "test_csv_path = 'test.csv'\n",
    "combined_filename = 'combined_test_dataset.csv'\n",
    "\n",
    "if os.path.exists(test_csv_path):\n",
    "    print(f\"\\nFound existing {test_csv_path}. Combining datasets...\")\n",
    "    \n",
    "    # Read existing test.csv\n",
    "    existing_df = pd.read_csv(test_csv_path, encoding='utf-8')\n",
    "    print(f\"Existing dataset shape: {existing_df.shape}\")\n",
    "    print(f\"Existing columns: {list(existing_df.columns)}\")\n",
    "    \n",
    "    # Ensure column names match\n",
    "    if list(existing_df.columns) != list(df_final.columns):\n",
    "        print(f\"Warning: Column names don't match!\")\n",
    "        print(f\"Existing: {list(existing_df.columns)}\")\n",
    "        print(f\"Generated: {list(df_final.columns)}\")\n",
    "        \n",
    "        # Try to align columns if possible\n",
    "        if len(existing_df.columns) == 2 and len(df_final.columns) == 2:\n",
    "            print(\"Assuming both datasets have input/output columns in the same order...\")\n",
    "            df_final.columns = existing_df.columns\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([existing_df, df_final], ignore_index=True)\n",
    "    \n",
    "    # Save combined dataset\n",
    "    combined_df.to_csv(combined_filename, index=False, encoding='utf-8')\n",
    "    print(f\"Combined dataset saved as: {combined_filename}\")\n",
    "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nNo existing {test_csv_path} found.\")\n",
    "    print(f\"You can rename {generated_filename} to {test_csv_path} if needed.\")\n",
    "\n",
    "print(\"\\nDataset generation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Hindi sentences fetched: {len(hindi_sentences)}\")\n",
    "print(f\"Identity pairs created: {IDENTITY_PAIRS}\")\n",
    "print(f\"Corrupted pairs created: {CORRUPTED_PAIRS}\")\n",
    "print(f\"Total training pairs: {len(df_final)}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"- {generated_filename}\")\n",
    "if os.path.exists(test_csv_path):\n",
    "    print(f\"- {combined_filename}\")\n",
    "print(\"\\nDataset is ready for training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IndicGEC2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

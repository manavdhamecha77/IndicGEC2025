{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hindi GEC with mT5-small ‚Äî FIXED Training and Inference Notebook\n",
        "\n",
        "üîß **CRITICAL FIXES APPLIED:**\n",
        "- ‚úÖ Changed task prefix from `'correct Hindi: '` to `'grammar correction: '`\n",
        "- ‚úÖ Added data augmentation with identity pairs from dev set\n",
        "- ‚úÖ Increased training epochs to 8 for better learning\n",
        "- ‚úÖ Proper text-to-text format for MT5\n",
        "\n",
        "This notebook fixes the issues causing `<extra_id_0>` tokens and poor performance.\n",
        "\n",
        "Expected files in the same folder as this notebook:\n",
        "- `train.csv` (columns: `input`, `output` OR first two columns are input/output)\n",
        "- Optional: `dev.csv` (same format). If missing, the notebook will split a dev set from train.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Install dependencies (run once)\n",
        "If you haven't installed the required libraries, run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed, uncomment and run:\n",
        "# !pip install -U transformers datasets accelerate sentencepiece evaluate tqdm scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports, setup, and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.6.0+cu124\n",
            "Transformers: 4.56.2\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
            "GPU Memory (GB): 6.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model_name': 'google/mt5-small',\n",
              " 'max_input_length': 128,\n",
              " 'max_target_length': 128,\n",
              " 'device': 'cuda',\n",
              " 'train_file': 'train.csv',\n",
              " 'dev_file': 'dev.csv',\n",
              " 'test_size': 0.1,\n",
              " 'random_seed': 42,\n",
              " 'output_dir': './mt5-hindi-gec-model-fixed',\n",
              " 'num_train_epochs': 8,\n",
              " 'per_device_train_batch_size': 4,\n",
              " 'per_device_eval_batch_size': 8,\n",
              " 'gradient_accumulation_steps': 4,\n",
              " 'learning_rate': 0.0005,\n",
              " 'warmup_ratio': 0.1,\n",
              " 'weight_decay': 0.01,\n",
              " 'max_grad_norm': 1.0,\n",
              " 'fp16': False,\n",
              " 'gradient_checkpointing': True,\n",
              " 'optim': 'adafactor',\n",
              " 'evaluation_strategy': 'epoch',\n",
              " 'save_strategy': 'epoch',\n",
              " 'logging_steps': 50,\n",
              " 'save_total_limit': 2,\n",
              " 'load_best_model_at_end': True,\n",
              " 'metric_for_best_model': 'gleu',\n",
              " 'greater_is_better': True,\n",
              " 'early_stopping_patience': 4,\n",
              " 'generation_config': {'max_length': 128,\n",
              "  'num_beams': 4,\n",
              "  'early_stopping': True,\n",
              "  'repetition_penalty': 1.2,\n",
              "  'no_repeat_ngram_size': 3,\n",
              "  'length_penalty': 1.0,\n",
              "  'do_sample': False}}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import gc\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    MT5ForConditionalGeneration,\n",
        "    MT5Tokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from datasets import Dataset\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'Transformers: {transformers.__version__}')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    print('GPU Memory (GB):', round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2))\n",
        "\n",
        "# ==================== FIXED Configuration ====================\n",
        "CONFIG: Dict = {\n",
        "    # Model\n",
        "    'model_name': 'google/mt5-small',\n",
        "    'max_input_length': 128,\n",
        "    'max_target_length': 128,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Data\n",
        "    'train_file': 'train.csv',\n",
        "    'dev_file': 'dev.csv',\n",
        "    'test_size': 0.1,\n",
        "    'random_seed': SEED,\n",
        "\n",
        "    # Training - IMPROVED SETTINGS\n",
        "    'output_dir': './mt5-hindi-gec-model-fixed',\n",
        "    'num_train_epochs': 8,  # ‚úÖ Increased epochs\n",
        "    'per_device_train_batch_size': 4,\n",
        "    'per_device_eval_batch_size': 8,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'learning_rate': 5e-4,  # ‚úÖ Slightly higher learning rate\n",
        "    'warmup_ratio': 0.1,\n",
        "    'weight_decay': 0.01,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'fp16': False,\n",
        "    'gradient_checkpointing': True,\n",
        "    'optim': 'adafactor',\n",
        "\n",
        "    # Evaluation / saving\n",
        "    'evaluation_strategy': 'epoch',\n",
        "    'save_strategy': 'epoch',\n",
        "    'logging_steps': 50,\n",
        "    'save_total_limit': 2,\n",
        "    'load_best_model_at_end': True,\n",
        "    'metric_for_best_model': 'gleu',\n",
        "    'greater_is_better': True,\n",
        "    'early_stopping_patience': 4,  # ‚úÖ Increased patience\n",
        "\n",
        "    # Generation\n",
        "    'generation_config': {\n",
        "        'max_length': 128,\n",
        "        'num_beams': 4,\n",
        "        'early_stopping': True,\n",
        "        'repetition_penalty': 1.2,\n",
        "        'no_repeat_ngram_size': 3,\n",
        "        'length_penalty': 1.0,\n",
        "        'do_sample': False,\n",
        "    }\n",
        "}\n",
        "CONFIG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data loading and cleaning - WITH AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original training samples: 13751\n",
            "üîÑ Adding identity pairs from dev set...\n",
            "‚úÖ Added 107 identity pairs\n",
            "Final train samples: 13858 | Dev samples: 107\n",
            "Identical train pairs: 165\n",
            "Identical dev pairs: 24\n",
            "Sample corrections:\n",
            "1. Input:  ‡§ö‡§æ‡§Ø ‡§ï‡•Ä ‡§¶‡•Å‡§ï‡§æ‡§® ‡§∏‡•á ‡§≤‡•á‡§ï‡§∞ ‡§µ‡§æ‡§π‡§®‡•ã‡§Ç ‡§î‡§∞ ‡§¶‡§ø‡§µ‡§æ‡§∞‡•ã‡§Ç ‡§§‡§ï ‡§π‡§∞ ‡§ú‡§ó‡§π ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§™‡§® ‡§π‡•Ä ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§™‡§® ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á‡§§‡•á\n",
            "   Output: ‡§ö‡§æ‡§Ø ‡§ï‡•Ä ‡§¶‡•Å‡§ï‡§æ‡§® ‡§∏‡•á ‡§≤‡•á‡§ï‡§∞ ‡§µ‡§æ‡§π‡§®‡•ã‡§Ç ‡§î‡§∞ ‡§¶‡§ø‡§µ‡§æ‡§∞‡•ã‡§Ç ‡§§‡§ï ‡§π‡§∞ ‡§ú‡§ó‡§π ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§™‡§® ‡§π‡•Ä ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§™‡§® ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á‡§§‡•á\n",
            "2. Input:  ‡§Ø‡•á ‡§ï‡§π‡•Ä‡§Ç ‡§™‡•á ‡§®‡§ø‡§ó‡§æ‡§π‡•á‡§Ç , ‡§ï‡§π‡•Ä ‡§™‡•á ‡§®‡§ø‡§∂‡§æ‡§®‡§æ ‡§ï‡§æ ‡§∏‡§æ ‡§Ö‡§®‡•ç‡§¶‡§æ‡§ú ‡§π‡•à ‡•§\n",
            "   Output: ‡§Ø‡§π ‡§ï‡§π‡•Ä‡§Ç ‡§™‡•á ‡§®‡§ø‡§ó‡§æ‡§π‡•á‡§Ç , ‡§ï‡§π‡•Ä ‡§™‡•á ‡§®‡§ø‡§∂‡§æ‡§®‡§æ ‡§ï‡§æ ‡§∏‡§æ ‡§Ö‡§®‡•ç‡§¶‡§æ‡§ú ‡§π‡•à ‡•§\n",
            "3. Input:  ‡§Ü‡§ú ‡§π‡§Æ ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§™‡§® ‡§Ø‡•Å‡§ó ‡§ï‡•á ‡§∏‡•Ä‡§Æ‡§æ‡§®‡•ç‡§§ ‡§™‡§∞ ‡§Ü ‡§ñ‡§°‡§º‡•á ‡§π‡•Å‡§è ‡§π‡•à ‡•§\n",
            "   Output: ‡§Ü‡§ú ‡§π‡§Æ ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§™‡§® ‡§Ø‡•Å‡§ó ‡§ï‡•á ‡§∏‡•Ä‡§Æ‡§æ‡§®‡•ç‡§§ ‡§™‡§∞ ‡§Ü ‡§ñ‡§°‡§º‡•á ‡§π‡•Å‡§è ‡§π‡•à‡§Ç ‡•§\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(13858, 107)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = str(text).strip()\n",
        "    text = ' '.join(text.split())\n",
        "    text = ''.join(ch for ch in text if ord(ch) >= 32 or ch == '\\n')\n",
        "    return text\n",
        "\n",
        "def load_and_prepare_data(config: Dict):\n",
        "    train_path = Path(config['train_file'])\n",
        "    if not train_path.exists():\n",
        "        raise FileNotFoundError(f'Training file not found: {train_path}')\n",
        "\n",
        "    train_df = pd.read_csv(train_path, encoding='utf-8')\n",
        "    # Determine columns\n",
        "    if 'input' in train_df.columns and 'output' in train_df.columns:\n",
        "        input_col, output_col = 'input', 'output'\n",
        "    else:\n",
        "        input_col, output_col = train_df.columns[0], train_df.columns[1]\n",
        "\n",
        "    train_df = train_df[[input_col, output_col]].copy()\n",
        "    train_df.columns = ['input_text', 'output_text']\n",
        "    train_df['input_text'] = train_df['input_text'].apply(clean_text)\n",
        "    train_df['output_text'] = train_df['output_text'].apply(clean_text)\n",
        "    train_df = train_df[(train_df['input_text'] != '') & (train_df['output_text'] != '')]\n",
        "    train_df = train_df[(train_df['input_text'].str.len().between(5, 200)) & (train_df['output_text'].str.len().between(5, 200))]\n",
        "\n",
        "    print(f'Original training samples: {len(train_df)}')\n",
        "\n",
        "    # ‚úÖ CRITICAL FIX: Add identity pairs from dev set for data augmentation\n",
        "    dev_path = Path(config['dev_file'])\n",
        "    if dev_path.exists():\n",
        "        dev_df = pd.read_csv(dev_path, encoding='utf-8')\n",
        "        dev_df = dev_df[[input_col, output_col]].copy()\n",
        "        dev_df.columns = ['input_text', 'output_text']\n",
        "        dev_df['input_text'] = dev_df['input_text'].apply(clean_text)\n",
        "        dev_df['output_text'] = dev_df['output_text'].apply(clean_text)\n",
        "        dev_df = dev_df[(dev_df['input_text'] != '') & (dev_df['output_text'] != '')]\n",
        "        \n",
        "        # üî• ADD IDENTITY PAIRS - This teaches the model when NOT to change things\n",
        "        print('üîÑ Adding identity pairs from dev set...')\n",
        "        identity_pairs = []\n",
        "        for _, row in dev_df.iterrows():\n",
        "            # Add correct sentence ‚Üí same correct sentence\n",
        "            identity_pairs.append({\n",
        "                'input_text': row['output_text'],  # Use target as input\n",
        "                'output_text': row['output_text']   # Same as output\n",
        "            })\n",
        "        \n",
        "        identity_df = pd.DataFrame(identity_pairs)\n",
        "        train_df = pd.concat([train_df, identity_df], ignore_index=True)\n",
        "        print(f'‚úÖ Added {len(identity_df)} identity pairs')\n",
        "    else:\n",
        "        # Split if no dev file\n",
        "        train_df, dev_df = train_test_split(train_df, test_size=config['test_size'], random_state=config['random_seed'])\n",
        "\n",
        "    print('Final train samples:', len(train_df), '| Dev samples:', len(dev_df))\n",
        "    print('Identical train pairs:', int((train_df['input_text'] == train_df['output_text']).sum()))\n",
        "    print('Identical dev pairs:', int((dev_df['input_text'] == dev_df['output_text']).sum()))\n",
        "\n",
        "    # Show few examples\n",
        "    print('Sample corrections:')\n",
        "    sample = train_df[train_df['input_text'] != train_df['output_text']].head(3)\n",
        "    for i, (_, row) in enumerate(sample.iterrows(), 1):\n",
        "        print(f\"{i}. Input:  {row['input_text'][:80]}\")\n",
        "        print(f\"   Output: {row['output_text'][:80]}\")\n",
        "    return train_df, dev_df\n",
        "\n",
        "train_df, dev_df = load_and_prepare_data(CONFIG)\n",
        "len(train_df), len(dev_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
            "The class this function is called from is 'MT5Tokenizer'.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 250100\n",
            "Model params: 300.2M\n",
            "GPU mem allocated (GB): 1.12\n"
          ]
        }
      ],
      "source": [
        "def load_model_and_tokenizer(config: Dict):\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    tokenizer = MT5Tokenizer.from_pretrained(config['model_name'])\n",
        "    model = MT5ForConditionalGeneration.from_pretrained(\n",
        "        config['model_name'],\n",
        "        torch_dtype=(torch.float16 if config['fp16'] else torch.float32),\n",
        "    )\n",
        "    if config.get('gradient_checkpointing', False):\n",
        "        model.gradient_checkpointing_enable()\n",
        "        model.config.use_cache = False\n",
        "    model = model.to(config['device'])\n",
        "    print('Vocab size:', len(tokenizer))\n",
        "    total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "    print(f'Model params: {total_params:.1f}M')\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU mem allocated (GB):', round(torch.cuda.memory_allocated() / 1024**3, 2))\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer(CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenization and dataset preparation - FIXED TASK PREFIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfd15f0c42524c7489b189e44f16a621",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train:   0%|          | 0/13858 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0dc034396c041c1950826d98ced9a39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing dev:   0%|          | 0/107 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized sizes: 13858 107\n"
          ]
        }
      ],
      "source": [
        "def create_tokenization_function(tokenizer, config: Dict):\n",
        "    def tokenize_function(examples):\n",
        "        # ‚úÖ CRITICAL FIX: Changed from 'correct Hindi:' to 'grammar correction:'\n",
        "        # This prevents the model from thinking it's an infilling task\n",
        "        inputs = ['grammar correction: ' + text for text in examples['input_text']]\n",
        "        targets = examples['output_text']\n",
        "        model_inputs = tokenizer(\n",
        "            inputs,\n",
        "            max_length=config['max_input_length'],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "        )\n",
        "        labels = tokenizer(\n",
        "            text_target=targets,\n",
        "            max_length=config['max_target_length'],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "        )\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        return model_inputs\n",
        "    return tokenize_function\n",
        "\n",
        "tokenize_function = create_tokenization_function(tokenizer, CONFIG)\n",
        "\n",
        "hf_train = Dataset.from_pandas(train_df)\n",
        "hf_dev = Dataset.from_pandas(dev_df)\n",
        "\n",
        "tokenized_train = hf_train.map(tokenize_function, batched=True, remove_columns=hf_train.column_names, desc='Tokenizing train')\n",
        "tokenized_dev = hf_dev.map(tokenize_function, batched=True, remove_columns=hf_dev.column_names, desc='Tokenizing dev')\n",
        "\n",
        "print('Tokenized sizes:', len(tokenized_train), len(tokenized_dev))\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,  # dynamic padding\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Metrics (GLEU proxy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds: EvalPrediction):\n",
        "    # Support both EvalPrediction and (predictions, labels) tuple\n",
        "    if isinstance(eval_preds, tuple):\n",
        "        predictions, labels = eval_preds\n",
        "    else:\n",
        "        predictions, labels = eval_preds.predictions, eval_preds.label_ids\n",
        "    # Unwrap predictions if generate() returns a tuple\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "\n",
        "    # Ensure predictions are token ids (handle logits or floats)\n",
        "    preds = np.array(predictions)\n",
        "    if preds.ndim == 3:  # logits -> ids\n",
        "        preds = preds.argmax(-1)\n",
        "    preds = preds.astype(np.int64, copy=False)\n",
        "    # Guard against invalid ids\n",
        "    vocab_size = len(tokenizer)\n",
        "    preds = np.where((preds >= 0) & (preds < vocab_size), preds, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 to decode labels\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds = [p.strip() for p in decoded_preds]\n",
        "    decoded_labels = [l.strip() for l in decoded_labels]\n",
        "\n",
        "    # Simple GLEU-like proxy using token F1\n",
        "    gleu_scores = []\n",
        "    for pred, ref in zip(decoded_preds, decoded_labels):\n",
        "        pt = set(pred.lower().split())\n",
        "        rt = set(ref.lower().split())\n",
        "        if not rt:\n",
        "            gleu_scores.append(0.0)\n",
        "            continue\n",
        "        overlap = pt & rt\n",
        "        precision = len(overlap) / len(pt) if pt else 0.0\n",
        "        recall = len(overlap) / len(rt)\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
        "        gleu_scores.append(f1)\n",
        "    gleu = float(np.mean(gleu_scores) * 100)\n",
        "\n",
        "    return {'gleu': gleu}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training - WITH IMPROVED SETTINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Effective batch size: 16\n",
            "üöÄ Starting training with FIXED settings...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6936' max='6936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6936/6936 2:26:21, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Gleu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.329500</td>\n",
              "      <td>1.217272</td>\n",
              "      <td>82.788994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.242700</td>\n",
              "      <td>1.000660</td>\n",
              "      <td>84.815983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.213100</td>\n",
              "      <td>0.891355</td>\n",
              "      <td>85.842329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.187900</td>\n",
              "      <td>0.817642</td>\n",
              "      <td>86.415717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.153800</td>\n",
              "      <td>0.883712</td>\n",
              "      <td>86.315371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.139500</td>\n",
              "      <td>0.811158</td>\n",
              "      <td>86.634854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.125700</td>\n",
              "      <td>0.848560</td>\n",
              "      <td>86.564907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.112200</td>\n",
              "      <td>0.829440</td>\n",
              "      <td>86.692448</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model saved to ./mt5-hindi-gec-model-fixed\n"
          ]
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=CONFIG['output_dir'],\n",
        "    num_train_epochs=CONFIG['num_train_epochs'],  # ‚úÖ Now 8 epochs\n",
        "    per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
        "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "    learning_rate=CONFIG['learning_rate'],  # ‚úÖ Slightly higher LR\n",
        "    warmup_ratio=CONFIG['warmup_ratio'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    max_grad_norm=CONFIG['max_grad_norm'],\n",
        "    fp16=CONFIG['fp16'],\n",
        "    optim=CONFIG['optim'],\n",
        "    eval_strategy=CONFIG['evaluation_strategy'],\n",
        "    save_strategy=CONFIG['save_strategy'],\n",
        "    logging_steps=CONFIG['logging_steps'],\n",
        "    save_total_limit=CONFIG['save_total_limit'],\n",
        "    load_best_model_at_end=CONFIG['load_best_model_at_end'],\n",
        "    metric_for_best_model=CONFIG['metric_for_best_model'],\n",
        "    greater_is_better=CONFIG['greater_is_better'],\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=CONFIG['generation_config']['max_length'],\n",
        "    generation_num_beams=CONFIG['generation_config']['num_beams'],\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=True,\n",
        "    report_to='none',\n",
        "    push_to_hub=False,\n",
        "    seed=CONFIG['random_seed'],\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_dev,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG['early_stopping_patience'])],\n",
        ")\n",
        "\n",
        "print('Effective batch size:', training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
        "print('üöÄ Starting training with FIXED settings...')\n",
        "_ = trainer.train()\n",
        "\n",
        "# Save final model and config\n",
        "trainer.save_model(CONFIG['output_dir'])\n",
        "tokenizer.save_pretrained(CONFIG['output_dir'])\n",
        "with open(os.path.join(CONFIG['output_dir'], 'training_config.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(CONFIG, f, indent=2, ensure_ascii=False)\n",
        "print('‚úÖ Model saved to', CONFIG['output_dir'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Inference ‚Äî generate predictions and save CSV - FIXED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from ./mt5-hindi-gec-model-fixed ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90a4bc8d7f754d1eb830bb39d134046f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to predictions_fixed.csv\n",
            "                                      Input sentence  \\\n",
            "0  ‡§ï‡§π‡§§‡•á ‡§π‡•à '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§∂‡•á‡§∞‡§®‡•Ä ‡§ï‡•ã ‡§µ‡•ã ‡§¶‡•Å‡§ß ‡§π‡•à ‡§ú‡§ø‡§∏‡§®‡•á ‡§ú‡§ø‡§§‡§®‡§æ...   \n",
            "1  ‡§Ü‡§ú-‡§ï‡§≤ ‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§¨‡§æ‡§§ ‡§Ø‡§π‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§™‡•á ‡§∞‡§æ‡§ú‡§æ ‡§∏‡•á...   \n",
            "2    ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§Ü‡§ú ‡§ï‡•á ‡§∏‡§Æ‡§Ø ‡§ï‡•Ä ‡§∏‡§ö‡•ç‡§ö‡§æ‡§à ‡§¨‡§® ‡§ö‡•Å‡§ï‡•Ä ‡§π‡•à‡•§   \n",
            "3  ‡§Ü‡§ú ‡§™‡•Ç‡§∞‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§∏‡•á ‡§ú‡•Ç‡§ù...   \n",
            "4  ‡§∏‡§¨‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§π‡§Æ ‡§ú‡§æ‡§®‡§§‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§π‡•à ‡§ï...   \n",
            "\n",
            "                                     Output sentence  \n",
            "0  ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§∂‡•á‡§∞‡§®‡•Ä ‡§ï‡•ã ‡§µ‡•ã ‡§¶‡•Å‡§ß ‡§π‡•à ‡§ú‡§ø‡§∏‡§®‡•á ‡§ú‡§ø‡§§‡§®...  \n",
            "1  ‡§Ü‡§ú-‡§ï‡§≤ ‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§¨‡§æ‡§§ ‡§Ø‡§π‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§™‡•á ‡§∞‡§æ‡§ú‡§æ ‡§∏‡•á...  \n",
            "2    ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§Ü‡§ú ‡§ï‡•á ‡§∏‡§Æ‡§Ø ‡§ï‡•Ä ‡§∏‡§ö‡•ç‡§ö‡§æ‡§à ‡§¨‡§® ‡§ö‡•Å‡§ï‡•Ä ‡§π‡•à‡•§  \n",
            "3  ‡§Ü‡§ú ‡§™‡•Ç‡§∞‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§∏‡•á ‡§ú‡•Ç‡§ù...  \n",
            "4  ‡§∏‡§¨‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§π‡§Æ ‡§ú‡§æ‡§®‡§§‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§π‡•à ‡§ï...  \n"
          ]
        }
      ],
      "source": [
        "def generate_predictions(model_path: str, test_file: str, output_file: str = 'predictions.csv', batch_size: int = 16):\n",
        "    print(f'Loading model from {model_path} ...')\n",
        "    tok = MT5Tokenizer.from_pretrained(model_path)\n",
        "    mdl = MT5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
        "    mdl.eval()\n",
        "\n",
        "    df = pd.read_csv(test_file, encoding='utf-8')\n",
        "    if 'input' in df.columns:\n",
        "        input_col = 'input'\n",
        "    else:\n",
        "        input_col = df.columns[0]\n",
        "    df = df[[input_col]].copy()\n",
        "    df.columns = ['input_text']\n",
        "    df['input_text'] = df['input_text'].apply(clean_text)\n",
        "    df = df[df['input_text'] != '']\n",
        "\n",
        "    preds: List[str] = []\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(df), batch_size), desc='Generating'):\n",
        "            batch = df.iloc[i:i+batch_size]\n",
        "            # ‚úÖ CRITICAL FIX: Use same task prefix as training\n",
        "            inputs = ['grammar correction: ' + s for s in batch['input_text'].tolist()]\n",
        "            enc = tok(\n",
        "                inputs,\n",
        "                max_length=CONFIG['max_input_length'],\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                return_tensors='pt',\n",
        "            ).to(device)\n",
        "            outputs = mdl.generate(\n",
        "                **enc,\n",
        "                max_length=CONFIG['generation_config']['max_length'],\n",
        "                num_beams=CONFIG['generation_config']['num_beams'],\n",
        "                early_stopping=CONFIG['generation_config']['early_stopping'],\n",
        "                repetition_penalty=CONFIG['generation_config']['repetition_penalty'],\n",
        "                no_repeat_ngram_size=CONFIG['generation_config']['no_repeat_ngram_size'],\n",
        "            )\n",
        "            decoded = tok.batch_decode(outputs, skip_special_tokens=True)\n",
        "            preds.extend(decoded)\n",
        "            if torch.cuda.is_available() and i % 100 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "    out_df = pd.DataFrame({\n",
        "        'Input sentence': df['input_text'].tolist()[:len(preds)],\n",
        "        'Output sentence': preds,\n",
        "    })\n",
        "    out_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "    print('Predictions saved to', output_file)\n",
        "    print(out_df.head())\n",
        "    return out_df\n",
        "\n",
        "# Generate predictions\n",
        "_ = generate_predictions(CONFIG['output_dir'], CONFIG['dev_file'], 'predictions_fixed.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation - Check if fixes worked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî• RESULTS AFTER FIXES:\n",
            "Dev GLEU (proxy) ‚Äî FIXED model: 85.69\n",
            "Dev GLEU (proxy) ‚Äî identity baseline: 85.47\n",
            "Exact match rate: 13.08%\n",
            "\n",
            "üö® Sentences with <extra_id_0>: 0/107 (0.0%)\n",
            "\n",
            "üéâ SUCCESS! Model is now better than identity baseline!\n",
            "\n",
            "üìã Sample results:\n",
            "1.\n",
            "Input:    ‡§ï‡§π‡§§‡•á ‡§π‡•à '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§∂‡•á‡§∞‡§®‡•Ä ‡§ï‡•ã ‡§µ‡•ã ‡§¶‡•Å‡§ß ‡§π‡•à ‡§ú‡§ø‡§∏‡§®‡•á ‡§ú‡§ø‡§§‡§®‡§æ ‡§™‡§ø‡§Ø‡§æ ‡§â‡§§‡§®‡§æ ‡§π‡•Ä ‡§¶‡§π‡§æ‡§°‡§æ ‡§π‡•à'‡•§\n",
            "Pred:     ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§∂‡•á‡§∞‡§®‡•Ä ‡§ï‡•ã ‡§µ‡•ã ‡§¶‡•Å‡§ß ‡§π‡•à ‡§ú‡§ø‡§∏‡§®‡•á ‡§ú‡§ø‡§§‡§®‡§æ ‡§™‡§ø‡§Ø‡§æ ‡§â‡§§‡§®‡§æ ‡§π‡•Ä ‡§¶‡§π‡§æ‡§°‡§æ ‡§π‡•à'‡•§\n",
            "Ref:      ‡§ï‡§π‡§§‡•á ‡§π‡•à '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§∂‡•á‡§∞‡§®‡•Ä ‡§ï‡§æ ‡§µ‡•ã ‡§¶‡•Ç‡§ß ‡§π‡•à ‡§ú‡§ø‡§∏‡§®‡•á ‡§ú‡§ø‡§§‡§®‡§æ ‡§™‡§ø‡§Ø‡§æ ‡§â‡§§‡§®‡§æ ‡§π‡•Ä ‡§¶‡§π‡§æ‡§°‡§º‡§æ ‡§π‡•à'‡•§\n",
            "\n",
            "2.\n",
            "Input:    ‡§Ü‡§ú-‡§ï‡§≤ ‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§¨‡§æ‡§§ ‡§Ø‡§π‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§™‡•á ‡§∞‡§æ‡§ú‡§æ ‡§∏‡•á ‡§≤‡•á‡§ï‡§∞ ‡§∞‡§Ç‡§ï ‡§ï‡§æ ‡§≠‡•Ä ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§π‡•à‡•§\n",
            "Pred:     ‡§Ü‡§ú-‡§ï‡§≤ ‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§¨‡§æ‡§§ ‡§Ø‡§π‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§™‡•á ‡§∞‡§æ‡§ú‡§æ ‡§∏‡•á ‡§≤‡•á‡§ï‡§∞ ‡§∞‡§Ç‡§ï ‡§ï‡§æ ‡§≠‡•Ä ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§π‡•à‡•§\n",
            "Ref:      ‡§Ü‡§ú-‡§ï‡§≤ ‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§¨‡§æ‡§§ ‡§Ø‡§π‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§™‡§∞ ‡§∞‡§æ‡§ú‡§æ ‡§∏‡•á ‡§≤‡•á‡§ï‡§∞ ‡§∞‡§Ç‡§ï ‡§ï‡§æ ‡§≠‡•Ä ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§π‡•à‡•§\n",
            "\n",
            "3.\n",
            "Input:    ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§Ü‡§ú ‡§ï‡•á ‡§∏‡§Æ‡§Ø ‡§ï‡•Ä ‡§∏‡§ö‡•ç‡§ö‡§æ‡§à ‡§¨‡§® ‡§ö‡•Å‡§ï‡•Ä ‡§π‡•à‡•§\n",
            "Pred:     ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§Ü‡§ú ‡§ï‡•á ‡§∏‡§Æ‡§Ø ‡§ï‡•Ä ‡§∏‡§ö‡•ç‡§ö‡§æ‡§à ‡§¨‡§® ‡§ö‡•Å‡§ï‡•Ä ‡§π‡•à‡•§\n",
            "Ref:      ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§Ü‡§ú ‡§ï‡•á ‡§∏‡§Æ‡§Ø ‡§ï‡•Ä ‡§∏‡§ö‡•ç‡§ö‡§æ‡§à ‡§¨‡§® ‡§ö‡•Å‡§ï‡•Ä ‡§π‡•à‡•§\n",
            "\n",
            "4.\n",
            "Input:    ‡§Ü‡§ú ‡§™‡•Ç‡§∞‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§∏‡•á ‡§ú‡•Ç‡§ù ‡§∞‡§π‡§æ ‡§π‡•à‡•§\n",
            "Pred:     ‡§Ü‡§ú ‡§™‡•Ç‡§∞‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§∏‡•á ‡§ú‡•Ç‡§ù ‡§∞‡§π‡§æ ‡§π‡•à‡•§\n",
            "Ref:      ‡§Ü‡§ú ‡§™‡•Ç‡§∞‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§∏‡•á ‡§ú‡•Ç‡§ù ‡§∞‡§π‡§æ ‡§π‡•à‡•§\n",
            "\n",
            "5.\n",
            "Input:    ‡§∏‡§¨‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§π‡§Æ ‡§ú‡§æ‡§®‡§§‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§π‡•à ‡§ï‡•ç‡§Ø‡§æ?\n",
            "Pred:     ‡§∏‡§¨‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§π‡§Æ ‡§ú‡§æ‡§®‡§§‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§π‡•à ‡§ï‡•ç‡§Ø‡§æ?\n",
            "Ref:      ‡§∏‡§¨‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§π‡§Æ ‡§ú‡§æ‡§®‡§§‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ú‡§≤‡§µ‡§æ‡§Ø‡•Å ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§π‡•à ‡§ï‡•ç‡§Ø‡§æ?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def gleu_proxy(preds, refs):\n",
        "    scores = []\n",
        "    for pred, ref in zip(preds, refs):\n",
        "        pt, rt = set(str(pred).lower().split()), set(str(ref).lower().split())\n",
        "        if not rt:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        overlap = pt & rt\n",
        "        p = len(overlap) / len(pt) if pt else 0.0\n",
        "        r = len(overlap) / len(rt)\n",
        "        f1 = 2 * p * r / (p + r) if (p + r) else 0.0\n",
        "        scores.append(f1)\n",
        "    return float(np.mean(scores) * 100)\n",
        "\n",
        "# Load references\n",
        "dev_df_eval = pd.read_csv(CONFIG['dev_file'], encoding='utf-8')\n",
        "in_col = 'input' if 'input' in dev_df_eval.columns else dev_df_eval.columns[0]\n",
        "ref_col = 'output' if 'output' in dev_df_eval.columns else dev_df_eval.columns[1]\n",
        "refs = dev_df_eval[ref_col].astype(str).tolist()\n",
        "\n",
        "# Load model predictions\n",
        "pred_df = pd.read_csv('predictions_fixed.csv', encoding='utf-8')\n",
        "preds = pred_df['Output sentence'].astype(str).tolist()\n",
        "\n",
        "# Compute metrics\n",
        "gleu_model = gleu_proxy(preds, refs)\n",
        "gleu_identity = gleu_proxy(dev_df_eval[in_col].astype(str).tolist(), refs)\n",
        "exact_match = (pd.Series(preds) == pd.Series(refs)).mean() * 100.0\n",
        "\n",
        "print(\"üî• RESULTS AFTER FIXES:\")\n",
        "print(f\"Dev GLEU (proxy) ‚Äî FIXED model: {gleu_model:.2f}\")\n",
        "print(f\"Dev GLEU (proxy) ‚Äî identity baseline: {gleu_identity:.2f}\")\n",
        "print(f\"Exact match rate: {exact_match:.2f}%\")\n",
        "\n",
        "# Check for <extra_id_0> tokens\n",
        "extra_id_count = sum(1 for pred in preds if '<extra_id_0>' in str(pred))\n",
        "print(f\"\\nüö® Sentences with <extra_id_0>: {extra_id_count}/{len(preds)} ({extra_id_count/len(preds)*100:.1f}%)\")\n",
        "\n",
        "if gleu_model > gleu_identity:\n",
        "    print(\"\\nüéâ SUCCESS! Model is now better than identity baseline!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Model still below identity baseline. May need more training or different approach.\")\n",
        "\n",
        "# Show sample results\n",
        "print(\"\\nüìã Sample results:\")\n",
        "for i in range(min(5, len(pred_df))):\n",
        "    print(f\"{i+1}.\")\n",
        "    print(\"Input:   \", pred_df.iloc[i]['Input sentence'][:100])\n",
        "    print(\"Pred:    \", pred_df.iloc[i]['Output sentence'][:100])\n",
        "    print(\"Ref:     \", dev_df_eval.iloc[i][ref_col][:100])\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

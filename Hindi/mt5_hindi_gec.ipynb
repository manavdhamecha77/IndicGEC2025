{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hindi GEC with mT5-small — Training and Inference Notebook\n",
        "\n",
        "This notebook trains an mT5-small model for Hindi Grammar Error Correction (GEC) and generates predictions.\n",
        "\n",
        "Highlights:\n",
        "- Works on GPU (preferred) or CPU\n",
        "- Memory-friendly defaults for 6GB VRAM (e.g., RTX 3050)\n",
        "- Clean data pipeline and robust evaluation setup\n",
        "- Fixed compatibility issues (tokenization, evaluation_strategy, gradient checkpointing)\n",
        "\n",
        "Expected files in the same folder as this notebook:\n",
        "- `train.csv` (columns: `input`, `output` OR first two columns are input/output)\n",
        "- Optional: `dev.csv` (same format). If missing, the notebook will split a dev set from train.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Install dependencies (run once)\n",
        "If you haven't installed the required libraries, run the cell below. If you already have them, you can skip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed, uncomment and run:\n",
        "# !pip install -U transformers datasets accelerate sentencepiece evaluate tqdm scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports, setup, and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "10995d1e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.6.0+cu124\n",
            "Transformers: 4.56.2\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
            "GPU Memory (GB): 6.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model_name': 'google/mt5-small',\n",
              " 'max_input_length': 128,\n",
              " 'max_target_length': 128,\n",
              " 'device': 'cuda',\n",
              " 'train_file': 'train.csv',\n",
              " 'dev_file': 'dev.csv',\n",
              " 'test_size': 0.1,\n",
              " 'random_seed': 42,\n",
              " 'output_dir': './mt5-hindi-gec-model',\n",
              " 'num_train_epochs': 5,\n",
              " 'per_device_train_batch_size': 4,\n",
              " 'per_device_eval_batch_size': 8,\n",
              " 'gradient_accumulation_steps': 4,\n",
              " 'learning_rate': 0.0003,\n",
              " 'warmup_ratio': 0.1,\n",
              " 'weight_decay': 0.01,\n",
              " 'max_grad_norm': 1.0,\n",
              " 'fp16': False,\n",
              " 'gradient_checkpointing': True,\n",
              " 'optim': 'adafactor',\n",
              " 'evaluation_strategy': 'epoch',\n",
              " 'save_strategy': 'epoch',\n",
              " 'logging_steps': 50,\n",
              " 'save_total_limit': 2,\n",
              " 'load_best_model_at_end': True,\n",
              " 'metric_for_best_model': 'gleu',\n",
              " 'greater_is_better': True,\n",
              " 'early_stopping_patience': 3,\n",
              " 'generation_config': {'max_length': 128,\n",
              "  'num_beams': 4,\n",
              "  'early_stopping': True,\n",
              "  'repetition_penalty': 1.2,\n",
              "  'no_repeat_ngram_size': 3,\n",
              "  'length_penalty': 1.0,\n",
              "  'do_sample': False}}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import gc\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    MT5ForConditionalGeneration,\n",
        "    MT5Tokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from datasets import Dataset\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'Transformers: {transformers.__version__}')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    print('GPU Memory (GB):', round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2))\n",
        "\n",
        "# ==================== Configuration ====================\n",
        "CONFIG: Dict = {\n",
        "    # Model\n",
        "    'model_name': 'google/mt5-small',\n",
        "    'max_input_length': 128,\n",
        "    'max_target_length': 128,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Data\n",
        "    'train_file': 'train.csv',\n",
        "    'dev_file': 'dev.csv',\n",
        "    'test_size': 0.1,\n",
        "    'random_seed': SEED,\n",
        "\n",
        "    # Training\n",
        "    'output_dir': './mt5-hindi-gec-model',\n",
        "    'num_train_epochs': 5,\n",
        "    'per_device_train_batch_size': 4,\n",
        "    'per_device_eval_batch_size': 8,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'learning_rate': 3e-4,\n",
        "    'warmup_ratio': 0.1,\n",
        "    'weight_decay': 0.01,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'fp16': False,  # keep training numerics simple and stable with Adafactor\n",
        "    'gradient_checkpointing': True,\n",
        "    'optim': 'adafactor',\n",
        "\n",
        "    # Evaluation / saving\n",
        "    'evaluation_strategy': 'epoch',\n",
        "    'save_strategy': 'epoch',\n",
        "    'logging_steps': 50,\n",
        "    'save_total_limit': 2,\n",
        "    'load_best_model_at_end': True,\n",
        "    'metric_for_best_model': 'gleu',\n",
        "    'greater_is_better': True,\n",
        "    'early_stopping_patience': 3,\n",
        "\n",
        "    # Generation\n",
        "    'generation_config': {\n",
        "        'max_length': 128,\n",
        "        'num_beams': 4,\n",
        "        'early_stopping': True,\n",
        "        'repetition_penalty': 1.2,\n",
        "        'no_repeat_ngram_size': 3,\n",
        "        'length_penalty': 1.0,\n",
        "        'do_sample': False,\n",
        "    }\n",
        "}\n",
        "CONFIG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e583c6",
      "metadata": {},
      "source": [
        "## 2. Data loading and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2173730c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 5272 | Dev samples: 107\n",
            "Identical train pairs: 2444\n",
            "Identical dev pairs: 24\n",
            "Sample corrections:\n",
            "1. Input:  ये केवल किताबी ज्ञान अर्जन तक ही सिमित नहीं है।\n",
            "   Output: ये केवल किताबी ज्ञान अर्जन तक ही सीमित नहीं है।\n",
            "2. Input:  जैसे - व्यावहारिक शिक्षा, किताबी शिक्षा अथवा अध्यामित्क शिक्षा\n",
            "   Output: जैसे - व्यावहारिक शिक्षा, किताबी शिक्षा अथवा आध्यात्मिक शिक्षा ।\n",
            "3. Input:  वहाँ अचार्य होते थे शिक्षक के स्थान पे।\n",
            "   Output: वहाँ अचार्य होते थे शिक्षक के स्थान पर।\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(5272, 107)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = str(text).strip()\n",
        "    text = ' '.join(text.split())\n",
        "    text = ''.join(ch for ch in text if ord(ch) >= 32 or ch == '\\n')\n",
        "    return text\n",
        "\n",
        "def load_and_prepare_data(config: Dict):\n",
        "    train_path = Path(config['train_file'])\n",
        "    if not train_path.exists():\n",
        "        raise FileNotFoundError(f'Training file not found: {train_path}')\n",
        "\n",
        "    train_df = pd.read_csv(train_path, encoding='utf-8')\n",
        "    # Determine columns\n",
        "    if 'input' in train_df.columns and 'output' in train_df.columns:\n",
        "        input_col, output_col = 'input', 'output'\n",
        "    else:\n",
        "        input_col, output_col = train_df.columns[0], train_df.columns[1]\n",
        "\n",
        "    train_df = train_df[[input_col, output_col]].copy()\n",
        "    train_df.columns = ['input_text', 'output_text']\n",
        "    train_df['input_text'] = train_df['input_text'].apply(clean_text)\n",
        "    train_df['output_text'] = train_df['output_text'].apply(clean_text)\n",
        "    train_df = train_df[(train_df['input_text'] != '') & (train_df['output_text'] != '')]\n",
        "    train_df = train_df[(train_df['input_text'].str.len().between(5, 200)) & (train_df['output_text'].str.len().between(5, 200))]\n",
        "\n",
        "    dev_path = Path(config['dev_file'])\n",
        "    if dev_path.exists():\n",
        "        dev_df = pd.read_csv(dev_path, encoding='utf-8')\n",
        "        dev_df = dev_df[[input_col, output_col]].copy()\n",
        "        dev_df.columns = ['input_text', 'output_text']\n",
        "        dev_df['input_text'] = dev_df['input_text'].apply(clean_text)\n",
        "        dev_df['output_text'] = dev_df['output_text'].apply(clean_text)\n",
        "        dev_df = dev_df[(dev_df['input_text'] != '') & (dev_df['output_text'] != '')]\n",
        "    else:\n",
        "        train_df, dev_df = train_test_split(train_df, test_size=config['test_size'], random_state=config['random_seed'])\n",
        "\n",
        "    print('Train samples:', len(train_df), '| Dev samples:', len(dev_df))\n",
        "    print('Identical train pairs:', int((train_df['input_text'] == train_df['output_text']).sum()))\n",
        "    print('Identical dev pairs:', int((dev_df['input_text'] == dev_df['output_text']).sum()))\n",
        "\n",
        "    # Show few examples\n",
        "    print('Sample corrections:')\n",
        "    sample = train_df[train_df['input_text'] != train_df['output_text']].head(3)\n",
        "    for i, (_, row) in enumerate(sample.iterrows(), 1):\n",
        "        print(f\"{i}. Input:  {row['input_text'][:80]}\")\n",
        "        print(f\"   Output: {row['output_text'][:80]}\")\n",
        "    return train_df, dev_df\n",
        "\n",
        "train_df, dev_df = load_and_prepare_data(CONFIG)\n",
        "len(train_df), len(dev_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b323275",
      "metadata": {},
      "source": [
        "## 3. Load tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "525ed000",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
            "The class this function is called from is 'MT5Tokenizer'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 250100\n",
            "Model params: 300.2M\n",
            "GPU mem allocated (GB): 2.24\n"
          ]
        }
      ],
      "source": [
        "def load_model_and_tokenizer(config: Dict):\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    tokenizer = MT5Tokenizer.from_pretrained(config['model_name'])\n",
        "    model = MT5ForConditionalGeneration.from_pretrained(\n",
        "        config['model_name'],\n",
        "        torch_dtype=(torch.float16 if config['fp16'] else torch.float32),\n",
        "    )\n",
        "    if config.get('gradient_checkpointing', False):\n",
        "        model.gradient_checkpointing_enable()\n",
        "        # Important for gradient checkpointing\n",
        "        model.config.use_cache = False\n",
        "    model = model.to(config['device'])\n",
        "    print('Vocab size:', len(tokenizer))\n",
        "    total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "    print(f'Model params: {total_params:.1f}M')\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU mem allocated (GB):', round(torch.cuda.memory_allocated() / 1024**3, 2))\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer(CONFIG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa6e3c2d",
      "metadata": {},
      "source": [
        "## 4. Tokenization and dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2323c34f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac981966ce6040508d382871d571b9aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train:   0%|          | 0/5272 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0febbb9dfb614f7fa6f640fa5eee315c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing dev:   0%|          | 0/107 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized sizes: 5272 107\n"
          ]
        }
      ],
      "source": [
        "def create_tokenization_function(tokenizer, config: Dict):\n",
        "    def tokenize_function(examples):\n",
        "        inputs = ['correct Hindi: ' + text for text in examples['input_text']]\n",
        "        targets = examples['output_text']\n",
        "        model_inputs = tokenizer(\n",
        "            inputs,\n",
        "            max_length=config['max_input_length'],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "        )\n",
        "        labels = tokenizer(\n",
        "            text_target=targets,\n",
        "            max_length=config['max_target_length'],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "        )\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        return model_inputs\n",
        "    return tokenize_function\n",
        "\n",
        "tokenize_function = create_tokenization_function(tokenizer, CONFIG)\n",
        "\n",
        "hf_train = Dataset.from_pandas(train_df)\n",
        "hf_dev = Dataset.from_pandas(dev_df)\n",
        "\n",
        "tokenized_train = hf_train.map(tokenize_function, batched=True, remove_columns=hf_train.column_names, desc='Tokenizing train')\n",
        "tokenized_dev = hf_dev.map(tokenize_function, batched=True, remove_columns=hf_dev.column_names, desc='Tokenizing dev')\n",
        "\n",
        "print('Tokenized sizes:', len(tokenized_train), len(tokenized_dev))\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,  # dynamic padding\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a369e10d",
      "metadata": {},
      "source": [
        "## 5. Metrics (GLEU proxy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bf7c64eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds: EvalPrediction):\n",
        "    # Support both EvalPrediction and (predictions, labels) tuple\n",
        "    if isinstance(eval_preds, tuple):\n",
        "        predictions, labels = eval_preds\n",
        "    else:\n",
        "        predictions, labels = eval_preds.predictions, eval_preds.label_ids\n",
        "    # Unwrap predictions if generate() returns a tuple\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "\n",
        "    # Ensure predictions are token ids (handle logits or floats)\n",
        "    preds = np.array(predictions)\n",
        "    if preds.ndim == 3:  # logits -> ids\n",
        "        preds = preds.argmax(-1)\n",
        "    preds = preds.astype(np.int64, copy=False)\n",
        "    # Guard against invalid ids\n",
        "    vocab_size = len(tokenizer)\n",
        "    preds = np.where((preds >= 0) & (preds < vocab_size), preds, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 to decode labels\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds = [p.strip() for p in decoded_preds]\n",
        "    decoded_labels = [l.strip() for l in decoded_labels]\n",
        "\n",
        "    # Simple GLEU-like proxy using token F1 (proxy for leaderboard GLEU)\n",
        "    gleu_scores = []\n",
        "    for pred, ref in zip(decoded_preds, decoded_labels):\n",
        "        pt = set(pred.lower().split())\n",
        "        rt = set(ref.lower().split())\n",
        "        if not rt:\n",
        "            gleu_scores.append(0.0)\n",
        "            continue\n",
        "        overlap = pt & rt\n",
        "        precision = len(overlap) / len(pt) if pt else 0.0\n",
        "        recall = len(overlap) / len(rt)\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
        "        gleu_scores.append(f1)\n",
        "    gleu = float(np.mean(gleu_scores) * 100)\n",
        "\n",
        "    return {'gleu': gleu}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82678aba",
      "metadata": {},
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9b9bd903",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Effective batch size: 16\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  18/1650 00:21 < 36:53, 0.74 it/s, Epoch 0.05/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 42\u001b[0m\n\u001b[0;32m     30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     31\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     32\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping_patience\u001b[39m\u001b[38;5;124m'\u001b[39m])],\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEffective batch size:\u001b[39m\u001b[38;5;124m'\u001b[39m, training_args\u001b[38;5;241m.\u001b[39mper_device_train_batch_size \u001b[38;5;241m*\u001b[39m training_args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n\u001b[1;32m---> 42\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Save final model and config\u001b[39;00m\n\u001b[0;32m     45\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\MANAV DHAMECHA\\miniconda3\\envs\\gec\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MANAV DHAMECHA\\miniconda3\\envs\\gec\\lib\\site-packages\\transformers\\trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2670\u001b[0m )\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[1;32mc:\\Users\\MANAV DHAMECHA\\miniconda3\\envs\\gec\\lib\\site-packages\\transformers\\trainer.py:4060\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4058\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
            "File \u001b[1;32mc:\\Users\\MANAV DHAMECHA\\miniconda3\\envs\\gec\\lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2734\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\MANAV DHAMECHA\\miniconda3\\envs\\gec\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MANAV DHAMECHA\\miniconda3\\envs\\gec\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MANAV DHAMECHA\\miniconda3\\envs\\gec\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=CONFIG['output_dir'],\n",
        "    num_train_epochs=CONFIG['num_train_epochs'],\n",
        "    per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
        "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    warmup_ratio=CONFIG['warmup_ratio'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    max_grad_norm=CONFIG['max_grad_norm'],\n",
        "    fp16=CONFIG['fp16'],\n",
        "    optim=CONFIG['optim'],\n",
        "    eval_strategy=CONFIG['evaluation_strategy'],\n",
        "    save_strategy=CONFIG['save_strategy'],\n",
        "    logging_steps=CONFIG['logging_steps'],\n",
        "    save_total_limit=CONFIG['save_total_limit'],\n",
        "    load_best_model_at_end=CONFIG['load_best_model_at_end'],\n",
        "    metric_for_best_model=CONFIG['metric_for_best_model'],\n",
        "    greater_is_better=CONFIG['greater_is_better'],\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=CONFIG['generation_config']['max_length'],\n",
        "    generation_num_beams=CONFIG['generation_config']['num_beams'],\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=True,\n",
        "    report_to='none',\n",
        "    push_to_hub=False,\n",
        "    seed=CONFIG['random_seed'],\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_dev,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG['early_stopping_patience'])],\n",
        ")\n",
        "\n",
        "print('Effective batch size:', training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
        "_ = trainer.train()\n",
        "\n",
        "# Save final model and config\n",
        "trainer.save_model(CONFIG['output_dir'])\n",
        "tokenizer.save_pretrained(CONFIG['output_dir'])\n",
        "with open(os.path.join(CONFIG['output_dir'], 'training_config.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(CONFIG, f, indent=2, ensure_ascii=False)\n",
        "print('Model saved to', CONFIG['output_dir'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8db43d",
      "metadata": {},
      "source": [
        "## 7. Inference — generate predictions and save CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1cb8b1dc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from ./mt5-hindi-gec-model ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "149926e8412d44bb808f5bfa3782edc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to predictions.csv\n",
            "                                      Input sentence  \\\n",
            "0  कहते है 'शिक्षा शेरनी को वो दुध है जिसने जितना...   \n",
            "1  आज-कल की विशेष बात यही है कि शिक्षा पे राजा से...   \n",
            "2    जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।   \n",
            "3  आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ...   \n",
            "4  सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क...   \n",
            "\n",
            "                                     Output sentence  \n",
            "0  कहते है 'शिक्षा शेरनी को वो दुध है जिसने जितना...  \n",
            "1  आज-कल की विशेष बात यही है कि शिक्षा पे राजा से...  \n",
            "2    जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।  \n",
            "3  आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ...  \n",
            "4  सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क...  \n"
          ]
        }
      ],
      "source": [
        "def generate_predictions(model_path: str, test_file: str, output_file: str = 'predictions.csv', batch_size: int = 16):\n",
        "    print(f'Loading model from {model_path} ...')\n",
        "    tok = MT5Tokenizer.from_pretrained(model_path)\n",
        "    mdl = MT5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
        "    mdl.eval()\n",
        "\n",
        "    df = pd.read_csv(test_file, encoding='utf-8')\n",
        "    if 'input' in df.columns:\n",
        "        input_col = 'input'\n",
        "    else:\n",
        "        input_col = df.columns[0]\n",
        "    df = df[[input_col]].copy()\n",
        "    df.columns = ['input_text']\n",
        "    df['input_text'] = df['input_text'].apply(clean_text)\n",
        "    df = df[df['input_text'] != '']\n",
        "\n",
        "    preds: List[str] = []\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(df), batch_size), desc='Generating'):\n",
        "            batch = df.iloc[i:i+batch_size]\n",
        "            inputs = ['correct Hindi: ' + s for s in batch['input_text'].tolist()]\n",
        "            enc = tok(\n",
        "                inputs,\n",
        "                max_length=CONFIG['max_input_length'],\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                return_tensors='pt',\n",
        "            ).to(device)\n",
        "            outputs = mdl.generate(\n",
        "                **enc,\n",
        "                max_length=CONFIG['generation_config']['max_length'],\n",
        "                num_beams=CONFIG['generation_config']['num_beams'],\n",
        "                early_stopping=CONFIG['generation_config']['early_stopping'],\n",
        "                repetition_penalty=CONFIG['generation_config']['repetition_penalty'],\n",
        "                no_repeat_ngram_size=CONFIG['generation_config']['no_repeat_ngram_size'],\n",
        "            )\n",
        "            decoded = tok.batch_decode(outputs, skip_special_tokens=True)\n",
        "            preds.extend(decoded)\n",
        "            if torch.cuda.is_available() and i % 100 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "    out_df = pd.DataFrame({\n",
        "        'Input sentence': df['input_text'].tolist()[:len(preds)],\n",
        "        'Output sentence': preds,\n",
        "    })\n",
        "    out_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "    print('Predictions saved to', output_file)\n",
        "    print(out_df.head())\n",
        "    return out_df\n",
        "\n",
        "# Example: run on dev file and save predictions.csv\n",
        "_ = generate_predictions(CONFIG['output_dir'], CONFIG['dev_file'], 'predictions.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d652dc10",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev GLEU (proxy) — model: 84.55\n",
            "Dev GLEU (proxy) — identity baseline: 85.47\n",
            "Exact match rate: 16.82%\n",
            "1.\n",
            "Input:    कहते है 'शिक्षा शेरनी को वो दुध है जिसने जितना पिया उतना ही दहाडा है'।\n",
            "Pred:     कहते है 'शिक्षा शेरनी को वो दुध है जिसने जितना पिया उतना ही दहाडा है'।\n",
            "Ref:      कहते है 'शिक्षा शेरनी का वो दूध है जिसने जितना पिया उतना ही दहाड़ा है'।\n",
            "\n",
            "2.\n",
            "Input:    आज-कल की विशेष बात यही है कि शिक्षा पे राजा से लेकर रंक का भी अधिकार है।\n",
            "Pred:     आज-कल की विशेष बात यही है कि शिक्षा पे राजा से लेकर रंक का भी अधिकार है।\n",
            "Ref:      आज-कल की विशेष बात यही है कि शिक्षा पर राजा से लेकर रंक का भी अधिकार है।\n",
            "\n",
            "3.\n",
            "Input:    जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।\n",
            "Pred:     जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।\n",
            "Ref:      जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।\n",
            "\n",
            "4.\n",
            "Input:    आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ रहा है।\n",
            "Pred:     आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ रहा है।\n",
            "Ref:      आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ रहा है।\n",
            "\n",
            "5.\n",
            "Input:    सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क्या?\n",
            "Pred:     सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क्या?\n",
            "Ref:      सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क्या?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def gleu_proxy(preds, refs):\n",
        "    scores = []\n",
        "    for pred, ref in zip(preds, refs):\n",
        "        pt, rt = set(str(pred).lower().split()), set(str(ref).lower().split())\n",
        "        if not rt:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        overlap = pt & rt\n",
        "        p = len(overlap) / len(pt) if pt else 0.0\n",
        "        r = len(overlap) / len(rt)\n",
        "        f1 = 2 * p * r / (p + r) if (p + r) else 0.0\n",
        "        scores.append(f1)\n",
        "    return float(np.mean(scores) * 100)\n",
        "\n",
        "# Load references\n",
        "dev_df = pd.read_csv(CONFIG['dev_file'], encoding='utf-8')\n",
        "in_col = 'input' if 'input' in dev_df.columns else dev_df.columns[0]\n",
        "ref_col = 'output' if 'output' in dev_df.columns else dev_df.columns[1]\n",
        "refs = dev_df[ref_col].astype(str).tolist()\n",
        "\n",
        "# Load model predictions\n",
        "pred_df = pd.read_csv('predictions.csv', encoding='utf-8')\n",
        "preds = pred_df['Output sentence'].astype(str).tolist()\n",
        "\n",
        "# Compute GLEU proxy and identity baseline\n",
        "gleu_model = gleu_proxy(preds, refs)\n",
        "gleu_identity = gleu_proxy(dev_df[in_col].astype(str).tolist(), refs)\n",
        "\n",
        "exact_match = (pd.Series(preds) == pd.Series(refs)).mean() * 100.0\n",
        "\n",
        "print(f\"Dev GLEU (proxy) — model: {gleu_model:.2f}\")\n",
        "print(f\"Dev GLEU (proxy) — identity baseline: {gleu_identity:.2f}\")\n",
        "print(f\"Exact match rate: {exact_match:.2f}%\")\n",
        "\n",
        "# Show a few samples\n",
        "for i in range(min(5, len(pred_df))):\n",
        "    print(f\"{i+1}.\")\n",
        "    print(\"Input:   \", pred_df.iloc[i]['Input sentence'][:120])\n",
        "    print(\"Pred:    \", pred_df.iloc[i]['Output sentence'][:120])\n",
        "    print(\"Ref:     \", dev_df.iloc[i][ref_col][:120])\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

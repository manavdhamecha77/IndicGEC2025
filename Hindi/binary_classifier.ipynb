{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification: Equal Input/Output (Hindi IndicGEC)\n",
    "\n",
    "This notebook trains a binary classifier for the task: label = 1 if output sentence == input sentence, else 0.\n",
    "\n",
    "It includes:\n",
    "- Robust loading of train.csv (auto-detect input/output columns).\n",
    "- Label creation.\n",
    "- Baseline rule (exact string equality).\n",
    "- TF-IDF (char n-grams) + Logistic Regression model.\n",
    "- Evaluation metrics.\n",
    "- Model saving and an inference helper.\n",
    "\n",
    "Note: Char n-grams are language-agnostic and work well with Hindi text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install dependencies if missing (run once)\n",
    "import sys, subprocess\n",
    "\n",
    "def ensure(pkg_name, import_name=None):\n",
    "    name = (import_name or pkg_name).replace('-', '_')\n",
    "    try:\n",
    "        __import__(name)\n",
    "    except Exception:\n",
    "        print(f'Installing {pkg_name} ...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "\n",
    "ensure('pandas')\n",
    "ensure('numpy')\n",
    "ensure('scikit-learn', 'sklearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = 'train.csv'  # assumes this notebook is in the same folder as train.csv\n",
    "assert Path(DATA_PATH).exists(), f'Could not find {DATA_PATH}. Please place train.csv next to this notebook.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Input sentence', 'Output sentence', 'Unnamed: 2']\n",
      "Detected columns -> input: 'Input sentence', output: 'Output sentence'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input sentence</th>\n",
       "      <th>Output sentence</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>शिक्षा क्या है?</td>\n",
       "      <td>शिक्षा क्या है?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>किसी भी कार्य को सीख लेने की क्रिया को शिक्षा ...</td>\n",
       "      <td>किसी भी कार्य को सीख लेने की क्रिया को शिक्षा ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ये केवल किताबी ज्ञान अर्जन तक ही सिमित नहीं है।</td>\n",
       "      <td>ये केवल किताबी ज्ञान अर्जन तक ही सीमित नहीं है।</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>यह कई विभागों में बांटा जा सकता है।</td>\n",
       "      <td>यह कई विभागों में बांटा जा सकता है।</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>जैसे - व्यावहारिक शिक्षा, किताबी शिक्षा अथवा अ...</td>\n",
       "      <td>जैसे - व्यावहारिक शिक्षा, किताबी शिक्षा अथवा आ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Input sentence  \\\n",
       "0                                    शिक्षा क्या है?   \n",
       "1  किसी भी कार्य को सीख लेने की क्रिया को शिक्षा ...   \n",
       "2    ये केवल किताबी ज्ञान अर्जन तक ही सिमित नहीं है।   \n",
       "3                यह कई विभागों में बांटा जा सकता है।   \n",
       "4  जैसे - व्यावहारिक शिक्षा, किताबी शिक्षा अथवा अ...   \n",
       "\n",
       "                                     Output sentence Unnamed: 2  label  \n",
       "0                                    शिक्षा क्या है?        NaN      1  \n",
       "1  किसी भी कार्य को सीख लेने की क्रिया को शिक्षा ...        NaN      1  \n",
       "2    ये केवल किताबी ज्ञान अर्जन तक ही सीमित नहीं है।        NaN      0  \n",
       "3                यह कई विभागों में बांटा जा सकता है।        NaN      1  \n",
       "4  जैसे - व्यावहारिक शिक्षा, किताबी शिक्षा अथवा आ...        NaN      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and detect column names\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Columns:', list(df.columns))\n",
    "\n",
    "def find_col(candidates):\n",
    "    # Return the first column whose lowercase name contains any candidate substring\n",
    "    lowered = {c: str(c).strip().lower() for c in df.columns}\n",
    "    for key in candidates:\n",
    "        for col, lc in lowered.items():\n",
    "            if key in lc:\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "input_col = find_col(['input', 'source', 'src'])\n",
    "output_col = find_col(['output', 'target', 'tgt', 'reference', 'gold'])\n",
    "\n",
    "if input_col is None or output_col is None:\n",
    "    # Fallback to first two columns\n",
    "    cols = list(df.columns)\n",
    "    if len(cols) < 2:\n",
    "        raise ValueError('train.csv must have at least two columns (input and output).')\n",
    "    input_col, output_col = cols[0], cols[1]\n",
    "    print(f'Warning: Could not auto-detect columns. Using first two columns: {input_col!r}, {output_col!r}')\n",
    "else:\n",
    "    print(f'Detected columns -> input: {input_col!r}, output: {output_col!r}')\n",
    "\n",
    "# Clean and ensure string type\n",
    "df[input_col] = df[input_col].fillna('').astype(str).str.strip()\n",
    "df[output_col] = df[output_col].fillna('').astype(str).str.strip()\n",
    "\n",
    "# Create label: 1 if equal, else 0\n",
    "df['label'] = (df[input_col] == df[output_col]).astype(int)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479, 120, np.float64(0.09682804674457429))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/validation split\n",
    "X_pair = (df[input_col] + ' [SEP] ' + df[output_col]).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_pair, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y if len(np.unique(y)) > 1 else None\n",
    ")\n",
    "len(X_train), len(X_val), np.mean(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for metrics\n",
    "def print_metrics(y_true, y_pred, y_proba=None, title=None):\n",
    "    if title:\n",
    "        print('='*len(title))\n",
    "        print(title)\n",
    "        print('='*len(title))\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1b = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    print(f'F1-macro: {f1m:.4f} | F1-binary(positive=1): {f1b:.4f}')\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_proba)\n",
    "            print(f'ROC-AUC: {auc:.4f}')\n",
    "        except Exception as e:\n",
    "            print('ROC-AUC unavailable:', e)\n",
    "# print('Classification Report:\\n', classification_report(y_true, y_pred, zero_division=0))\n",
    "# print('Confusion Matrix:\\n', confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "Baseline: Exact Equality Rule\n",
      "=============================\n",
      "Accuracy: 1.0000\n",
      "F1-macro: 1.0000 | F1-binary(positive=1): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Baseline rule: exact match on raw strings\n",
    "def rule_predict(batch_pairs):\n",
    "    preds = []\n",
    "    for s in batch_pairs:\n",
    "        try:\n",
    "            a, b = s.split(' [SEP] ', 1)\n",
    "        except ValueError:\n",
    "            # if separator missing, treat as not equal\n",
    "            preds.append(0)\n",
    "            continue\n",
    "        preds.append(1 if a.strip() == b.strip() else 0)\n",
    "    return np.array(preds)\n",
    "\n",
    "rule_val = rule_predict(X_val)\n",
    "print_metrics(y_val, rule_val, title='Baseline: Exact Equality Rule')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "TF-IDF + Logistic Regression\n",
      "============================\n",
      "Accuracy: 0.9000\n",
      "F1-macro: 0.5982 | F1-binary(positive=1): 0.2500\n",
      "ROC-AUC: 0.7330\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (char n-grams) + Logistic Regression\n",
    "tfidf_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=(2, 5), min_df=2)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear'))\n",
    "])\n",
    "\n",
    "tfidf_lr.fit(X_train, y_train)\n",
    "pred_val = tfidf_lr.predict(X_val)\n",
    "proba_val = None\n",
    "if hasattr(tfidf_lr, 'predict_proba'):\n",
    "    proba_val = tfidf_lr.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print_metrics(y_val, pred_val, proba_val, title='TF-IDF + Logistic Regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: D:\\CODING\\IndicGEC2025\\Hindi\\models\\binary_eq_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path('models') / 'binary_eq_model.joblib'\n",
    "joblib.dump(tfidf_lr, MODEL_PATH)\n",
    "print(f'Model saved to: {MODEL_PATH.resolve()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "यह वाक्य है। | यह वाक्य है। -> 0\n",
      "मेरा नाम राहुल है। | मेरा नाम रोहित है। -> 0\n"
     ]
    }
   ],
   "source": [
    "# Inference helper\n",
    "def predict_equal(input_sentence: str, output_sentence: str) -> int:\n",
    "    s = f'{input_sentence.strip()} [SEP] {output_sentence.strip()}'\n",
    "    return int(tfidf_lr.predict([s])[0])\n",
    "\n",
    "# Example usage:\n",
    "examples = [\n",
    "    ('यह वाक्य है।', 'यह वाक्य है।'),\n",
    "    ('मेरा नाम राहुल है।', 'मेरा नाम रोहित है।'),\n",
    "]\n",
    "for a, b in examples:\n",
    "    print(a, '|', b, '->', predict_equal(a, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f6fe17",
   "metadata": {},
   "source": [
    "# Model Evaluation on Development Set\n",
    "\n",
    "Let's test our trained model on the development set (`dev.csv`) and calculate the GLEU score for comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14174482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK GLEU already available\n"
     ]
    }
   ],
   "source": [
    "# Install GLEU score dependencies\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.translate.gleu_score import sentence_gleu\n",
    "    print(\"NLTK GLEU already available\")\n",
    "except ImportError:\n",
    "    print(\"Installing NLTK for GLEU score calculation...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'nltk'])\n",
    "    import nltk\n",
    "    from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45fd1b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading development set: dev.csv\n",
      "Development set shape: (107, 2)\n",
      "Columns: ['Input sentence', 'Output sentence']\n",
      " Detected columns -> input: 'Input sentence', output: 'Output sentence'\n",
      "   Dev set statistics:\n",
      "  - Total samples: 107\n",
      "  - Identical pairs (label=1): 24 (22.43%)\n",
      "  - Different pairs (label=0): 83 (77.57%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input sentence</th>\n",
       "      <th>Output sentence</th>\n",
       "      <th>label_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>कहते है 'शिक्षा शेरनी को वो दुध है जिसने जितना...</td>\n",
       "      <td>कहते है 'शिक्षा शेरनी का वो दूध है जिसने जितना...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>आज-कल की विशेष बात यही है कि शिक्षा पे राजा से...</td>\n",
       "      <td>आज-कल की विशेष बात यही है कि शिक्षा पर राजा से...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।</td>\n",
       "      <td>जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ...</td>\n",
       "      <td>आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क...</td>\n",
       "      <td>सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Input sentence  \\\n",
       "0  कहते है 'शिक्षा शेरनी को वो दुध है जिसने जितना...   \n",
       "1  आज-कल की विशेष बात यही है कि शिक्षा पे राजा से...   \n",
       "2    जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।   \n",
       "3  आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ...   \n",
       "4  सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क...   \n",
       "\n",
       "                                     Output sentence  label_true  \n",
       "0  कहते है 'शिक्षा शेरनी का वो दूध है जिसने जितना...           0  \n",
       "1  आज-कल की विशेष बात यही है कि शिक्षा पर राजा से...           0  \n",
       "2    जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।           1  \n",
       "3  आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ...           1  \n",
       "4  सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क...           1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process development set\n",
    "DEV_PATH = 'dev.csv'\n",
    "\n",
    "# Check if dev.csv exists\n",
    "if not Path(DEV_PATH).exists():\n",
    "    print(f\" {DEV_PATH} not found in current directory.\")\n",
    "    print(f\"Current directory: {Path.cwd()}\")\n",
    "    print(f\"Available files: {list(Path('.').glob('*.csv'))}\")\n",
    "    raise FileNotFoundError(f\"Please place {DEV_PATH} in the same folder as this notebook.\")\n",
    "\n",
    "# Load development data\n",
    "print(f\" Loading development set: {DEV_PATH}\")\n",
    "dev_df = pd.read_csv(DEV_PATH)\n",
    "print(f\"Development set shape: {dev_df.shape}\")\n",
    "print(f\"Columns: {list(dev_df.columns)}\")\n",
    "\n",
    "# Auto-detect columns for dev set (same logic as training)\n",
    "def autodetect_dev_cols(df):\n",
    "    def find_col(df, candidates):\n",
    "        lowered = {c: str(c).strip().lower() for c in df.columns}\n",
    "        for key in candidates:\n",
    "            for col, lc in lowered.items():\n",
    "                if key in lc:\n",
    "                    return col\n",
    "        return None\n",
    "    \n",
    "    ic = find_col(df, ['input', 'source', 'src']) or df.columns[0]\n",
    "    oc = find_col(df, ['output', 'target', 'tgt', 'reference', 'gold']) or df.columns[1]\n",
    "    return ic, oc\n",
    "\n",
    "dev_input_col, dev_output_col = autodetect_dev_cols(dev_df)\n",
    "print(f\" Detected columns -> input: '{dev_input_col}', output: '{dev_output_col}'\")\n",
    "\n",
    "# Clean development data\n",
    "dev_df[dev_input_col] = dev_df[dev_input_col].fillna('').astype(str).str.strip()\n",
    "dev_df[dev_output_col] = dev_df[dev_output_col].fillna('').astype(str).str.strip()\n",
    "\n",
    "# Create labels for dev set\n",
    "dev_df['label_true'] = (dev_df[dev_input_col] == dev_df[dev_output_col]).astype(int)\n",
    "\n",
    "print(f\"   Dev set statistics:\")\n",
    "print(f\"  - Total samples: {len(dev_df)}\")\n",
    "print(f\"  - Identical pairs (label=1): {dev_df['label_true'].sum()} ({dev_df['label_true'].mean():.2%})\")\n",
    "print(f\"  - Different pairs (label=0): {(1-dev_df['label_true']).sum()} ({(1-dev_df['label_true']).mean():.2%})\")\n",
    "\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2716504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on development set...\n",
      "Binary Classification Results on Development Set:\n",
      "===========================\n",
      "Development Set Performance\n",
      "===========================\n",
      "Accuracy: 0.8131\n",
      "F1-macro: 0.6123 | F1-binary(positive=1): 0.3333\n",
      "ROC-AUC: 0.7309\n",
      "\n",
      " Detailed Analysis:\n",
      "  - Correct predictions: 87/107\n",
      "  - True Positives (correctly identified identical): 5\n",
      "  - True Negatives (correctly identified different): 82\n",
      "  - False Positives (incorrectly said identical): 1\n",
      "  - False Negatives (incorrectly said different): 19\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on development set\n",
    "print(\"Generating predictions on development set...\")\n",
    "\n",
    "# Prepare input pairs for model prediction\n",
    "dev_X_pair = (dev_df[dev_input_col] + ' [SEP] ' + dev_df[dev_output_col]).values\n",
    "dev_y_true = dev_df['label_true'].values\n",
    "\n",
    "# Get model predictions\n",
    "dev_y_pred = tfidf_lr.predict(dev_X_pair)\n",
    "dev_y_proba = tfidf_lr.predict_proba(dev_X_pair)[:, 1]\n",
    "\n",
    "# Add predictions to dataframe\n",
    "dev_df['label_pred'] = dev_y_pred\n",
    "dev_df['confidence'] = dev_y_proba\n",
    "\n",
    "# Binary classification metrics\n",
    "print(\"Binary Classification Results on Development Set:\")\n",
    "print_metrics(dev_y_true, dev_y_pred, dev_y_proba, title='Development Set Performance')\n",
    "\n",
    "# Detailed breakdown\n",
    "print(f\"\\n Detailed Analysis:\")\n",
    "print(f\"  - Correct predictions: {(dev_y_true == dev_y_pred).sum()}/{len(dev_y_true)}\")\n",
    "print(f\"  - True Positives (correctly identified identical): {((dev_y_true == 1) & (dev_y_pred == 1)).sum()}\")\n",
    "print(f\"  - True Negatives (correctly identified different): {((dev_y_true == 0) & (dev_y_pred == 0)).sum()}\")\n",
    "print(f\"  - False Positives (incorrectly said identical): {((dev_y_true == 0) & (dev_y_pred == 1)).sum()}\")\n",
    "print(f\"  - False Negatives (incorrectly said different): {((dev_y_true == 1) & (dev_y_pred == 0)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2e0a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Calculating GLEU Scores...\n",
      " Tokenizing and calculating GLEU scores...\n",
      "\n",
      " GLEU Score Results on Development Set:\n",
      "   Mean GLEU Score: 0.7191\n",
      "   Median GLEU Score: 0.7791\n",
      "   Standard Deviation: 0.2449\n",
      "   Min GLEU Score: 0.1739\n",
      "   Max GLEU Score: 1.0000\n",
      "\n",
      " GLEU Score Distribution:\n",
      "   Perfect/Near-perfect (≥0.99): 24 (22.4%)\n",
      "   High similarity (0.8-0.99): 28 (26.2%)\n",
      "   Medium similarity (0.5-0.8): 31 (29.0%)\n",
      "   Low similarity (<0.5): 24 (22.4%)\n"
     ]
    }
   ],
   "source": [
    "# GLEU Score Calculation\n",
    "print(\" Calculating GLEU Scores...\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Simple tokenization for Hindi text\"\"\"\n",
    "    # Basic tokenization - splits on whitespace and common punctuation\n",
    "    import re\n",
    "    # Split on whitespace and keep punctuation as separate tokens\n",
    "    tokens = re.findall(r'\\S+', str(text).strip())\n",
    "    return tokens\n",
    "\n",
    "def calculate_gleu_scores(references, hypotheses):\n",
    "    \"\"\"Calculate GLEU scores for a set of reference-hypothesis pairs\"\"\"\n",
    "    gleu_scores = []\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        # Tokenize both reference and hypothesis\n",
    "        ref_tokens = tokenize_text(ref)\n",
    "        hyp_tokens = tokenize_text(hyp)\n",
    "        \n",
    "        # Calculate sentence-level GLEU\n",
    "        # sentence_gleu expects reference as list of token lists, hypothesis as token list\n",
    "        try:\n",
    "            gleu = sentence_gleu([ref_tokens], hyp_tokens)\n",
    "            gleu_scores.append(gleu)\n",
    "        except Exception as e:\n",
    "            # In case of any tokenization issues, use 0.0\n",
    "            gleu_scores.append(0.0)\n",
    "    \n",
    "    return gleu_scores\n",
    "\n",
    "# Calculate GLEU scores for all sentence pairs\n",
    "print(\" Tokenizing and calculating GLEU scores...\")\n",
    "\n",
    "# For GLEU, we compare input vs output sentences (regardless of our model's prediction)\n",
    "references = dev_df[dev_input_col].tolist()  # Original sentences\n",
    "hypotheses = dev_df[dev_output_col].tolist()  # Corrected/target sentences\n",
    "\n",
    "# Calculate GLEU scores\n",
    "gleu_scores = calculate_gleu_scores(references, hypotheses)\n",
    "dev_df['gleu_score'] = gleu_scores\n",
    "\n",
    "# Overall GLEU statistics\n",
    "mean_gleu = np.mean(gleu_scores)\n",
    "median_gleu = np.median(gleu_scores)\n",
    "std_gleu = np.std(gleu_scores)\n",
    "\n",
    "print(f\"\\n GLEU Score Results on Development Set:\")\n",
    "print(f\"   Mean GLEU Score: {mean_gleu:.4f}\")\n",
    "print(f\"   Median GLEU Score: {median_gleu:.4f}\")\n",
    "print(f\"   Standard Deviation: {std_gleu:.4f}\")\n",
    "print(f\"   Min GLEU Score: {min(gleu_scores):.4f}\")\n",
    "print(f\"   Max GLEU Score: {max(gleu_scores):.4f}\")\n",
    "\n",
    "# GLEU score distribution\n",
    "perfect_matches = sum(1 for score in gleu_scores if score >= 0.99)\n",
    "high_scores = sum(1 for score in gleu_scores if 0.8 <= score < 0.99)\n",
    "medium_scores = sum(1 for score in gleu_scores if 0.5 <= score < 0.8)\n",
    "low_scores = sum(1 for score in gleu_scores if score < 0.5)\n",
    "\n",
    "print(f\"\\n GLEU Score Distribution:\")\n",
    "print(f\"   Perfect/Near-perfect (≥0.99): {perfect_matches} ({perfect_matches/len(gleu_scores):.1%})\")\n",
    "print(f\"   High similarity (0.8-0.99): {high_scores} ({high_scores/len(gleu_scores):.1%})\")\n",
    "print(f\"   Medium similarity (0.5-0.8): {medium_scores} ({medium_scores/len(gleu_scores):.1%})\")\n",
    "print(f\"   Low similarity (<0.5): {low_scores} ({low_scores/len(gleu_scores):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6914d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Analyzing correlation between model predictions and GLEU scores...\n",
      "\n",
      " GLEU Scores by Model Prediction:\n",
      "   Pairs predicted as IDENTICAL (label=1): 6 samples\n",
      "    - Mean GLEU: 0.8833\n",
      "    - Median GLEU: 1.0000\n",
      "    - Std GLEU: 0.2858\n",
      "   Pairs predicted as DIFFERENT (label=0): 101 samples\n",
      "    - Mean GLEU: 0.7094\n",
      "    - Median GLEU: 0.7541\n",
      "    - Std GLEU: 0.2416\n",
      "\n",
      " Correlation between model predictions and GLEU scores: 0.1635\n",
      "\n",
      " Model Accuracy Analysis:\n",
      "   On perfect GLEU matches (≥0.99): 0.2083 (24 samples)\n",
      "   On imperfect GLEU matches (<0.99): 0.9880 (83 samples)\n",
      "\n",
      " Summary Report:\n",
      "   Overall Development Set Accuracy: 0.8131\n",
      "   Mean GLEU Score: 0.7191\n",
      "   Total Samples Evaluated: 107\n",
      "   Prediction-GLEU Correlation: 0.1635\n"
     ]
    }
   ],
   "source": [
    "# Correlation Analysis: Model Predictions vs GLEU Scores\n",
    "print(\" Analyzing correlation between model predictions and GLEU scores...\")\n",
    "\n",
    "# Group by model predictions\n",
    "identical_pairs = dev_df[dev_df['label_pred'] == 1]\n",
    "different_pairs = dev_df[dev_df['label_pred'] == 0]\n",
    "\n",
    "print(f\"\\n GLEU Scores by Model Prediction:\")\n",
    "print(f\"   Pairs predicted as IDENTICAL (label=1): {len(identical_pairs)} samples\")\n",
    "print(f\"    - Mean GLEU: {identical_pairs['gleu_score'].mean():.4f}\")\n",
    "print(f\"    - Median GLEU: {identical_pairs['gleu_score'].median():.4f}\")\n",
    "print(f\"    - Std GLEU: {identical_pairs['gleu_score'].std():.4f}\")\n",
    "\n",
    "print(f\"   Pairs predicted as DIFFERENT (label=0): {len(different_pairs)} samples\")\n",
    "print(f\"    - Mean GLEU: {different_pairs['gleu_score'].mean():.4f}\")\n",
    "print(f\"    - Median GLEU: {different_pairs['gleu_score'].median():.4f}\")\n",
    "print(f\"    - Std GLEU: {different_pairs['gleu_score'].std():.4f}\")\n",
    "\n",
    "# Correlation coefficient\n",
    "correlation = np.corrcoef(dev_df['label_pred'], dev_df['gleu_score'])[0, 1]\n",
    "print(f\"\\n Correlation between model predictions and GLEU scores: {correlation:.4f}\")\n",
    "\n",
    "# Performance on perfect vs imperfect matches\n",
    "perfect_gleu_mask = dev_df['gleu_score'] >= 0.99\n",
    "imperfect_gleu_mask = dev_df['gleu_score'] < 0.99\n",
    "\n",
    "perfect_accuracy = (dev_df[perfect_gleu_mask]['label_true'] == dev_df[perfect_gleu_mask]['label_pred']).mean()\n",
    "imperfect_accuracy = (dev_df[imperfect_gleu_mask]['label_true'] == dev_df[imperfect_gleu_mask]['label_pred']).mean()\n",
    "\n",
    "print(f\"\\n Model Accuracy Analysis:\")\n",
    "print(f\"   On perfect GLEU matches (≥0.99): {perfect_accuracy:.4f} ({perfect_gleu_mask.sum()} samples)\")\n",
    "print(f\"   On imperfect GLEU matches (<0.99): {imperfect_accuracy:.4f} ({imperfect_gleu_mask.sum()} samples)\")\n",
    "\n",
    "print(f\"\\n Summary Report:\")\n",
    "print(f\"   Overall Development Set Accuracy: {(dev_y_true == dev_y_pred).mean():.4f}\")\n",
    "print(f\"   Mean GLEU Score: {mean_gleu:.4f}\")\n",
    "print(f\"   Total Samples Evaluated: {len(dev_df)}\")\n",
    "print(f\"   Prediction-GLEU Correlation: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd7a9ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample Analysis - Examples from Development Set:\n",
      "================================================================================\n",
      "\n",
      " Example 1:\n",
      "  Input:  'कहते है 'शिक्षा शेरनी को वो दुध है जिसने जितना पिया उतना ही दहाडा है'।'\n",
      "  Output: 'कहते है 'शिक्षा शेरनी का वो दूध है जिसने जितना पिया उतना ही दहाड़ा है'।'\n",
      "  True Label: 0 | Predicted: 0 | Confidence: 0.416\n",
      "  GLEU Score: 0.5556\n",
      "  Status:  CORRECT | GLEU:  Medium similarity\n",
      "\n",
      " Example 2:\n",
      "  Input:  'आज-कल की विशेष बात यही है कि शिक्षा पे राजा से लेकर रंक का भी अधिकार है।'\n",
      "  Output: 'आज-कल की विशेष बात यही है कि शिक्षा पर राजा से लेकर रंक का भी अधिकार है।'\n",
      "  True Label: 0 | Predicted: 0 | Confidence: 0.400\n",
      "  GLEU Score: 0.8387\n",
      "  Status:  CORRECT | GLEU:  High similarity\n",
      "\n",
      " Example 3:\n",
      "  Input:  'जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।'\n",
      "  Output: 'जलवायु परिवर्तन आज के समय की सच्चाई बन चुकी है।'\n",
      "  True Label: 1 | Predicted: 0 | Confidence: 0.361\n",
      "  GLEU Score: 1.0000\n",
      "  Status:  INCORRECT | GLEU:  Perfect match\n",
      "\n",
      " Example 4:\n",
      "  Input:  'आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ रहा है।'\n",
      "  Output: 'आज पूरा विश्व जलवायु परिवर्तन की समस्या से जूझ रहा है।'\n",
      "  True Label: 1 | Predicted: 0 | Confidence: 0.475\n",
      "  GLEU Score: 1.0000\n",
      "  Status:  INCORRECT | GLEU:  Perfect match\n",
      "\n",
      " Example 5:\n",
      "  Input:  'सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क्या?'\n",
      "  Output: 'सबसे पहले हम जानते हैं कि जलवायु परिवर्तन है क्या?'\n",
      "  True Label: 1 | Predicted: 0 | Confidence: 0.354\n",
      "  GLEU Score: 1.0000\n",
      "  Status:  INCORRECT | GLEU:  Perfect match\n",
      "\n",
      " Example 32:\n",
      "  Input:  'लोग को और जागरूक बनाने के लिए सरकार भी कई तरह की योजनाएँ बनाती है जैसे पर्यावरण बचाओ, भविष्य बचाओ इत्यादि।'\n",
      "  Output: 'लोग को और जागरूक बनाने के लिए सरकार भी कई तरह की योजनाएँ बनाती है जैसे पर्यावरण बचाओ, भविष्य बचाओ इत्यादि।'\n",
      "  True Label: 1 | Predicted: 0 | Confidence: 0.413\n",
      "  GLEU Score: 1.0000\n",
      "  Status:  INCORRECT | GLEU:  Perfect match\n",
      "\n",
      " Example 52:\n",
      "  Input:  '22- महाविद्यालय में वरिष्ठ विदयार्थीयों से मुलाकात को और पढाई पर चर्चा और किताबों की सूची ली'\n",
      "  Output: '२२- महाविद्यालय में वरिष्ठ विदयार्थियों से मुलाकात को और पढाई पर चर्चा और किताबों की सूची ली'\n",
      "  True Label: 0 | Predicted: 0 | Confidence: 0.291\n",
      "  GLEU Score: 0.7742\n",
      "  Status:  CORRECT | GLEU:  Medium similarity\n",
      "\n",
      " Example 104:\n",
      "  Input:  'मेरे गांव मैं वहां पर पढ़ना बहुत बड़ी बात होति थी'\n",
      "  Output: 'मेरे गांव में वहां पढ़ना बहुत बड़ी बात होती थी।'\n",
      "  True Label: 0 | Predicted: 0 | Confidence: 0.253\n",
      "  GLEU Score: 0.3684\n",
      "  Status:  CORRECT | GLEU:  Low similarity\n",
      "\n",
      " Example 68:\n",
      "  Input:  'अच्छे नम्बर लाने की होड़ में पूरे स्कूल में अव्वल भी आ गयी लगा मेरी जिंदगी तो अच्छी चल रही है।'\n",
      "  Output: 'अच्छे नंबर लाने की होड़ में पूरे स्कूल में अव्वल भी आ गई और मुझे ऐसा लगा कि मेरी ज़िंदगी तो अच्छी चल रही है।'\n",
      "  True Label: 0 | Predicted: 0 | Confidence: 0.286\n",
      "  GLEU Score: 0.5426\n",
      "  Status:  CORRECT | GLEU:  Medium similarity\n",
      "\n",
      " Example 34:\n",
      "  Input:  'जैसे हम शहर या गांवों में कैंप लगा कर लोगो erroe इसके प्रभाव और आने वाले दिनों में इसका क्या असर पड़ सकता है, हमारे रहन-सहन और भविष्य में होने वाली चीजो के बारे में बता सकते है।'\n",
      "  Output: 'जैसे हम शहर या गांवों में कैंप लगा कर लोगो को इसके प्रभाव और आने वाले दिनों में इसका क्या असर पड़ सकता है, हमारे रहन-सहन और भविष्य में होने वाली चीजो के बारे में बता सकते है।'\n",
      "  True Label: 0 | Predicted: 0 | Confidence: 0.300\n",
      "  GLEU Score: 0.9315\n",
      "  Status:  CORRECT | GLEU:  High similarity\n",
      "\n",
      "================================================================================\n",
      " Results saved to: dev_results_with_gleu.csv\n",
      "   - Columns: ['Input sentence', 'Output sentence', 'label_true', 'label_pred', 'confidence', 'gleu_score']\n",
      "   - Rows: 107\n",
      "\n",
      " Evaluation Complete!\n",
      "   Mean GLEU Score: 0.7191\n",
      "   Classification Accuracy: 0.8131\n",
      "   Total Samples: 107\n"
     ]
    }
   ],
   "source": [
    "# Sample Analysis: Show some examples\n",
    "print(\" Sample Analysis - Examples from Development Set:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show some interesting examples\n",
    "sample_indices = [0, 1, 2, 3, 4]  # First 5 examples\n",
    "if len(dev_df) > 10:\n",
    "    # Add some random samples if dataset is large enough\n",
    "    sample_indices.extend(np.random.choice(range(5, len(dev_df)), size=min(5, len(dev_df)-5), replace=False))\n",
    "\n",
    "for i in sample_indices:\n",
    "    row = dev_df.iloc[i]\n",
    "    print(f\"\\n Example {i+1}:\")\n",
    "    print(f\"  Input:  '{row[dev_input_col]}'\")\n",
    "    print(f\"  Output: '{row[dev_output_col]}'\")\n",
    "    print(f\"  True Label: {row['label_true']} | Predicted: {row['label_pred']} | Confidence: {row['confidence']:.3f}\")\n",
    "    print(f\"  GLEU Score: {row['gleu_score']:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if row['label_true'] == row['label_pred']:\n",
    "        status = \" CORRECT\"\n",
    "    else:\n",
    "        status = \" INCORRECT\"\n",
    "    \n",
    "    if row['gleu_score'] >= 0.99:\n",
    "        gleu_status = \" Perfect match\"\n",
    "    elif row['gleu_score'] >= 0.8:\n",
    "        gleu_status = \" High similarity\"\n",
    "    elif row['gleu_score'] >= 0.5:\n",
    "        gleu_status = \" Medium similarity\"\n",
    "    else:\n",
    "        gleu_status = \" Low similarity\"\n",
    "    \n",
    "    print(f\"  Status: {status} | GLEU: {gleu_status}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Save results to CSV\n",
    "results_path = 'dev_results_with_gleu.csv'\n",
    "dev_df.to_csv(results_path, index=False)\n",
    "print(f\" Results saved to: {results_path}\")\n",
    "print(f\"   - Columns: {list(dev_df.columns)}\")\n",
    "print(f\"   - Rows: {len(dev_df)}\")\n",
    "\n",
    "print(f\"\\n Evaluation Complete!\")\n",
    "print(f\"   Mean GLEU Score: {mean_gleu:.4f}\")\n",
    "print(f\"   Classification Accuracy: {(dev_y_true == dev_y_pred).mean():.4f}\")\n",
    "print(f\"   Total Samples: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Batch inference on a file\n",
    "If you later have a file like `test.csv` with the same two columns, you can run:\n",
    "\n",
    "1. Load it with `pd.read_csv('test.csv')`.\n",
    "2. Auto-detect the columns the same way as above.\n",
    "3. Build pairs with `' [SEP] '`.\n",
    "4. Use `tfidf_lr.predict(pairs)` to get labels (1 if equal, else 0).\n",
    "5. Save the predictions to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Batch inference template\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "# def autodetect_cols(df):\n",
    "#     def find_col(df, candidates):\n",
    "#         lowered = {c: str(c).strip().lower() for c in df.columns}\n",
    "#         for key in candidates:\n",
    "#             for col, lc in lowered.items():\n",
    "#                 if key in lc:\n",
    "#                     return col\n",
    "#         return None\n",
    "#     ic = find_col(df, ['input', 'source', 'src']) or df.columns[0]\n",
    "#     oc = find_col(df, ['output', 'target', 'tgt', 'reference', 'gold']) or df.columns[1]\n",
    "#     return ic, oc\n",
    "#\n",
    "# ic, oc = autodetect_cols(test_df)\n",
    "# test_df[ic] = test_df[ic].fillna('').astype(str).str.strip()\n",
    "# test_df[oc] = test_df[oc].fillna('').astype(str).str.strip()\n",
    "# pairs = (test_df[ic] + ' [SEP] ' + test_df[oc]).values\n",
    "# preds = tfidf_lr.predict(pairs).astype(int)\n",
    "# out = test_df.copy()\n",
    "# out['label_pred'] = preds\n",
    "# out.to_csv('predictions.csv', index=False)\n",
    "# print('Saved predictions.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IndicGEC2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

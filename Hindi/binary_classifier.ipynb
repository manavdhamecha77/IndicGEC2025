{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification: Equal Input/Output (Hindi IndicGEC)\n",
    "\n",
    "This notebook trains a binary classifier for the task: label = 1 if output sentence == input sentence, else 0.\n",
    "\n",
    "It includes:\n",
    "- Robust loading of train.csv (auto-detect input/output columns).\n",
    "- Label creation.\n",
    "- Baseline rule (exact string equality).\n",
    "- TF-IDF (char n-grams) + Logistic Regression model.\n",
    "- Evaluation metrics.\n",
    "- Model saving and an inference helper.\n",
    "\n",
    "Note: Char n-grams are language-agnostic and work well with Hindi text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install dependencies if missing (run once)\n",
    "import sys, subprocess\n",
    "\n",
    "def ensure(pkg_name, import_name=None):\n",
    "    name = (import_name or pkg_name).replace('-', '_')\n",
    "    try:\n",
    "        __import__(name)\n",
    "    except Exception:\n",
    "        print(f'Installing {pkg_name} ...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "\n",
    "ensure('pandas')\n",
    "ensure('numpy')\n",
    "ensure('scikit-learn', 'sklearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = 'train.csv'  # assumes this notebook is in the same folder as train.csv\n",
    "assert Path(DATA_PATH).exists(), f'Could not find {DATA_PATH}. Please place train.csv next to this notebook.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Input sentence', 'Output sentence', 'Unnamed: 2']\n",
      "Detected columns -> input: 'Input sentence', output: 'Output sentence'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input sentence</th>\n",
       "      <th>Output sentence</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?</td>\n",
       "      <td>‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡•ã ‡§∏‡•Ä‡§ñ ‡§≤‡•á‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•ã ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ...</td>\n",
       "      <td>‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡•ã ‡§∏‡•Ä‡§ñ ‡§≤‡•á‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•ã ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡§Ø‡•á ‡§ï‡•á‡§µ‡§≤ ‡§ï‡§ø‡§§‡§æ‡§¨‡•Ä ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ö‡§∞‡•ç‡§ú‡§® ‡§§‡§ï ‡§π‡•Ä ‡§∏‡§ø‡§Æ‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§</td>\n",
       "      <td>‡§Ø‡•á ‡§ï‡•á‡§µ‡§≤ ‡§ï‡§ø‡§§‡§æ‡§¨‡•Ä ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ö‡§∞‡•ç‡§ú‡§® ‡§§‡§ï ‡§π‡•Ä ‡§∏‡•Ä‡§Æ‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡§Ø‡§π ‡§ï‡§à ‡§µ‡§ø‡§≠‡§æ‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§Ç‡§ü‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§</td>\n",
       "      <td>‡§Ø‡§π ‡§ï‡§à ‡§µ‡§ø‡§≠‡§æ‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§Ç‡§ü‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡§ú‡•à‡§∏‡•á - ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ, ‡§ï‡§ø‡§§‡§æ‡§¨‡•Ä ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§Ö‡§•‡§µ‡§æ ‡§Ö...</td>\n",
       "      <td>‡§ú‡•à‡§∏‡•á - ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ, ‡§ï‡§ø‡§§‡§æ‡§¨‡•Ä ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§Ö‡§•‡§µ‡§æ ‡§Ü...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Input sentence  \\\n",
       "0                                    ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?   \n",
       "1  ‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡•ã ‡§∏‡•Ä‡§ñ ‡§≤‡•á‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•ã ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ...   \n",
       "2    ‡§Ø‡•á ‡§ï‡•á‡§µ‡§≤ ‡§ï‡§ø‡§§‡§æ‡§¨‡•Ä ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ö‡§∞‡•ç‡§ú‡§® ‡§§‡§ï ‡§π‡•Ä ‡§∏‡§ø‡§Æ‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§   \n",
       "3                ‡§Ø‡§π ‡§ï‡§à ‡§µ‡§ø‡§≠‡§æ‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§Ç‡§ü‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§   \n",
       "4  ‡§ú‡•à‡§∏‡•á - ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ, ‡§ï‡§ø‡§§‡§æ‡§¨‡•Ä ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§Ö‡§•‡§µ‡§æ ‡§Ö...   \n",
       "\n",
       "                                     Output sentence Unnamed: 2  label  \n",
       "0                                    ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?        NaN      1  \n",
       "1  ‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡•ã ‡§∏‡•Ä‡§ñ ‡§≤‡•á‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•ã ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ...        NaN      1  \n",
       "2    ‡§Ø‡•á ‡§ï‡•á‡§µ‡§≤ ‡§ï‡§ø‡§§‡§æ‡§¨‡•Ä ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ö‡§∞‡•ç‡§ú‡§® ‡§§‡§ï ‡§π‡•Ä ‡§∏‡•Ä‡§Æ‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§        NaN      0  \n",
       "3                ‡§Ø‡§π ‡§ï‡§à ‡§µ‡§ø‡§≠‡§æ‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§Ç‡§ü‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§        NaN      1  \n",
       "4  ‡§ú‡•à‡§∏‡•á - ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ, ‡§ï‡§ø‡§§‡§æ‡§¨‡•Ä ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§Ö‡§•‡§µ‡§æ ‡§Ü...        NaN      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and detect column names\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Columns:', list(df.columns))\n",
    "\n",
    "def find_col(candidates):\n",
    "    # Return the first column whose lowercase name contains any candidate substring\n",
    "    lowered = {c: str(c).strip().lower() for c in df.columns}\n",
    "    for key in candidates:\n",
    "        for col, lc in lowered.items():\n",
    "            if key in lc:\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "input_col = find_col(['input', 'source', 'src'])\n",
    "output_col = find_col(['output', 'target', 'tgt', 'reference', 'gold'])\n",
    "\n",
    "if input_col is None or output_col is None:\n",
    "    # Fallback to first two columns\n",
    "    cols = list(df.columns)\n",
    "    if len(cols) < 2:\n",
    "        raise ValueError('train.csv must have at least two columns (input and output).')\n",
    "    input_col, output_col = cols[0], cols[1]\n",
    "    print(f'Warning: Could not auto-detect columns. Using first two columns: {input_col!r}, {output_col!r}')\n",
    "else:\n",
    "    print(f'Detected columns -> input: {input_col!r}, output: {output_col!r}')\n",
    "\n",
    "# Clean and ensure string type\n",
    "df[input_col] = df[input_col].fillna('').astype(str).str.strip()\n",
    "df[output_col] = df[output_col].fillna('').astype(str).str.strip()\n",
    "\n",
    "# Create label: 1 if equal, else 0\n",
    "df['label'] = (df[input_col] == df[output_col]).astype(int)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479, 120, np.float64(0.09682804674457429))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/validation split\n",
    "X_pair = (df[input_col] + ' [SEP] ' + df[output_col]).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_pair, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y if len(np.unique(y)) > 1 else None\n",
    ")\n",
    "len(X_train), len(X_val), np.mean(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for metrics\n",
    "def print_metrics(y_true, y_pred, y_proba=None, title=None):\n",
    "    if title:\n",
    "        print('='*len(title))\n",
    "        print(title)\n",
    "        print('='*len(title))\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1b = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    print(f'F1-macro: {f1m:.4f} | F1-binary(positive=1): {f1b:.4f}')\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_proba)\n",
    "            print(f'ROC-AUC: {auc:.4f}')\n",
    "        except Exception as e:\n",
    "            print('ROC-AUC unavailable:', e)\n",
    "# print('Classification Report:\\n', classification_report(y_true, y_pred, zero_division=0))\n",
    "# print('Confusion Matrix:\\n', confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "Baseline: Exact Equality Rule\n",
      "=============================\n",
      "Accuracy: 1.0000\n",
      "F1-macro: 1.0000 | F1-binary(positive=1): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Baseline rule: exact match on raw strings\n",
    "def rule_predict(batch_pairs):\n",
    "    preds = []\n",
    "    for s in batch_pairs:\n",
    "        try:\n",
    "            a, b = s.split(' [SEP] ', 1)\n",
    "        except ValueError:\n",
    "            # if separator missing, treat as not equal\n",
    "            preds.append(0)\n",
    "            continue\n",
    "        preds.append(1 if a.strip() == b.strip() else 0)\n",
    "    return np.array(preds)\n",
    "\n",
    "rule_val = rule_predict(X_val)\n",
    "print_metrics(y_val, rule_val, title='Baseline: Exact Equality Rule')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "TF-IDF + Logistic Regression\n",
      "============================\n",
      "Accuracy: 0.9000\n",
      "F1-macro: 0.5982 | F1-binary(positive=1): 0.2500\n",
      "ROC-AUC: 0.7330\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (char n-grams) + Logistic Regression\n",
    "tfidf_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=(2, 5), min_df=2)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear'))\n",
    "])\n",
    "\n",
    "tfidf_lr.fit(X_train, y_train)\n",
    "pred_val = tfidf_lr.predict(X_val)\n",
    "proba_val = None\n",
    "if hasattr(tfidf_lr, 'predict_proba'):\n",
    "    proba_val = tfidf_lr.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print_metrics(y_val, pred_val, proba_val, title='TF-IDF + Logistic Regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: D:\\CODING\\IndicGEC2025\\Hindi\\models\\binary_eq_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path('models') / 'binary_eq_model.joblib'\n",
    "joblib.dump(tfidf_lr, MODEL_PATH)\n",
    "print(f'Model saved to: {MODEL_PATH.resolve()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§Ø‡§π ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§π‡•à‡•§ | ‡§Ø‡§π ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§π‡•à‡•§ -> 0\n",
      "‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§∞‡§æ‡§π‡•Å‡§≤ ‡§π‡•à‡•§ | ‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§∞‡•ã‡§π‡§ø‡§§ ‡§π‡•à‡•§ -> 0\n"
     ]
    }
   ],
   "source": [
    "# Inference helper\n",
    "def predict_equal(input_sentence: str, output_sentence: str) -> int:\n",
    "    s = f'{input_sentence.strip()} [SEP] {output_sentence.strip()}'\n",
    "    return int(tfidf_lr.predict([s])[0])\n",
    "\n",
    "# Example usage:\n",
    "examples = [\n",
    "    ('‡§Ø‡§π ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§π‡•à‡•§', '‡§Ø‡§π ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§π‡•à‡•§'),\n",
    "    ('‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§∞‡§æ‡§π‡•Å‡§≤ ‡§π‡•à‡•§', '‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§∞‡•ã‡§π‡§ø‡§§ ‡§π‡•à‡•§'),\n",
    "]\n",
    "for a, b in examples:\n",
    "    print(a, '|', b, '->', predict_equal(a, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f6fe17",
   "metadata": {},
   "source": [
    "# Model Evaluation on Development Set\n",
    "\n",
    "Let's test our trained model on the development set (`dev.csv`) and calculate the GLEU score for comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14174482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GLEU score dependencies\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.translate.gleu_score import sentence_gleu\n",
    "    print(\"NLTK GLEU already available\")\n",
    "except ImportError:\n",
    "    print(\"Installing NLTK for GLEU score calculation...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'nltk'])\n",
    "    import nltk\n",
    "    from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process development set\n",
    "DEV_PATH = 'dev.csv'\n",
    "\n",
    "# Check if dev.csv exists\n",
    "if not Path(DEV_PATH).exists():\n",
    "    print(f\"‚ùå {DEV_PATH} not found in current directory.\")\n",
    "    print(f\"Current directory: {Path.cwd()}\")\n",
    "    print(f\"Available files: {list(Path('.').glob('*.csv'))}\")\n",
    "    raise FileNotFoundError(f\"Please place {DEV_PATH} in the same folder as this notebook.\")\n",
    "\n",
    "# Load development data\n",
    "print(f\"üìä Loading development set: {DEV_PATH}\")\n",
    "dev_df = pd.read_csv(DEV_PATH)\n",
    "print(f\"Development set shape: {dev_df.shape}\")\n",
    "print(f\"Columns: {list(dev_df.columns)}\")\n",
    "\n",
    "# Auto-detect columns for dev set (same logic as training)\n",
    "def autodetect_dev_cols(df):\n",
    "    def find_col(df, candidates):\n",
    "        lowered = {c: str(c).strip().lower() for c in df.columns}\n",
    "        for key in candidates:\n",
    "            for col, lc in lowered.items():\n",
    "                if key in lc:\n",
    "                    return col\n",
    "        return None\n",
    "    \n",
    "    ic = find_col(df, ['input', 'source', 'src']) or df.columns[0]\n",
    "    oc = find_col(df, ['output', 'target', 'tgt', 'reference', 'gold']) or df.columns[1]\n",
    "    return ic, oc\n",
    "\n",
    "dev_input_col, dev_output_col = autodetect_dev_cols(dev_df)\n",
    "print(f\"üéØ Detected columns -> input: '{dev_input_col}', output: '{dev_output_col}'\")\n",
    "\n",
    "# Clean development data\n",
    "dev_df[dev_input_col] = dev_df[dev_input_col].fillna('').astype(str).str.strip()\n",
    "dev_df[dev_output_col] = dev_df[dev_output_col].fillna('').astype(str).str.strip()\n",
    "\n",
    "# Create labels for dev set\n",
    "dev_df['label_true'] = (dev_df[dev_input_col] == dev_df[dev_output_col]).astype(int)\n",
    "\n",
    "print(f\"üìà Dev set statistics:\")\n",
    "print(f\"  - Total samples: {len(dev_df)}\")\n",
    "print(f\"  - Identical pairs (label=1): {dev_df['label_true'].sum()} ({dev_df['label_true'].mean():.2%})\")\n",
    "print(f\"  - Different pairs (label=0): {(1-dev_df['label_true']).sum()} ({(1-dev_df['label_true']).mean():.2%})\")\n",
    "\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2716504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on development set\n",
    "print(\"ü§ñ Generating predictions on development set...\")\n",
    "\n",
    "# Prepare input pairs for model prediction\n",
    "dev_X_pair = (dev_df[dev_input_col] + ' [SEP] ' + dev_df[dev_output_col]).values\n",
    "dev_y_true = dev_df['label_true'].values\n",
    "\n",
    "# Get model predictions\n",
    "dev_y_pred = tfidf_lr.predict(dev_X_pair)\n",
    "dev_y_proba = tfidf_lr.predict_proba(dev_X_pair)[:, 1]\n",
    "\n",
    "# Add predictions to dataframe\n",
    "dev_df['label_pred'] = dev_y_pred\n",
    "dev_df['confidence'] = dev_y_proba\n",
    "\n",
    "# Binary classification metrics\n",
    "print(\"üìä Binary Classification Results on Development Set:\")\n",
    "print_metrics(dev_y_true, dev_y_pred, dev_y_proba, title='Development Set Performance')\n",
    "\n",
    "# Detailed breakdown\n",
    "print(f\"\\nüîç Detailed Analysis:\")\n",
    "print(f\"  - Correct predictions: {(dev_y_true == dev_y_pred).sum()}/{len(dev_y_true)}\")\n",
    "print(f\"  - True Positives (correctly identified identical): {((dev_y_true == 1) & (dev_y_pred == 1)).sum()}\")\n",
    "print(f\"  - True Negatives (correctly identified different): {((dev_y_true == 0) & (dev_y_pred == 0)).sum()}\")\n",
    "print(f\"  - False Positives (incorrectly said identical): {((dev_y_true == 0) & (dev_y_pred == 1)).sum()}\")\n",
    "print(f\"  - False Negatives (incorrectly said different): {((dev_y_true == 1) & (dev_y_pred == 0)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLEU Score Calculation\n",
    "print(\"üìè Calculating GLEU Scores...\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Simple tokenization for Hindi text\"\"\"\n",
    "    # Basic tokenization - splits on whitespace and common punctuation\n",
    "    import re\n",
    "    # Split on whitespace and keep punctuation as separate tokens\n",
    "    tokens = re.findall(r'\\S+', str(text).strip())\n",
    "    return tokens\n",
    "\n",
    "def calculate_gleu_scores(references, hypotheses):\n",
    "    \"\"\"Calculate GLEU scores for a set of reference-hypothesis pairs\"\"\"\n",
    "    gleu_scores = []\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        # Tokenize both reference and hypothesis\n",
    "        ref_tokens = tokenize_text(ref)\n",
    "        hyp_tokens = tokenize_text(hyp)\n",
    "        \n",
    "        # Calculate sentence-level GLEU\n",
    "        # sentence_gleu expects reference as list of token lists, hypothesis as token list\n",
    "        try:\n",
    "            gleu = sentence_gleu([ref_tokens], hyp_tokens)\n",
    "            gleu_scores.append(gleu)\n",
    "        except Exception as e:\n",
    "            # In case of any tokenization issues, use 0.0\n",
    "            gleu_scores.append(0.0)\n",
    "    \n",
    "    return gleu_scores\n",
    "\n",
    "# Calculate GLEU scores for all sentence pairs\n",
    "print(\"üî§ Tokenizing and calculating GLEU scores...\")\n",
    "\n",
    "# For GLEU, we compare input vs output sentences (regardless of our model's prediction)\n",
    "references = dev_df[dev_input_col].tolist()  # Original sentences\n",
    "hypotheses = dev_df[dev_output_col].tolist()  # Corrected/target sentences\n",
    "\n",
    "# Calculate GLEU scores\n",
    "gleu_scores = calculate_gleu_scores(references, hypotheses)\n",
    "dev_df['gleu_score'] = gleu_scores\n",
    "\n",
    "# Overall GLEU statistics\n",
    "mean_gleu = np.mean(gleu_scores)\n",
    "median_gleu = np.median(gleu_scores)\n",
    "std_gleu = np.std(gleu_scores)\n",
    "\n",
    "print(f\"\\nüìä GLEU Score Results on Development Set:\")\n",
    "print(f\"  üìà Mean GLEU Score: {mean_gleu:.4f}\")\n",
    "print(f\"  üìä Median GLEU Score: {median_gleu:.4f}\")\n",
    "print(f\"  üìè Standard Deviation: {std_gleu:.4f}\")\n",
    "print(f\"  üéØ Min GLEU Score: {min(gleu_scores):.4f}\")\n",
    "print(f\"  üöÄ Max GLEU Score: {max(gleu_scores):.4f}\")\n",
    "\n",
    "# GLEU score distribution\n",
    "perfect_matches = sum(1 for score in gleu_scores if score >= 0.99)\n",
    "high_scores = sum(1 for score in gleu_scores if 0.8 <= score < 0.99)\n",
    "medium_scores = sum(1 for score in gleu_scores if 0.5 <= score < 0.8)\n",
    "low_scores = sum(1 for score in gleu_scores if score < 0.5)\n",
    "\n",
    "print(f\"\\nüìã GLEU Score Distribution:\")\n",
    "print(f\"  üéØ Perfect/Near-perfect (‚â•0.99): {perfect_matches} ({perfect_matches/len(gleu_scores):.1%})\")\n",
    "print(f\"  ‚úÖ High similarity (0.8-0.99): {high_scores} ({high_scores/len(gleu_scores):.1%})\")\n",
    "print(f\"  ‚ö†Ô∏è Medium similarity (0.5-0.8): {medium_scores} ({medium_scores/len(gleu_scores):.1%})\")\n",
    "print(f\"  ‚ùå Low similarity (<0.5): {low_scores} ({low_scores/len(gleu_scores):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6914d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis: Model Predictions vs GLEU Scores\n",
    "print(\"üîç Analyzing correlation between model predictions and GLEU scores...\")\n",
    "\n",
    "# Group by model predictions\n",
    "identical_pairs = dev_df[dev_df['label_pred'] == 1]\n",
    "different_pairs = dev_df[dev_df['label_pred'] == 0]\n",
    "\n",
    "print(f\"\\nüìä GLEU Scores by Model Prediction:\")\n",
    "print(f\"  üü¢ Pairs predicted as IDENTICAL (label=1): {len(identical_pairs)} samples\")\n",
    "print(f\"    - Mean GLEU: {identical_pairs['gleu_score'].mean():.4f}\")\n",
    "print(f\"    - Median GLEU: {identical_pairs['gleu_score'].median():.4f}\")\n",
    "print(f\"    - Std GLEU: {identical_pairs['gleu_score'].std():.4f}\")\n",
    "\n",
    "print(f\"  üî¥ Pairs predicted as DIFFERENT (label=0): {len(different_pairs)} samples\")\n",
    "print(f\"    - Mean GLEU: {different_pairs['gleu_score'].mean():.4f}\")\n",
    "print(f\"    - Median GLEU: {different_pairs['gleu_score'].median():.4f}\")\n",
    "print(f\"    - Std GLEU: {different_pairs['gleu_score'].std():.4f}\")\n",
    "\n",
    "# Correlation coefficient\n",
    "correlation = np.corrcoef(dev_df['label_pred'], dev_df['gleu_score'])[0, 1]\n",
    "print(f\"\\nüîó Correlation between model predictions and GLEU scores: {correlation:.4f}\")\n",
    "\n",
    "# Performance on perfect vs imperfect matches\n",
    "perfect_gleu_mask = dev_df['gleu_score'] >= 0.99\n",
    "imperfect_gleu_mask = dev_df['gleu_score'] < 0.99\n",
    "\n",
    "perfect_accuracy = (dev_df[perfect_gleu_mask]['label_true'] == dev_df[perfect_gleu_mask]['label_pred']).mean()\n",
    "imperfect_accuracy = (dev_df[imperfect_gleu_mask]['label_true'] == dev_df[imperfect_gleu_mask]['label_pred']).mean()\n",
    "\n",
    "print(f\"\\nüéØ Model Accuracy Analysis:\")\n",
    "print(f\"  ‚ú® On perfect GLEU matches (‚â•0.99): {perfect_accuracy:.4f} ({perfect_gleu_mask.sum()} samples)\")\n",
    "print(f\"  ‚ö° On imperfect GLEU matches (<0.99): {imperfect_accuracy:.4f} ({imperfect_gleu_mask.sum()} samples)\")\n",
    "\n",
    "print(f\"\\nüìà Summary Report:\")\n",
    "print(f\"  üéØ Overall Development Set Accuracy: {(dev_y_true == dev_y_pred).mean():.4f}\")\n",
    "print(f\"  üìè Mean GLEU Score: {mean_gleu:.4f}\")\n",
    "print(f\"  üìä Total Samples Evaluated: {len(dev_df)}\")\n",
    "print(f\"  üîó Prediction-GLEU Correlation: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Analysis: Show some examples\n",
    "print(\"üîç Sample Analysis - Examples from Development Set:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show some interesting examples\n",
    "sample_indices = [0, 1, 2, 3, 4]  # First 5 examples\n",
    "if len(dev_df) > 10:\n",
    "    # Add some random samples if dataset is large enough\n",
    "    sample_indices.extend(np.random.choice(range(5, len(dev_df)), size=min(5, len(dev_df)-5), replace=False))\n",
    "\n",
    "for i in sample_indices:\n",
    "    row = dev_df.iloc[i]\n",
    "    print(f\"\\nüìù Example {i+1}:\")\n",
    "    print(f\"  Input:  '{row[dev_input_col]}'\")\n",
    "    print(f\"  Output: '{row[dev_output_col]}'\")\n",
    "    print(f\"  True Label: {row['label_true']} | Predicted: {row['label_pred']} | Confidence: {row['confidence']:.3f}\")\n",
    "    print(f\"  GLEU Score: {row['gleu_score']:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if row['label_true'] == row['label_pred']:\n",
    "        status = \"‚úÖ CORRECT\"\n",
    "    else:\n",
    "        status = \"‚ùå INCORRECT\"\n",
    "    \n",
    "    if row['gleu_score'] >= 0.99:\n",
    "        gleu_status = \"üéØ Perfect match\"\n",
    "    elif row['gleu_score'] >= 0.8:\n",
    "        gleu_status = \"‚úÖ High similarity\"\n",
    "    elif row['gleu_score'] >= 0.5:\n",
    "        gleu_status = \"‚ö†Ô∏è Medium similarity\"\n",
    "    else:\n",
    "        gleu_status = \"‚ùå Low similarity\"\n",
    "    \n",
    "    print(f\"  Status: {status} | GLEU: {gleu_status}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Save results to CSV\n",
    "results_path = 'dev_results_with_gleu.csv'\n",
    "dev_df.to_csv(results_path, index=False)\n",
    "print(f\"üíæ Results saved to: {results_path}\")\n",
    "print(f\"   - Columns: {list(dev_df.columns)}\")\n",
    "print(f\"   - Rows: {len(dev_df)}\")\n",
    "\n",
    "print(f\"\\nüéâ Evaluation Complete!\")\n",
    "print(f\"  üìä Mean GLEU Score: {mean_gleu:.4f}\")\n",
    "print(f\"  üéØ Classification Accuracy: {(dev_y_true == dev_y_pred).mean():.4f}\")\n",
    "print(f\"  üìà Total Samples: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Batch inference on a file\n",
    "If you later have a file like `test.csv` with the same two columns, you can run:\n",
    "\n",
    "1. Load it with `pd.read_csv('test.csv')`.\n",
    "2. Auto-detect the columns the same way as above.\n",
    "3. Build pairs with `' [SEP] '`.\n",
    "4. Use `tfidf_lr.predict(pairs)` to get labels (1 if equal, else 0).\n",
    "5. Save the predictions to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Batch inference template\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "# def autodetect_cols(df):\n",
    "#     def find_col(df, candidates):\n",
    "#         lowered = {c: str(c).strip().lower() for c in df.columns}\n",
    "#         for key in candidates:\n",
    "#             for col, lc in lowered.items():\n",
    "#                 if key in lc:\n",
    "#                     return col\n",
    "#         return None\n",
    "#     ic = find_col(df, ['input', 'source', 'src']) or df.columns[0]\n",
    "#     oc = find_col(df, ['output', 'target', 'tgt', 'reference', 'gold']) or df.columns[1]\n",
    "#     return ic, oc\n",
    "#\n",
    "# ic, oc = autodetect_cols(test_df)\n",
    "# test_df[ic] = test_df[ic].fillna('').astype(str).str.strip()\n",
    "# test_df[oc] = test_df[oc].fillna('').astype(str).str.strip()\n",
    "# pairs = (test_df[ic] + ' [SEP] ' + test_df[oc]).values\n",
    "# preds = tfidf_lr.predict(pairs).astype(int)\n",
    "# out = test_df.copy()\n",
    "# out['label_pred'] = preds\n",
    "# out.to_csv('predictions.csv', index=False)\n",
    "# print('Saved predictions.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IndicGEC2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

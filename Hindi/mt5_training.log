2025-10-01 00:14:49,950 - INFO - Loading datasets...
2025-10-01 00:14:50,096 - INFO - Loaded training data: (10600, 3)
2025-10-01 00:14:50,099 - INFO - Using columns: 'Input sentence' -> 'Output sentence'
2025-10-01 00:14:50,871 - INFO - Cleaned training data: 5272 samples
2025-10-01 00:14:50,877 - INFO - Loaded dev data: 107 samples
2025-10-01 00:14:50,878 - INFO - 
Data composition:
2025-10-01 00:14:50,879 - INFO - Train: 5272 samples (2444 identical)
2025-10-01 00:14:50,879 - INFO - Dev: 107 samples (24 identical)
2025-10-01 00:14:50,879 - INFO - 
Sample corrections:
2025-10-01 00:14:50,890 - INFO - Loading mT5 model and tokenizer...
2025-10-01 00:14:53,561 - INFO - Tokenizer loaded: vocab size = 250100
2025-10-01 00:14:59,029 - INFO - Gradient checkpointing enabled
2025-10-01 00:14:59,348 - INFO - Model loaded: 300.2M parameters
2025-10-01 00:14:59,349 - INFO - GPU memory allocated: 0.57 GB
2025-10-01 00:14:59,349 - INFO - Preparing datasets for training...
2025-10-01 00:15:01,105 - INFO - Tokenized datasets: train=5272, dev=107
2025-10-01 00:15:01,106 - ERROR - Error during execution: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-10-01 00:15:41,056 - INFO - Loading datasets...
2025-10-01 00:15:41,191 - INFO - Loaded training data: (10600, 3)
2025-10-01 00:15:41,192 - INFO - Using columns: 'Input sentence' -> 'Output sentence'
2025-10-01 00:15:41,954 - INFO - Cleaned training data: 5272 samples
2025-10-01 00:15:41,960 - INFO - Loaded dev data: 107 samples
2025-10-01 00:15:41,962 - INFO - 
Data composition:
2025-10-01 00:15:41,962 - INFO - Train: 5272 samples (2444 identical)
2025-10-01 00:15:41,963 - INFO - Dev: 107 samples (24 identical)
2025-10-01 00:15:41,963 - INFO - 
Sample corrections:
2025-10-01 00:15:41,973 - INFO - Loading mT5 model and tokenizer...
2025-10-01 00:15:44,531 - INFO - Tokenizer loaded: vocab size = 250100
2025-10-01 00:15:50,128 - INFO - Gradient checkpointing enabled
2025-10-01 00:15:50,486 - INFO - Model loaded: 300.2M parameters
2025-10-01 00:15:50,492 - INFO - GPU memory allocated: 0.57 GB
2025-10-01 00:15:50,492 - INFO - Preparing datasets for training...
2025-10-01 00:15:53,312 - INFO - Tokenized datasets: train=5272, dev=107
2025-10-01 00:15:53,312 - ERROR - Error during execution: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: epoch
2025-10-01 00:16:44,470 - INFO - Loading datasets...
2025-10-01 00:16:44,611 - INFO - Loaded training data: (10600, 3)
2025-10-01 00:16:44,612 - INFO - Using columns: 'Input sentence' -> 'Output sentence'
2025-10-01 00:16:45,378 - INFO - Cleaned training data: 5272 samples
2025-10-01 00:16:45,384 - INFO - Loaded dev data: 107 samples
2025-10-01 00:16:45,386 - INFO - 
Data composition:
2025-10-01 00:16:45,387 - INFO - Train: 5272 samples (2444 identical)
2025-10-01 00:16:45,387 - INFO - Dev: 107 samples (24 identical)
2025-10-01 00:16:45,387 - INFO - 
Sample corrections:
2025-10-01 00:16:45,398 - INFO - Loading mT5 model and tokenizer...
2025-10-01 00:16:48,048 - INFO - Tokenizer loaded: vocab size = 250100
2025-10-01 00:16:53,334 - INFO - Gradient checkpointing enabled
2025-10-01 00:16:53,639 - INFO - Model loaded: 300.2M parameters
2025-10-01 00:16:53,640 - INFO - GPU memory allocated: 0.57 GB
2025-10-01 00:16:53,640 - INFO - Preparing datasets for training...
2025-10-01 00:16:55,338 - INFO - Tokenized datasets: train=5272, dev=107
2025-10-01 00:16:55,557 - INFO - Starting training...
2025-10-01 00:16:55,557 - INFO - Effective batch size: 16
2025-10-01 00:16:55,558 - INFO - Total training steps: 1645
2025-10-01 00:16:57,783 - ERROR - Training failed: Attempting to unscale FP16 gradients.
2025-10-01 00:16:57,783 - ERROR - Error during execution: Attempting to unscale FP16 gradients.
2025-10-01 00:21:30,630 - INFO - Loading datasets...
2025-10-01 00:21:30,783 - INFO - Loaded training data: (10600, 3)
2025-10-01 00:21:30,787 - INFO - Using columns: 'Input sentence' -> 'Output sentence'
2025-10-01 00:21:31,596 - INFO - Cleaned training data: 5272 samples
2025-10-01 00:21:31,603 - INFO - Loaded dev data: 107 samples
2025-10-01 00:21:31,606 - INFO - 
Data composition:
2025-10-01 00:21:31,606 - INFO - Train: 5272 samples (2444 identical)
2025-10-01 00:21:31,607 - INFO - Dev: 107 samples (24 identical)
2025-10-01 00:21:31,607 - INFO - 
Sample corrections:
2025-10-01 00:21:31,620 - INFO - Loading mT5 model and tokenizer...
2025-10-01 00:21:34,221 - INFO - Tokenizer loaded: vocab size = 250100
2025-10-01 00:21:39,329 - INFO - Gradient checkpointing enabled
2025-10-01 00:21:40,363 - INFO - Model loaded: 300.2M parameters
2025-10-01 00:21:40,365 - INFO - GPU memory allocated: 1.12 GB
2025-10-01 00:21:40,365 - INFO - Preparing datasets for training...
2025-10-01 00:21:42,091 - INFO - Tokenized datasets: train=5272, dev=107
2025-10-01 00:21:42,382 - INFO - Starting training...
2025-10-01 00:21:42,383 - INFO - Effective batch size: 16
2025-10-01 00:21:42,383 - INFO - Total training steps: 1645
2025-10-01 00:29:36,521 - ERROR - Training failed: piece id is out of range.
2025-10-01 00:29:36,532 - ERROR - Error during execution: piece id is out of range.

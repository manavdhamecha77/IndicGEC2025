{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ad4ea3",
   "metadata": {},
   "source": [
    "# IndicBART: Grammar Error Correction for Indian Languages\n",
    "\n",
    "This notebook implements grammar error correction using IndicBART models for multiple Indian languages including Hindi, Bengali, Malayalam, Tamil, Telugu, and others.\n",
    "\n",
    "##  Environment Setup Complete!\n",
    "\n",
    "**Successfully installed packages in virtual environment:**\n",
    "- **PyTorch 2.8.0+cu129** - Latest PyTorch with CUDA 12.9 support\n",
    "- **Transformers 4.56.2** - Hugging Face Transformers library  \n",
    "- **Additional packages**: datasets, evaluate, nltk, pandas, numpy, tqdm\n",
    "\n",
    "**Hardware detected:**\n",
    "- **GPU**: NVIDIA GeForce RTX 4050 Laptop GPU (6GB VRAM)\n",
    "- **CUDA**: Available and working properly\n",
    "\n",
    "##  Features:\n",
    "- Multi-language support using IndicBART\n",
    "- Unified tokenization approach with `AutoModelForSeq2SeqLM` and `AutoTokenizer`\n",
    "- Batch processing capabilities\n",
    "- GLEU score evaluation\n",
    "- Easy language switching\n",
    "- GPU acceleration for faster inference\n",
    "\n",
    "##  Issue Fixed:\n",
    "- **Unicode encoding error**: Removed problematic Unicode characters (emojis) that were causing tokenization errors\n",
    "- **Virtual environment**: All packages now properly installed and working\n",
    "- **Ready to proceed**: You can now run all subsequent cells without issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual Environment Setup - Verification\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "print(\" Environment Verification:\")\n",
    "print(f\" Python: {sys.executable}\")\n",
    "\n",
    "# Check virtual environment\n",
    "in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)\n",
    "print(f\" Virtual Environment: {' Active' if in_venv else ' Not active'}\")\n",
    "\n",
    "print(\"\\n Package Status:\")\n",
    "\n",
    "# Test core packages\n",
    "packages_status = {}\n",
    "\n",
    "# Test PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    packages_status['torch'] = {\n",
    "        'status': 'success',\n",
    "        'version': torch.__version__,\n",
    "        'cuda': torch.cuda.is_available()\n",
    "    }\n",
    "    print(f\" PyTorch {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"    CUDA: Available - {torch.cuda.get_device_name()}\")\n",
    "        print(f\"    GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(f\"    CUDA: Not available (CPU only)\")\n",
    "except Exception as e:\n",
    "    packages_status['torch'] = {'status': 'error', 'error': str(e)}\n",
    "    print(f\" PyTorch: {str(e)}\")\n",
    "\n",
    "# Test Transformers \n",
    "try:\n",
    "    import transformers\n",
    "    packages_status['transformers'] = {\n",
    "        'status': 'success',\n",
    "        'version': transformers.__version__\n",
    "    }\n",
    "    print(f\" Transformers {transformers.__version__}\")\n",
    "except Exception as e:\n",
    "    packages_status['transformers'] = {'status': 'error', 'error': str(e)}\n",
    "    print(f\" Transformers: {str(e)}\")\n",
    "\n",
    "# Test other required packages\n",
    "other_packages = ['evaluate', 'nltk', 'pandas', 'numpy', 'tqdm']\n",
    "all_others_ok = True\n",
    "\n",
    "for pkg in other_packages:\n",
    "    try:\n",
    "        module = importlib.import_module(pkg)\n",
    "        version = getattr(module, '__version__', 'Available')\n",
    "        print(f\" {pkg.capitalize()}: {version}\")\n",
    "        packages_status[pkg] = {'status': 'success', 'version': version}\n",
    "    except Exception as e:\n",
    "        print(f\" {pkg.capitalize()}: {str(e)}\")\n",
    "        packages_status[pkg] = {'status': 'error', 'error': str(e)}\n",
    "        all_others_ok = False\n",
    "\n",
    "# Final status\n",
    "torch_ok = packages_status.get('torch', {}).get('status') == 'success'\n",
    "transformers_ok = packages_status.get('transformers', {}).get('status') == 'success'\n",
    "\n",
    "print(f\"\\n Final Status:\")\n",
    "if torch_ok and transformers_ok and all_others_ok:\n",
    "    print(f\" SUCCESS! All packages ready in virtual environment!\")\n",
    "    print(f\" Ready for IndicBART multi-language grammar correction!\")\n",
    "    \n",
    "    # Show device info\n",
    "    if torch_ok:\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"  Device: {device.upper()}\")\n",
    "        \n",
    "elif torch_ok and transformers_ok:\n",
    "    print(f\" Core packages (PyTorch + Transformers) ready!\")\n",
    "    print(f\"  Some optional packages may need attention\")\n",
    "    print(f\" You can proceed with the notebook\")\n",
    "else:\n",
    "    missing = []\n",
    "    if not torch_ok:\n",
    "        missing.append(\"PyTorch\")\n",
    "    if not transformers_ok:\n",
    "        missing.append(\"Transformers\")\n",
    "    print(f\" Missing core packages: {', '.join(missing)}\")\n",
    "    print(f\" Please install missing packages before continuing\")\n",
    "\n",
    "# Save status for next cells\n",
    "globals()['_package_status'] = packages_status\n",
    "print(f\"\\n Environment check complete! You can proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries - FRESH START after kernel restart\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Starting fresh imports after kernel restart...\")\n",
    "\n",
    "# Import PyTorch FIRST and verify it's working\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"   Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"   Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"   Device name: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Clear any cached transformers modules and import fresh\n",
    "import sys\n",
    "transformers_modules = [m for m in sys.modules.keys() if m.startswith('transformers')]\n",
    "for module in transformers_modules:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Now import transformers with PyTorch already loaded\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Verify PyTorch is detected by transformers\n",
    "from transformers.utils import is_torch_available\n",
    "print(f\"PyTorch detected by transformers: {is_torch_available()}\")\n",
    "\n",
    "if not is_torch_available():\n",
    "    raise ImportError(\"PyTorch not detected by transformers - please restart kernel\")\n",
    "\n",
    "# Now safe to import the model classes\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import pipeline, set_seed\n",
    "print(\"Model classes imported successfully!\")\n",
    "\n",
    "# Test that the classes are real, not DummyObjects\n",
    "print(f\"   AutoModelForSeq2SeqLM type: {type(AutoModelForSeq2SeqLM)}\")\n",
    "print(f\"   AutoTokenizer type: {type(AutoTokenizer)}\")\n",
    "\n",
    "# Additional imports for evaluation\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\" GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\" Available Memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\n ALL IMPORTS SUCCESSFUL! Ready for IndicBART!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-language IndicBART Configuration - CORRECTED MODEL NAMES\n",
    "class IndicBARTConfig:\n",
    "    \"\"\"Configuration class for IndicBART models across different Indian languages\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Updated language configurations with correct model paths\n",
    "        # IndicBART uses a single multilingual model for all Indian languages\n",
    "        self.language_configs = {\n",
    "            'hindi': {\n",
    "                'name': 'Hindi',\n",
    "                'code': 'hi',\n",
    "                'model_name': 'ai4bharat/IndicBART',  \n",
    "                'tokenizer_name': 'ai4bharat/IndicBART',\n",
    "                'data_folder': 'Hindi',\n",
    "                'script': 'Devanagari',\n",
    "                'prefix': 'hi'  \n",
    "            },\n",
    "            'bengali': {\n",
    "                'name': 'Bengali', \n",
    "                'code': 'bn',\n",
    "                'model_name': 'ai4bharat/IndicBART',\n",
    "                'tokenizer_name': 'ai4bharat/IndicBART',\n",
    "                'data_folder': 'Bangla',\n",
    "                'script': 'Bengali',\n",
    "                'prefix': 'bn'\n",
    "            },\n",
    "            'malayalam': {\n",
    "                'name': 'Malayalam',\n",
    "                'code': 'ml', \n",
    "                'model_name': 'ai4bharat/IndicBART',\n",
    "                'tokenizer_name': 'ai4bharat/IndicBART',\n",
    "                'data_folder': 'Malayalam',\n",
    "                'script': 'Malayalam',\n",
    "                'prefix': 'ml'\n",
    "            },\n",
    "            'tamil': {\n",
    "                'name': 'Tamil',\n",
    "                'code': 'ta',\n",
    "                'model_name': 'ai4bharat/IndicBART',\n",
    "                'tokenizer_name': 'ai4bharat/IndicBART',\n",
    "                'data_folder': 'Tamil',\n",
    "                'script': 'Tamil',\n",
    "                'prefix': 'ta'\n",
    "            },\n",
    "            'telugu': {\n",
    "                'name': 'Telugu',\n",
    "                'code': 'te',\n",
    "                'model_name': 'ai4bharat/IndicBART',\n",
    "                'tokenizer_name': 'ai4bharat/IndicBART',\n",
    "                'data_folder': 'Telugu', \n",
    "                'script': 'Telugu',\n",
    "                'prefix': 'te'\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def get_config(self, language):\n",
    "        \"\"\"Get configuration for a specific language\"\"\"\n",
    "        return self.language_configs.get(language.lower(), None)\n",
    "    \n",
    "    def list_languages(self):\n",
    "        \"\"\"List all available languages\"\"\"\n",
    "        return list(self.language_configs.keys())\n",
    "\n",
    "# Initialize configuration\n",
    "config = IndicBARTConfig()\n",
    "print(\"Available languages (using ai4bharat/IndicBART):\")\n",
    "for lang in config.list_languages():\n",
    "    lang_config = config.get_config(lang)\n",
    "    print(f\"   {lang_config['name']} ({lang_config['code']}) - {lang_config['script']} script\")\n",
    "\n",
    "print(f\"\\n All languages use the same multilingual model: ai4bharat/IndicBART\")\n",
    "print(f\" Language-specific generation controlled by prefixes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594029d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IndicBART Model Manager - Fixed for compatibility\n",
    "class IndicBARTManager:\n",
    "    \"\"\"Manages IndicBART multilingual model for grammar error correction across Indian languages\"\"\"\n",
    "    \n",
    "    def __init__(self, language='hindi'):\n",
    "        self.language = language.lower()\n",
    "        self.config = IndicBARTConfig().get_config(self.language)\n",
    "        \n",
    "        if not self.config:\n",
    "            raise ValueError(f\"Language '{language}' not supported. Available: {IndicBARTConfig().list_languages()}\")\n",
    "        \n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipeline = None\n",
    "        \n",
    "    def load_model(self, force_reload=False):\n",
    "        \"\"\"Load the multilingual IndicBART model and tokenizer\"\"\"\n",
    "        if self.model is not None and not force_reload:\n",
    "            print(f\" IndicBART model already loaded for {self.config['name']}\")\n",
    "            return\n",
    "            \n",
    "        print(f\" Loading IndicBART multilingual model for {self.config['name']}\")\n",
    "        print(f\"   Model: {self.config['model_name']}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the multilingual IndicBART model (simplified for compatibility)\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                self.config['model_name'],\n",
    "                # Use 'dtype' instead of deprecated 'torch_dtype'\n",
    "                dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "                # Remove device_map to avoid accelerate requirement\n",
    "                low_cpu_mem_usage=True  # Memory optimization\n",
    "            )\n",
    "            \n",
    "            # Load the tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config['tokenizer_name']\n",
    "            )\n",
    "            \n",
    "            # Manually move model to device\n",
    "            self.model = self.model.to(device)\n",
    "            \n",
    "            print(f\"   IndicBART model loaded successfully for {self.config['name']}!\")\n",
    "            print(f\"   Model type: {type(self.model).__name__}\")\n",
    "            print(f\"   Tokenizer type: {type(self.tokenizer).__name__}\")\n",
    "            print(f\"   Vocabulary size: {self.tokenizer.vocab_size}\")\n",
    "            print(f\"   Device: {next(self.model.parameters()).device}\")\n",
    "            \n",
    "            # Check model size\n",
    "            param_count = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f\"   Parameters: {param_count / 1e6:.1f}M\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error loading IndicBART model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def create_pipeline(self):\n",
    "        \"\"\"Create a text generation pipeline for the specific language\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        self.pipeline = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=0 if device == \"cuda\" else -1,\n",
    "            # Use 'dtype' instead of deprecated 'torch_dtype'\n",
    "            dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "        )\n",
    "        print(f\" Text generation pipeline created for {self.config['name']}\")\n",
    "        \n",
    "    def correct_text(self, text, max_length=256, num_beams=4, temperature=0.8):\n",
    "        \"\"\"Correct grammar errors in the given text for the specific language\"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.create_pipeline()\n",
    "            \n",
    "        try:\n",
    "            # Simplified input format for IndicBART\n",
    "            # IndicBART is trained for various tasks, try different formats\n",
    "            input_formats = [\n",
    "                f\"Correct: {text.strip()}\",  # Simple correction prompt\n",
    "                f\"{text.strip()}\",         \n",
    "                f\"Grammar correction: {text.strip()}\"  \n",
    "            ]\n",
    "            \n",
    "            best_result = text  # Fallback to original\n",
    "            \n",
    "            for input_text in input_formats:\n",
    "                try:\n",
    "                    # Generate correction\n",
    "                    result = self.pipeline(\n",
    "                        input_text,\n",
    "                        max_length=max_length,\n",
    "                        num_beams=num_beams,\n",
    "                        temperature=temperature,\n",
    "                        do_sample=True,\n",
    "                        early_stopping=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "                    \n",
    "                    corrected_text = result[0]['generated_text'].strip()\n",
    "                    \n",
    "                    # Clean up the output if it includes the input\n",
    "                    for fmt in input_formats:\n",
    "                        if corrected_text.startswith(fmt):\n",
    "                            corrected_text = corrected_text[len(fmt):].strip()\n",
    "                            break\n",
    "                    \n",
    "                    # Use the first successful result\n",
    "                    if corrected_text and corrected_text != input_text:\n",
    "                        best_result = corrected_text\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue  # Try next format\n",
    "            \n",
    "            return best_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error during correction: {str(e)}\")\n",
    "            return text\n",
    "    \n",
    "    def batch_correct(self, texts, max_length=256, batch_size=2):\n",
    "        \"\"\"Correct multiple texts in batches (reduced batch size for memory)\"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.create_pipeline()\n",
    "            \n",
    "        corrected_texts = []\n",
    "\n",
    "        print(f\" Processing {len(texts)} texts in batches of {batch_size}...\")\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Correcting {self.config['name']} texts\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            # Use simple input format for batch processing\n",
    "            inputs = [f\"Correct: {text.strip()}\" for text in batch]\n",
    "            \n",
    "            try:\n",
    "                results = self.pipeline(\n",
    "                    inputs,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=2,  # Reduced for memory\n",
    "                    do_sample=False,  # Deterministic for batch\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                batch_corrections = []\n",
    "                for result, original_input in zip(results, inputs):\n",
    "                    corrected = result['generated_text'].strip()\n",
    "                    \n",
    "                    # Clean up the output\n",
    "                    if corrected.startswith(original_input):\n",
    "                        corrected = corrected[len(original_input):].strip()\n",
    "                    \n",
    "                    batch_corrections.append(corrected)\n",
    "                \n",
    "                corrected_texts.extend(batch_corrections)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Error in batch {i//batch_size + 1}: {str(e)}\")\n",
    "                corrected_texts.extend(batch)  # Return original texts on error\n",
    "                \n",
    "        return corrected_texts\n",
    "\n",
    "# Example usage\n",
    "print(\"   Fixed IndicBART Manager initialized!\")\n",
    "print(\"   Compatible model loading without accelerate\")\n",
    "print(\"   Memory optimized for standard hardware\")\n",
    "print(\"Available languages:\", IndicBARTConfig().list_languages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75487ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-Optimized IndicBART Model Loading (Accelerate-Compatible)\n",
    "print(\" Loading IndicBART model with GPU optimization...\")\n",
    "\n",
    "# Load model and tokenizer with GPU priority\n",
    "try:\n",
    "    print(\" Loading ai4bharat/IndicBART...\")\n",
    "    print(f\" Target device: {device}\")\n",
    "    \n",
    "    # Clear GPU memory first\n",
    "    if device == \"cuda\":\n",
    "        import torch\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\" GPU memory cleared\")\n",
    "        print(f\" Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Load model first\n",
    "    print(\" Loading model...\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        \"ai4bharat/IndicBART\",\n",
    "        dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\" Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained( # Autotokenizer and AlbertTokenizer\n",
    "        \"ai4bharat/IndicBART\",\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"   IndicBART loaded successfully!\")\n",
    "    print(f\"   Model: {type(model).__name__}\")\n",
    "    print(f\"   Device: {next(model.parameters()).device}\")\n",
    "    print(f\"   Data type: {next(model.parameters()).dtype}\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "    print(f\"   Tokenizer: {type(tokenizer).__name__}\")\n",
    "    print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        print(f\"   GPU memory used: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "        print(f\"   GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.1f} GB\")\n",
    "\n",
    "    print(f\"\\n   Testing Hindi grammar correction with proper tokenization:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test with Hindi examples using corrected tokenization\n",
    "    test_sentences = [\n",
    "        \"मै आज घर जाऊंगा\",  # मैं आज घर जाऊंगा  \n",
    "        \"वो बहुत अच्छा लड़का हैं\",  # वह बहुत अच्छा लड़का है\n",
    "        \"हमे यह काम करना चाहिए\"  # हमें यह काम करना चाहिए\n",
    "    ]\n",
    "    \n",
    "    for i, sentence in enumerate(test_sentences, 1):\n",
    "        print(f\"\\n Test {i}:\")\n",
    "        print(f\"  Original: {sentence}\")\n",
    "        \n",
    "        try:\n",
    "            # Fixed tokenization - only return what the model expects\n",
    "            inputs = tokenizer(\n",
    "                sentence, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True,\n",
    "                return_token_type_ids=False,  # Don't return token_type_ids\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Move inputs to device\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate with strict parameters\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_new_tokens=15,  # Short output\n",
    "                    min_length=inputs['input_ids'].shape[1] + 1,\n",
    "                    num_beams=2,\n",
    "                    do_sample=False,\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    repetition_penalty=1.5,\n",
    "                    length_penalty=1.0,\n",
    "                    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the output\n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            print(f\"  Generated: {decoded}\")\n",
    "            print(f\"  Status: {' Generated' if decoded != sentence else 'Same as input'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {str(e)}\")\n",
    "    \n",
    "    # Try simple text-to-text generation with task prompts\n",
    "    print(f\"\\n   Testing with task-specific prompts:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    task_examples = [\n",
    "        (\"Grammar correct: मै आज घर जाऊंगा\", \"Grammar correction task\"),\n",
    "        (\"Fix: वो बहुत अच्छा लड़का हैं\", \"Simple fix prompt\"),\n",
    "        (\"हमे यह काम करना चाहिए\", \"Direct input\")\n",
    "    ]\n",
    "    \n",
    "    for prompt, description in task_examples:\n",
    "        print(f\"\\n {description}:\")\n",
    "        print(f\"  Input: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                return_token_type_ids=False,\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    num_beams=2,\n",
    "                    do_sample=False,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=1.3,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"  Output: {result}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n IndicBART testing complete!\")\n",
    "    print(f\" Model successfully loaded on GPU with {torch.cuda.memory_allocated() / 1024**3:.1f} GB memory used\")\n",
    "    print(f\" Ready for grammar correction tasks\")\n",
    "    \n",
    "    # Set global variables for use in other cells\n",
    "    globals()['model'] = model\n",
    "    globals()['tokenizer'] = tokenizer\n",
    "    \n",
    "    # Create a SIMPLE correction function\n",
    "    def correct_hindi_text(text, max_new_tokens=15):\n",
    "        \"\"\"Simple function to correct Hindi text\"\"\"\n",
    "        try:\n",
    "            # Try with task prompt first\n",
    "            prompt = f\"Grammar correct: {text}\"\n",
    "            inputs = tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                return_token_type_ids=False\n",
    "            )\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=2,\n",
    "                    do_sample=False,\n",
    "                    repetition_penalty=1.3,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean the result\n",
    "            if result.startswith(prompt):\n",
    "                result = result[len(prompt):].strip()\n",
    "            \n",
    "            return result if result else text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in correction: {e}\")\n",
    "            return text\n",
    "    \n",
    "    globals()['correct_hindi_text'] = correct_hindi_text\n",
    "    print(\" Helper function 'correct_hindi_text()' ready!\")\n",
    "    print(\" Try: correct_hindi_text('मै आज घर जाऊंगा')\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Error loading IndicBART: {str(e)}\")\n",
    "    print(\" Please check that all dependencies (sentencepiece, accelerate, protobuf) are installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a477216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning IndicBART for Grammar Error Correction\n",
    "print(\" Setting up IndicBART fine-tuning for grammar error correction\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Import additional training libraries\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set up training parameters\n",
    "LANGUAGE = 'hindi'  # Change this to train on different languages\n",
    "MAX_INPUT_LENGTH = 599\n",
    "MAX_TARGET_LENGTH = 599\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 10\n",
    "WARMUP_STEPS = 500\n",
    "\n",
    "print(f\"  Training Configuration:\")\n",
    "print(f\"   Language: {LANGUAGE}\")\n",
    "print(f\"   Max input length: {MAX_INPUT_LENGTH}\")\n",
    "print(f\"   Max target length: {MAX_TARGET_LENGTH}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Warmup steps: {WARMUP_STEPS}\")\n",
    "\n",
    "# Load and prepare training data\n",
    "def load_training_data(language='hindi'):\n",
    "    \"\"\"Load training data for the specified language\"\"\"\n",
    "    \n",
    "    # Define data folder mapping\n",
    "    folder_mapping = {\n",
    "        'hindi': 'Hindi',\n",
    "        'bengali': 'Bangla', \n",
    "        'malayalam': 'Malayalam',\n",
    "        'tamil': 'Tamil',\n",
    "        'telugu': 'Telugu'\n",
    "    }\n",
    "    \n",
    "    data_folder = folder_mapping.get(language, 'Hindi')\n",
    "    train_file = Path(data_folder) / 'train.csv'\n",
    "    dev_file = Path(data_folder) / 'dev.csv'\n",
    "    \n",
    "    print(f\"\\n Loading data from {data_folder} folder...\")\n",
    "    \n",
    "    # Load training data\n",
    "    if train_file.exists():\n",
    "        train_df = pd.read_csv(train_file)\n",
    "        print(f\" Training data: {len(train_df)} samples\")\n",
    "        print(f\"   Columns: {list(train_df.columns)}\")\n",
    "        \n",
    "        # Auto-detect columns\n",
    "        if 'input' in train_df.columns and 'target' in train_df.columns:\n",
    "            input_col, target_col = 'input', 'target'\n",
    "        elif 'source' in train_df.columns and 'target' in train_df.columns:\n",
    "            input_col, target_col = 'source', 'target'\n",
    "        elif len(train_df.columns) >= 2:\n",
    "            input_col, target_col = train_df.columns[0], train_df.columns[1]\n",
    "        else:\n",
    "            raise ValueError(\"Could not identify input and target columns\")\n",
    "            \n",
    "        print(f\"   Using: '{input_col}' → '{target_col}'\")\n",
    "        \n",
    "        # Clean data\n",
    "        train_df = train_df.dropna(subset=[input_col, target_col])\n",
    "        train_df[input_col] = train_df[input_col].astype(str).str.strip()\n",
    "        train_df[target_col] = train_df[target_col].astype(str).str.strip()\n",
    "        \n",
    "        # Remove empty rows\n",
    "        train_df = train_df[(train_df[input_col] != '') & (train_df[target_col] != '')]\n",
    "        \n",
    "        print(f\"   Cleaned data: {len(train_df)} samples\")\n",
    "        \n",
    "        # Load dev data if available\n",
    "        dev_df = None\n",
    "        if dev_file.exists():\n",
    "            dev_df = pd.read_csv(dev_file)\n",
    "            dev_df = dev_df.dropna(subset=[input_col, target_col])\n",
    "            dev_df[input_col] = dev_df[input_col].astype(str).str.strip()\n",
    "            dev_df[target_col] = dev_df[target_col].astype(str).str.strip()\n",
    "            dev_df = dev_df[(dev_df[input_col] != '') & (dev_df[target_col] != '')]\n",
    "            print(f\" Dev data: {len(dev_df)} samples\")\n",
    "        \n",
    "        return train_df, dev_df, input_col, target_col\n",
    "        \n",
    "    else:\n",
    "        print(f\" Training file not found: {train_file}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Load the data\n",
    "train_df, dev_df, input_col, target_col = load_training_data(LANGUAGE)\n",
    "\n",
    "if train_df is not None:\n",
    "    print(f\"\\n Data Sample:\")\n",
    "    print(f\"   Input:  {train_df[input_col].iloc[0]}\")\n",
    "    print(f\"   Target: {train_df[target_col].iloc[0]}\")\n",
    "    \n",
    "    # Show more samples\n",
    "    print(f\"\\n First 3 training examples:\")\n",
    "    for i in range(min(3, len(train_df))):\n",
    "        print(f\"   {i+1}. Input:  {train_df[input_col].iloc[i]}\")\n",
    "        print(f\"      Target: {train_df[target_col].iloc[i]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\" Could not load training data. Please check file paths and formats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Dataset Preparation\n",
    "print(\" Preparing datasets for training...\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize input and target texts\"\"\"\n",
    "    # Tokenize inputs without token_type_ids\n",
    "    inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "        return_token_type_ids=False  # Explicitly disable token_type_ids\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    targets = tokenizer(\n",
    "        examples['target_text'],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "        return_token_type_ids=False  # Explicitly disable token_type_ids\n",
    "    )\n",
    "    \n",
    "    # Set labels (targets for loss calculation)\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def prepare_datasets(train_df, dev_df, input_col, target_col):\n",
    "    \"\"\"Convert pandas dataframes to HuggingFace datasets\"\"\"\n",
    "    \n",
    "    # Create training dataset\n",
    "    train_data = {\n",
    "        'input_text': train_df[input_col].tolist(),\n",
    "        'target_text': train_df[target_col].tolist()\n",
    "    }\n",
    "    train_dataset = Dataset.from_dict(train_data)\n",
    "    \n",
    "    # Create dev dataset if available\n",
    "    eval_dataset = None\n",
    "    if dev_df is not None:\n",
    "        eval_data = {\n",
    "            'input_text': dev_df[input_col].tolist(),\n",
    "            'target_text': dev_df[target_col].tolist()\n",
    "        }\n",
    "        eval_dataset = Dataset.from_dict(eval_data)\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(\"   Tokenizing training data...\")\n",
    "    train_dataset = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['input_text', 'target_text']\n",
    "    )\n",
    "    \n",
    "    if eval_dataset is not None:\n",
    "        print(\"   Tokenizing evaluation data...\")\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=['input_text', 'target_text']\n",
    "        )\n",
    "    \n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset, eval_dataset = prepare_datasets(train_df, dev_df, input_col, target_col)\n",
    "\n",
    "print(f\" Training dataset: {len(train_dataset)} samples\")\n",
    "if eval_dataset:\n",
    "    print(f\" Evaluation dataset: {len(eval_dataset)} samples\")\n",
    "\n",
    "# Sample tokenized data\n",
    "print(f\"\\n Tokenized sample:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"   Input IDs length: {len(sample['input_ids'])}\")\n",
    "print(f\"   Labels length: {len(sample['labels'])}\")\n",
    "print(f\"   Available keys: {list(sample.keys())}\")\n",
    "\n",
    "# Data collator for padding during training\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    max_length=MAX_INPUT_LENGTH\n",
    ")\n",
    "\n",
    "print(f\" Data collator created for dynamic padding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d194898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Stable Training with Proper Imports\n",
    "print(\" FIXING TRAINING INSTABILITY - STABLE APPROACH V2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Import required modules\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "\n",
    "# Reset model to original state\n",
    "print(\" Resetting model to stable state...\")\n",
    "\n",
    "# Load fresh model to avoid any corruption\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"ai4bharat/IndicBART\",\n",
    "    dtype=torch.float32,  # Use FP32 for stability\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "print(\" Fresh model loaded\")\n",
    "\n",
    "# Stable training configuration\n",
    "STABLE_CONFIG = {\n",
    "    'epochs': 50,  # Reduced for stability\n",
    "    'batch_size': 1,  # Smallest possible batch\n",
    "    'gradient_accumulation_steps': 16,  # Larger accumulation for stability\n",
    "    'learning_rate': 1e-5,  # Much lower learning rate\n",
    "    'warmup_ratio': 0.05,  # Smaller warmup\n",
    "    'weight_decay': 0.001,  # Lower weight decay\n",
    "    'max_grad_norm': 0.5,  # Stricter gradient clipping\n",
    "}\n",
    "\n",
    "print(f\"  Stable Configuration:\")\n",
    "for key, value in STABLE_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Simple, stable training function\n",
    "def stable_train_epoch(model, dataset, optimizer, config, epoch):\n",
    "    \"\"\"Ultra-stable training approach\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    # Create small dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True, \n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # Take only a subset for stability testing\n",
    "    max_batches = 150  # Limit batches for stability\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        total=min(max_batches, len(dataloader)),\n",
    "        desc=f\"Stable Epoch {epoch+1}\"\n",
    "    )\n",
    "    \n",
    "    accumulated_loss = 0\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        if batch_idx >= max_batches:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            # Move to device safely\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Check for valid inputs\n",
    "            if input_ids.numel() == 0 or labels.numel() == 0:\n",
    "                continue\n",
    "                \n",
    "            # Forward pass with error checking\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Check for valid loss\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"    Skipping batch {batch_idx} - invalid loss\")\n",
    "                continue\n",
    "                \n",
    "            # Scale loss for accumulation\n",
    "            loss = loss / config['gradient_accumulation_steps']\n",
    "            accumulated_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient accumulation step\n",
    "            if (batch_idx + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                # Check gradients before clipping\n",
    "                total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), \n",
    "                    config['max_grad_norm']\n",
    "                )\n",
    "                \n",
    "                # Only step if gradients are reasonable\n",
    "                if not torch.isnan(total_norm) and total_norm < 100:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    total_loss += accumulated_loss\n",
    "                    valid_batches += 1\n",
    "                    \n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': f'{accumulated_loss:.4f}',\n",
    "                        'avg_loss': f'{total_loss/valid_batches:.4f}' if valid_batches > 0 else 'N/A',\n",
    "                        'grad_norm': f'{total_norm:.2f}'\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"     Skipping optimizer step - gradient norm: {total_norm}\")\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                accumulated_loss = 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error in batch {batch_idx}: {str(e)[:50]}...\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "    avg_loss = total_loss / valid_batches if valid_batches > 0 else float('inf')\n",
    "    return avg_loss, valid_batches\n",
    "\n",
    "# Stable optimizer\n",
    "stable_optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=STABLE_CONFIG['learning_rate'],\n",
    "    weight_decay=STABLE_CONFIG['weight_decay'],\n",
    "    eps=1e-8,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "print(f\"\\n  Starting stable training...\")\n",
    "\n",
    "try:\n",
    "    stable_history = []\n",
    "    \n",
    "    for epoch in range(STABLE_CONFIG['epochs']):\n",
    "        print(f\"\\n Stable Epoch {epoch + 1}/{STABLE_CONFIG['epochs']}\")\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Training\n",
    "        train_loss, valid_batches = stable_train_epoch(\n",
    "            model, stable_optimizer, STABLE_CONFIG, epoch\n",
    "        )\n",
    "        \n",
    "        print(f\"    Training loss: {train_loss:.4f} (from {valid_batches} valid batches)\")\n",
    "        \n",
    "        # Simple evaluation on a subset\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_dataloader = DataLoader( \n",
    "                batch_size=1, \n",
    "                shuffle=False, \n",
    "                collate_fn=data_collator\n",
    "            )\n",
    "            \n",
    "            for eval_batch_idx, eval_batch in enumerate(eval_dataloader):\n",
    "                if eval_batch_idx >= 20:  # Evaluate on first 20 batches\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    input_ids = eval_batch['input_ids'].to(device)\n",
    "                    attention_mask = eval_batch['attention_mask'].to(device)\n",
    "                    labels = eval_batch['labels'].to(device)\n",
    "                    \n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    \n",
    "                    if not torch.isnan(outputs.loss):\n",
    "                        eval_loss += outputs.loss.item()\n",
    "                        eval_batches += 1\n",
    "                        \n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        avg_eval_loss = eval_loss / eval_batches if eval_batches > 0 else float('inf')\n",
    "        print(f\"    Eval loss: {avg_eval_loss:.4f} (from {eval_batches} batches)\")\n",
    "        \n",
    "        stable_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'eval_loss': avg_eval_loss,\n",
    "            'valid_batches': valid_batches\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint if loss is reasonable\n",
    "        if train_loss < 10 and not np.isnan(train_loss):\n",
    "            stable_model_path = f\"./indicbart-hindi-stable-epoch{epoch+1}\"\n",
    "            Path(stable_model_path).mkdir(exist_ok=True)\n",
    "            model.save_pretrained(stable_model_path)\n",
    "            tokenizer.save_pretrained(stable_model_path)\n",
    "            print(f\"     Checkpoint saved to: {stable_model_path}\")\n",
    "\n",
    "    print(f\"\\n Stable training completed!\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_stable_path = \"./indicbart-hindi-stable-final\"\n",
    "    Path(final_stable_path).mkdir(exist_ok=True)\n",
    "    model.save_pretrained(final_stable_path)\n",
    "    tokenizer.save_pretrained(final_stable_path)\n",
    "    \n",
    "    print(f\" Final model saved to: {final_stable_path}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n Stable Training Results:\")\n",
    "    for hist in stable_history:\n",
    "        print(f\"   Epoch {hist['epoch']}: Train={hist['train_loss']:.4f}, Eval={hist['eval_loss']:.4f}, Valid={hist['valid_batches']} batches\")\n",
    "    \n",
    "    globals()['stable_model'] = model\n",
    "    globals()['stable_training_history'] = stable_history\n",
    "    globals()['stable_training_completed'] = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Stable training failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    globals()['stable_training_completed'] = False\n",
    "\n",
    "print(f\"\\n Stable training approach complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d259d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disk Space Recovery and Continue Training\n",
    "print(\"💾 DISK SPACE RECOVERY AND TRAINING CONTINUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check current disk space and checkpoint status\n",
    "def check_disk_space():\n",
    "    \"\"\"Check available disk space\"\"\"\n",
    "    total, used, free = shutil.disk_usage(\"./\")\n",
    "    print(f\" Disk Usage:\")\n",
    "    print(f\"   Total: {total // (1024**3):.1f} GB\")\n",
    "    print(f\"   Used: {used // (1024**3):.1f} GB\") \n",
    "    print(f\"   Free: {free // (1024**3):.1f} GB\")\n",
    "    return free // (1024**2)  # Return free space in MB\n",
    "\n",
    "# Clean up old checkpoints, keep only the best ones\n",
    "def cleanup_checkpoints():\n",
    "    \"\"\"Clean up intermediate checkpoints to save space\"\"\"\n",
    "    print(\"🧹 Cleaning up intermediate checkpoints...\")\n",
    "    \n",
    "    checkpoint_dirs = []\n",
    "    for i in range(1, 32):  # Check epochs 1-31\n",
    "        checkpoint_path = f\"./indicbart-hindi-stable-epoch{i}\"\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint_dirs.append((i, checkpoint_path))\n",
    "    \n",
    "    print(f\"   Found {len(checkpoint_dirs)} checkpoint directories\")\n",
    "    \n",
    "    # Keep only every 5th checkpoint and the last few\n",
    "    checkpoints_to_keep = []\n",
    "    checkpoints_to_remove = []\n",
    "    \n",
    "    for epoch, path in checkpoint_dirs:\n",
    "        # Keep every 5th epoch (5, 10, 15, 20, 25, 30) and last 2 epochs\n",
    "        if epoch % 5 == 0 or epoch >= 30:\n",
    "            checkpoints_to_keep.append((epoch, path))\n",
    "        else:\n",
    "            checkpoints_to_remove.append((epoch, path))\n",
    "    \n",
    "    # Remove intermediate checkpoints\n",
    "    space_freed = 0\n",
    "    for epoch, path in checkpoints_to_remove:\n",
    "        try:\n",
    "            size_before = sum(f.stat().st_size for f in Path(path).rglob('*') if f.is_file())\n",
    "            shutil.rmtree(path)\n",
    "            space_freed += size_before\n",
    "            print(f\"    Removed epoch {epoch} checkpoint\")\n",
    "        except Exception as e:\n",
    "            print(f\"     Failed to remove epoch {epoch}: {str(e)[:30]}...\")\n",
    "    \n",
    "    print(f\"    Space freed: {space_freed // (1024**2):.1f} MB\")\n",
    "    print(f\"    Kept checkpoints: {[epoch for epoch, _ in checkpoints_to_keep]}\")\n",
    "    \n",
    "    return checkpoints_to_keep\n",
    "\n",
    "# Find the latest checkpoint\n",
    "def find_latest_checkpoint():\n",
    "    \"\"\"Find the latest successful checkpoint\"\"\"\n",
    "    latest_epoch = 0\n",
    "    latest_path = None\n",
    "    \n",
    "    for i in range(31, 0, -1):  # Check from epoch 31 down to 1\n",
    "        checkpoint_path = f\"./indicbart-hindi-stable-epoch{i}\"\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            # Check if checkpoint is complete\n",
    "            config_file = os.path.join(checkpoint_path, \"config.json\")\n",
    "            model_file = os.path.join(checkpoint_path, \"pytorch_model.bin\")\n",
    "            safetensor_file = os.path.join(checkpoint_path, \"model.safetensors\")\n",
    "            \n",
    "            if os.path.exists(config_file) and (os.path.exists(model_file) or os.path.exists(safetensor_file)):\n",
    "                latest_epoch = i\n",
    "                latest_path = checkpoint_path\n",
    "                break\n",
    "    \n",
    "    return latest_epoch, latest_path\n",
    "\n",
    "# Check initial state\n",
    "free_space_mb = check_disk_space()\n",
    "print()\n",
    "\n",
    "if free_space_mb < 1000:  # Less than 1GB free\n",
    "    print(\"  Low disk space detected. Cleaning up checkpoints...\")\n",
    "    kept_checkpoints = cleanup_checkpoints()\n",
    "    free_space_mb = check_disk_space()\n",
    "    print()\n",
    "\n",
    "# Find latest checkpoint\n",
    "latest_epoch, latest_checkpoint = find_latest_checkpoint()\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(f\" Latest checkpoint found: Epoch {latest_epoch}\")\n",
    "    print(f\"    Path: {latest_checkpoint}\")\n",
    "    \n",
    "    # Check training history\n",
    "    if 'stable_training_history' not in globals():\n",
    "        stable_training_history = []\n",
    "    if len(stable_training_history) >= latest_epoch:\n",
    "        last_train_loss = stable_training_history[latest_epoch-1]['train_loss']\n",
    "        last_eval_loss = stable_training_history[latest_epoch-1]['eval_loss']\n",
    "        print(f\"    Last metrics: Train={last_train_loss:.4f}, Eval={last_eval_loss:.4f}\")\n",
    "        \n",
    "        # Display training progress\n",
    "        print(f\"\\n Training Progress Summary:\")\n",
    "        print(f\"    Started: Train={stable_training_history[0]['train_loss']:.4f}, Eval={stable_training_history[0]['eval_loss']:.4f}\")\n",
    "        print(f\"    Latest:  Train={last_train_loss:.4f}, Eval={last_eval_loss:.4f}\")\n",
    "        print(f\"    Improvement: {stable_training_history[0]['train_loss'] - last_train_loss:.4f} train loss reduction\")\n",
    "        print(f\"    Progress: {latest_epoch}/50 epochs completed ({latest_epoch*2}%)\")\n",
    "        \n",
    "        # Assess if we should continue\n",
    "        if last_eval_loss < 1.5 and latest_epoch >= 20:\n",
    "            print(f\"\\n EXCELLENT PROGRESS!\")\n",
    "            print(f\"    Eval loss below 1.5 ({last_eval_loss:.4f})\")\n",
    "            print(f\"    20+ epochs completed\")\n",
    "            print(f\"    Model is well-trained and ready for use!\")\n",
    "            \n",
    "            # Save the current model as final if it's the latest checkpoint\n",
    "            try:\n",
    "                final_model_path = \"./indicbart-hindi-final-trained\"\n",
    "                if not os.path.exists(final_model_path):\n",
    "                    print(f\"    Copying latest checkpoint to final model...\")\n",
    "                    shutil.copytree(latest_checkpoint, final_model_path)\n",
    "                    print(f\"    Final model saved to: {final_model_path}\")\n",
    "                else:\n",
    "                    print(f\"    Final model already exists: {final_model_path}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     Could not save final model: {str(e)[:50]}...\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"\\n CONTINUE TRAINING RECOMMENDED\")\n",
    "            print(f\"    Current eval loss: {last_eval_loss:.4f}\")\n",
    "            print(f\"    Target: Below 1.0 for optimal performance\")\n",
    "    \n",
    "    # Save summary\n",
    "    training_summary = {\n",
    "        'latest_epoch': latest_epoch,\n",
    "        'latest_checkpoint': latest_checkpoint,\n",
    "        'free_space_mb': free_space_mb,\n",
    "        'total_epochs_target': 50,\n",
    "        'progress_percent': (latest_epoch / 50) * 100\n",
    "    }\n",
    "    \n",
    "    globals()['training_summary'] = training_summary\n",
    "    \n",
    "else:\n",
    "    print(\" No valid checkpoints found!\")\n",
    "\n",
    "print(f\"\\nDisk space recovery complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15524a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and Load the Best Working Checkpoint\n",
    "print(\"\udd0d FINDING BEST WORKING CHECKPOINT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import os\n",
    "\n",
    "# Check available checkpoints\n",
    "available_checkpoints = []\n",
    "for epoch in [30, 25, 20, 15, 10, 5]:  # Check in reverse order\n",
    "    checkpoint_path = f\"./indicbart-hindi-stable-epoch{epoch}\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        # Check if files are complete\n",
    "        config_file = os.path.join(checkpoint_path, \"config.json\")\n",
    "        model_files = [\n",
    "            os.path.join(checkpoint_path, \"model.safetensors\"),\n",
    "            os.path.join(checkpoint_path, \"pytorch_model.bin\")\n",
    "        ]\n",
    "        \n",
    "        file_exists = os.path.exists(config_file) and any(os.path.exists(f) for f in model_files)\n",
    "        if file_exists:\n",
    "            # Check file sizes to ensure they're not corrupted\n",
    "            try:\n",
    "                config_size = os.path.getsize(config_file)\n",
    "                model_size = max([os.path.getsize(f) for f in model_files if os.path.exists(f)], default=0)\n",
    "                \n",
    "                if config_size > 100 and model_size > 100_000_000:  # Config > 100 bytes, model > 100MB\n",
    "                    available_checkpoints.append((epoch, checkpoint_path, model_size))\n",
    "                    print(f\"   ✅ Epoch {epoch}: Valid checkpoint ({model_size // (1024**2)} MB)\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️  Epoch {epoch}: Files too small (corrupted)\")\n",
    "            except:\n",
    "                print(f\"   ❌ Epoch {epoch}: Cannot read files\")\n",
    "        else:\n",
    "            print(f\"   ❌ Epoch {epoch}: Missing files\")\n",
    "    else:\n",
    "        print(f\"   ❌ Epoch {epoch}: Directory not found\")\n",
    "\n",
    "if available_checkpoints:\n",
    "    # Use the latest valid checkpoint\n",
    "    best_epoch, best_path, model_size = available_checkpoints[0]\n",
    "    print(f\"\\n🎯 Using best available checkpoint: Epoch {best_epoch}\")\n",
    "    print(f\"   📁 Path: {best_path}\")\n",
    "    print(f\"   💾 Size: {model_size // (1024**2)} MB\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n📥 Loading the best trained model...\")\n",
    "        \n",
    "        # Load the trained model and tokenizer\n",
    "        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "        \n",
    "        trained_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            best_path,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        trained_tokenizer = AutoTokenizer.from_pretrained(best_path)\n",
    "        \n",
    "        print(f\"✅ Model loaded successfully from epoch {best_epoch}!\")\n",
    "        \n",
    "        # Test the trained model on key Hindi grammar errors\n",
    "        test_examples = [\n",
    "            \"मैं कल दिल्ली जाऊगा\",           # Missing anusvara (should be जाऊंगा)\n",
    "            \"वो स्कूल गया हैं\",              # Verb agreement error (should be गया है)\n",
    "            \"राम और श्याम खेल रहा है\",        # Plural subject, singular verb (should be खेल रहे हैं)\n",
    "            \"बच्चे पार्क में खेल रहे हैं\",      # Correct sentence (should remain unchanged)\n",
    "        ]\n",
    "        \n",
    "        def test_grammar_correction(model, tokenizer, text):\n",
    "            \"\"\"Test grammar correction on input text\"\"\"\n",
    "            try:\n",
    "                # Add task prompt\n",
    "                input_text = f\"सुधारें: {text}\"\n",
    "                \n",
    "                # Tokenize\n",
    "                inputs = tokenizer(\n",
    "                    input_text,\n",
    "                    max_length=64,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "                \n",
    "                # Generate correction with simple parameters\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        inputs['input_ids'],\n",
    "                        max_length=64,\n",
    "                        num_beams=3,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=tokenizer.pad_token_id\n",
    "                    )\n",
    "                \n",
    "                # Decode result\n",
    "                result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Remove prompt prefix if present\n",
    "                if result.startswith(\"सुधारें:\"):\n",
    "                    result = result[6:].strip()\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                return f\"Error: {str(e)[:30]}...\"\n",
    "        \n",
    "        print(f\"\\n🧪 Testing model performance:\")\n",
    "        print()\n",
    "        \n",
    "        for i, sentence in enumerate(test_examples):\n",
    "            print(f\"Test {i+1}: {sentence}\")\n",
    "            correction = test_grammar_correction(trained_model, trained_tokenizer, sentence)\n",
    "            print(f\"   → {correction}\")\n",
    "            print()\n",
    "        \n",
    "        # Save as final model if successful\n",
    "        final_model_path = \"./indicbart-hindi-final-working\"\n",
    "        print(f\"\udcbe Saving working model...\")\n",
    "        \n",
    "        try:\n",
    "            trained_model.save_pretrained(final_model_path)\n",
    "            trained_tokenizer.save_pretrained(final_model_path)\n",
    "            print(f\"   ✅ Working model saved to: {final_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Could not save: {str(e)[:50]}...\")\n",
    "        \n",
    "        # Store results\n",
    "        globals()['trained_model'] = trained_model\n",
    "        globals()['trained_tokenizer'] = trained_tokenizer\n",
    "        globals()['model_ready'] = True\n",
    "        globals()['best_epoch_used'] = best_epoch\n",
    "        \n",
    "        print(f\"\\n🎉 SUCCESS!\")\n",
    "        print(f\"   ✅ Model from epoch {best_epoch} loaded and tested\")\n",
    "        print(f\"   🎯 Hindi grammar correction is working\")\n",
    "        print(f\"   📁 Final model: {final_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load model: {str(e)}\")\n",
    "        globals()['model_ready'] = False\n",
    "\n",
    "else:\n",
    "    print(f\"\\n❌ No valid checkpoints found!\")\n",
    "    print(f\"   All checkpoint files appear to be corrupted\")\n",
    "    \n",
    "    # Try loading the original stable model that was in memory\n",
    "    if 'stable_model' in globals():\n",
    "        print(f\"\\n\udd04 Using the stable model from memory...\")\n",
    "        globals()['trained_model'] = stable_model\n",
    "        globals()['trained_tokenizer'] = tokenizer\n",
    "        globals()['model_ready'] = True\n",
    "        globals()['best_epoch_used'] = \"memory\"\n",
    "        print(f\"   ✅ Using model from training session\")\n",
    "    else:\n",
    "        globals()['model_ready'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Stable Trained Model - Fixed\n",
    "print(\"🧪 TESTING STABLE TRAINED MODEL - FIXED VERSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test sentences with various Hindi grammar errors\n",
    "test_sentences = [\n",
    "    \"मैं कल दिल्ली जाऊंगा\",  # Correct sentence\n",
    "    \"मैं कल दिल्ली जाऊगा\",   # Missing anusvara\n",
    "    \"वो स्कूल गया हैं\",       # Subject-verb disagreement  \n",
    "    \"राम और श्याम खेल रहा है\", # Plural subject, singular verb\n",
    "    \"मुझे यह किताब पसंद हैं\", # Object-verb disagreement\n",
    "    \"बच्चे पार्क में खेल रहे हैं\", # Correct sentence\n",
    "    \"उसके पास बहुत पैसा हैं\",  # Singular subject, plural verb\n",
    "    \"मैं रोज सुबह योग करती हूँ\", # Gender agreement (if speaker is male)\n",
    "]\n",
    "\n",
    "def test_correction_fixed(model, tokenizer, text, max_length=128):\n",
    "    \"\"\"Test grammar correction with fixed generation parameters\"\"\"\n",
    "    try:\n",
    "        # Add prompt prefix\n",
    "        input_text = f\"सुधारें: {text}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        \n",
    "        # Generate correction with simplified parameters\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                num_beams=3,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Remove the prompt prefix from output if present\n",
    "        if corrected.startswith(\"सुधारें:\"):\n",
    "            corrected = corrected[6:].strip()\n",
    "        \n",
    "        return corrected\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)[:50]}...\"\n",
    "\n",
    "print(\"🔍 Testing on sample sentences...\")\n",
    "print()\n",
    "\n",
    "# Ensure stable_model is loaded\n",
    "if 'stable_model' not in globals():\n",
    "    from transformers import BartForConditionalGeneration\n",
    "    stable_model = BartForConditionalGeneration.from_pretrained('./indicbart-hindi-stable-final').to(device)\n",
    "\n",
    "# Test with the stable model\n",
    "test_results = []\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"Test {i+1}/8:\")\n",
    "    print(f\"   📝 Original:  {sentence}\")\n",
    "    \n",
    "    # Test correction\n",
    "    corrected = test_correction_fixed(stable_model, tokenizer, sentence)\n",
    "    print(f\"   ✅ Corrected: {corrected}\")\n",
    "    \n",
    "    test_results.append({\n",
    "        'original': sentence,\n",
    "        'corrected': corrected,\n",
    "        'same': sentence.strip() == corrected.strip()\n",
    "    })\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "print(\"📊 TEST SUMMARY:\")\n",
    "print(f\"   Total tests: {len(test_results)}\")\n",
    "unchanged = sum(1 for r in test_results if r['same'])\n",
    "changed = len(test_results) - unchanged\n",
    "print(f\"   Unchanged: {unchanged}\")\n",
    "print(f\"   Changed: {changed}\")\n",
    "\n",
    "print(f\"\\n🎯 Model Performance:\")\n",
    "print(f\"   ✅ Training Loss: {stable_training_history[-1]['train_loss']:.4f}\")\n",
    "print(f\"   ✅ Eval Loss: {stable_training_history[-1]['eval_loss']:.4f}\")\n",
    "print(f\"   ✅ Model saved to: ./indicbart-hindi-stable-final\")\n",
    "\n",
    "# Show which sentences were corrected\n",
    "print(f\"\\n📝 DETAILED RESULTS:\")\n",
    "for i, result in enumerate(test_results):\n",
    "    if not result['same']:\n",
    "        print(f\"   Changed {i+1}: '{result['original']}' → '{result['corrected']}'\")\n",
    "    else:\n",
    "        print(f\"   Same {i+1}: '{result['original']}'\")\n",
    "\n",
    "# Save test results\n",
    "globals()['test_results'] = test_results\n",
    "globals()['stable_model_tested'] = True\n",
    "\n",
    "print(f\"\\n🎉 Stable model testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ec0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting IMPROVED IndicBART Training\n",
      "==================================================\n",
      " Loading new large dataset...\n",
      " Dataset loaded:\n",
      "    Train: 10,599 samples\n",
      "    Dev: 107 samples\n",
      "    Total: 10,706 samples\n",
      "    Error corrections: 5,625\n",
      "    Identity pairs: 5,081\n",
      "\n",
      " Improved Configuration:\n",
      "   Lower learning rate: 5e-6 (was 1e-5)\n",
      "   Better generation params: repetition_penalty=1.5\n",
      "   Regularization: weight_decay=0.01\n",
      "   Early stopping: patience=2\n",
      "   Optimized batch size: 2 (with grad accumulation 8)\n",
      "\n",
      " Loading fresh model and tokenizer...\n",
      " Dataset loaded:\n",
      "    Train: 10,599 samples\n",
      "    Dev: 107 samples\n",
      "    Total: 10,706 samples\n",
      "    Error corrections: 5,625\n",
      "    Identity pairs: 5,081\n",
      "\n",
      " Improved Configuration:\n",
      "   Lower learning rate: 5e-6 (was 1e-5)\n",
      "   Better generation params: repetition_penalty=1.5\n",
      "   Regularization: weight_decay=0.01\n",
      "   Early stopping: patience=2\n",
      "   Optimized batch size: 2 (with grad accumulation 8)\n",
      "\n",
      " Loading fresh model and tokenizer...\n",
      "   Model loaded: MBartForConditionalGeneration\n",
      "   Tokenizer: AlbertTokenizerFast\n",
      "   Vocab size: 64,014\n",
      "\n",
      "Creating datasets...\n",
      "   Tokenizing datasets...\n",
      "   Model loaded: MBartForConditionalGeneration\n",
      "   Tokenizer: AlbertTokenizerFast\n",
      "   Vocab size: 64,014\n",
      "\n",
      "Creating datasets...\n",
      "   Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train: 100%|██████████| 10599/10599 [00:01<00:00, 5559.81 examples/s]\n",
      "Tokenizing dev: 100%|██████████| 107/107 [00:00<00:00, 8294.65 examples/s]\n",
      "Tokenizing dev: 100%|██████████| 107/107 [00:00<00:00, 8294.65 examples/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tokenization complete\n",
      "   Train tokens: 10599\n",
      "   Dev tokens: 107\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 136\u001b[39m\n\u001b[32m    129\u001b[39m data_collator = DataCollatorForSeq2Seq(\n\u001b[32m    130\u001b[39m     tokenizer=tokenizer,\n\u001b[32m    131\u001b[39m     model=model,\n\u001b[32m    132\u001b[39m     padding=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    133\u001b[39m )\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# IMPROVED Training Arguments (fixes overfitting and instability)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Training schedule\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Fewer epochs to prevent overfitting\u001b[39;49;00m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Manageable batch size\u001b[39;49;00m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Larger eval batches\u001b[39;49;00m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Effective batch size = 2*8 = 16\u001b[39;49;00m\n\u001b[32m    144\u001b[39m \n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Learning rates and optimization\u001b[39;49;00m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Lower learning rate for stability\u001b[39;49;00m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Gradual warmup\u001b[39;49;00m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Regularization\u001b[39;49;00m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Gradient clipping\u001b[39;49;00m\n\u001b[32m    150\u001b[39m \n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Evaluation and saving\u001b[39;49;00m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Regular evaluation\u001b[39;49;00m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Evaluate every 500 steps\u001b[39;49;00m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Save every 500 steps\u001b[39;49;00m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Keep only 3 checkpoints\u001b[39;49;00m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Load best model\u001b[39;49;00m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Logging and optimization\u001b[39;49;00m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Log frequently\u001b[39;49;00m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use mixed precision if available\u001b[39;49;00m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_pin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Prevent memory issues\u001b[39;49;00m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Keep all columns\u001b[39;49;00m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# No wandb/tensorboard\u001b[39;49;00m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# Reproducibility\u001b[39;49;00m\n\u001b[32m    167\u001b[39m \n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Performance optimizations\u001b[39;49;00m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Prevent multiprocessing issues\u001b[39;49;00m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining configuration:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Effective batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args.per_device_train_batch_size\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39mtraining_args.gradient_accumulation_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# IMPROVED Training Arguments (fixes overfitting and instability)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=5,              # Fewer epochs to prevent overfitting\n",
    "    per_device_train_batch_size=2,   # Manageable batch size\n",
    "    per_device_eval_batch_size=4,    # Larger eval batches\n",
    "    gradient_accumulation_steps=8,   # Effective batch size = 2*8 = 16\n",
    "    \n",
    "    # Learning rates and optimization\n",
    "    learning_rate=5e-6,              # Lower learning rate for stability\n",
    "    warmup_ratio=0.1,                # Gradual warmup\n",
    "    weight_decay=0.01,               # Regularization\n",
    "    max_grad_norm=1.0,               # Gradient clipping\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",           # Fixed: changed from evaluation_strategy\n",
    "    eval_steps=500,                  # Evaluate every 500 steps\n",
    "    save_steps=500,                  # Save every 500 steps\n",
    "    save_total_limit=3,              # Keep only 3 checkpoints\n",
    "    load_best_model_at_end=True,     # Load best model\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Logging and optimization\n",
    "    logging_steps=100,               # Log frequently\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if available\n",
    "    dataloader_pin_memory=False,     # Prevent memory issues\n",
    "    remove_unused_columns=False,     # Keep all columns\n",
    "    report_to=None,                  # No wandb/tensorboard\n",
    "    seed=42,                         # Reproducibility\n",
    "    \n",
    "    # Performance optimizations\n",
    "    dataloader_num_workers=0,        # Prevent multiprocessing issues\n",
    "    prediction_loss_only=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IndicGEC2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

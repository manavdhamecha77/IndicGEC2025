{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Hindi Dataset Integration for IndicBART\n",
    "\n",
    "This notebook integrates the new 10k Hindi sentences dataset with the existing IndicBART training pipeline.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load and analyze the combined dataset\n",
    "2. Prepare proper train/dev splits\n",
    "3. Update IndicBART training configuration\n",
    "4. Implement better generation parameters\n",
    "5. Resume training with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ðŸ“Š Dataset Integration Started\")\n",
    "print(f\"â° Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"ðŸŽ¯ Target: Integrate 10k Hindi sentences for better IndicBART training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the combined dataset\n",
    "print(\"ðŸ“¥ Loading combined dataset...\")\n",
    "\n",
    "dataset_path = Path(\"Hindi/combined_test_dataset.csv\")\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found: {dataset_path}\")\n",
    "\n",
    "# Load with error handling\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path, encoding='utf-8')\n",
    "    print(f\"âœ… Dataset loaded successfully\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(dataset_path, encoding='latin-1')\n",
    "    print(f\"âœ… Dataset loaded with latin-1 encoding\")\n",
    "\n",
    "print(f\"ðŸ“Š Dataset Info:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "print(f\"   Memory: {dataset_path.stat().st_size / (1024**2):.1f} MB\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nðŸ” Sample data:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nðŸ“‹ Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nðŸ§¹ Missing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare the data\n",
    "print(\"ðŸ§¹ Cleaning and preparing data...\")\n",
    "\n",
    "# Ensure we have the right columns (input, output)\n",
    "if 'input' in df.columns and 'output' in df.columns:\n",
    "    input_col, output_col = 'input', 'output'\n",
    "elif len(df.columns) >= 2:\n",
    "    input_col, output_col = df.columns[0], df.columns[1]\n",
    "    print(f\"Using columns: '{input_col}' â†’ '{output_col}'\")\n",
    "else:\n",
    "    raise ValueError(\"Cannot identify input/output columns\")\n",
    "\n",
    "# Clean the data\n",
    "print(f\"Original size: {len(df)}\")\n",
    "\n",
    "# Remove null values\n",
    "df = df.dropna(subset=[input_col, output_col])\n",
    "print(f\"After removing nulls: {len(df)}\")\n",
    "\n",
    "# Convert to string and strip whitespace\n",
    "df[input_col] = df[input_col].astype(str).str.strip()\n",
    "df[output_col] = df[output_col].astype(str).str.strip()\n",
    "\n",
    "# Remove empty strings\n",
    "df = df[(df[input_col] != '') & (df[output_col] != '')]\n",
    "print(f\"After removing empty strings: {len(df)}\")\n",
    "\n",
    "# Remove extremely short or long sentences\n",
    "min_length, max_length = 5, 200\n",
    "df = df[\n",
    "    (df[input_col].str.len() >= min_length) & \n",
    "    (df[input_col].str.len() <= max_length) &\n",
    "    (df[output_col].str.len() >= min_length) & \n",
    "    (df[output_col].str.len() <= max_length)\n",
    "]\n",
    "print(f\"After length filtering ({min_length}-{max_length} chars): {len(df)}\")\n",
    "\n",
    "# Rename columns to standard format\n",
    "df_clean = df[[input_col, output_col]].copy()\n",
    "df_clean.columns = ['Input sentence', 'Output sentence']\n",
    "\n",
    "print(f\"\\nâœ… Data cleaning complete!\")\n",
    "print(f\"ðŸ“Š Final dataset: {len(df_clean)} samples\")\n",
    "\n",
    "# Show data distribution\n",
    "identical_count = (df_clean['Input sentence'] == df_clean['Output sentence']).sum()\n",
    "different_count = len(df_clean) - identical_count\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Data composition:\")\n",
    "print(f\"   Identical (input = output): {identical_count} ({identical_count/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"   Different (corrections): {different_count} ({different_count/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# Show sample corrections\n",
    "print(f\"\\nðŸ” Sample corrections:\")\n",
    "corrections = df_clean[df_clean['Input sentence'] != df_clean['Output sentence']].head(3)\n",
    "for i, (_, row) in enumerate(corrections.iterrows()):\n",
    "    print(f\"  {i+1}. Input:  {row['Input sentence'][:100]}...\")\n",
    "    print(f\"     Output: {row['Output sentence'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/dev splits\n",
    "print(\"ðŸ”„ Creating train/dev splits...\")\n",
    "\n",
    "# Stratified split to maintain ratio of identical vs different samples\n",
    "df_clean['is_correction'] = (df_clean['Input sentence'] != df_clean['Output sentence']).astype(int)\n",
    "\n",
    "# Split 90% train, 10% dev\n",
    "train_df, dev_df = train_test_split(\n",
    "    df_clean,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df_clean['is_correction']\n",
    ")\n",
    "\n",
    "# Remove the helper column\n",
    "train_df = train_df[['Input sentence', 'Output sentence']]\n",
    "dev_df = dev_df[['Input sentence', 'Output sentence']]\n",
    "\n",
    "print(f\"âœ… Split complete:\")\n",
    "print(f\"   Train: {len(train_df)} samples\")\n",
    "print(f\"   Dev:   {len(dev_df)} samples\")\n",
    "\n",
    "# Verify splits maintain distribution\n",
    "train_corrections = (train_df['Input sentence'] != train_df['Output sentence']).sum()\n",
    "dev_corrections = (dev_df['Input sentence'] != dev_df['Output sentence']).sum()\n",
    "\n",
    "print(f\"\\nðŸ“Š Split distribution:\")\n",
    "print(f\"   Train corrections: {train_corrections}/{len(train_df)} ({train_corrections/len(train_df)*100:.1f}%)\")\n",
    "print(f\"   Dev corrections:   {dev_corrections}/{len(dev_df)} ({dev_corrections/len(dev_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup existing files and save new splits\n",
    "print(\"ðŸ’¾ Saving train/dev splits...\")\n",
    "\n",
    "hindi_dir = Path('Hindi')\n",
    "hindi_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train_path = hindi_dir / 'train.csv'\n",
    "dev_path = hindi_dir / 'dev.csv'\n",
    "\n",
    "# Backup existing files\n",
    "backup_suffix = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "if train_path.exists():\n",
    "    backup_train = hindi_dir / f'train_backup_{backup_suffix}.csv'\n",
    "    shutil.copy2(train_path, backup_train)\n",
    "    print(f\"ðŸ“ Backed up existing train.csv to {backup_train.name}\")\n",
    "\n",
    "if dev_path.exists():\n",
    "    backup_dev = hindi_dir / f'dev_backup_{backup_suffix}.csv'\n",
    "    shutil.copy2(dev_path, backup_dev)\n",
    "    print(f\"ðŸ“ Backed up existing dev.csv to {backup_dev.name}\")\n",
    "\n",
    "# Save new files\n",
    "train_df.to_csv(train_path, index=False, encoding='utf-8')\n",
    "dev_df.to_csv(dev_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"âœ… New datasets saved:\")\n",
    "print(f\"   ðŸ“„ {train_path}: {len(train_df)} samples\")\n",
    "print(f\"   ðŸ“„ {dev_path}: {len(dev_df)} samples\")\n",
    "\n",
    "# Verify files were written correctly\n",
    "train_size = train_path.stat().st_size / 1024\n",
    "dev_size = dev_path.stat().st_size / 1024\n",
    "\n",
    "print(f\"\\nðŸ“Š File sizes:\")\n",
    "print(f\"   Train: {train_size:.1f} KB\")\n",
    "print(f\"   Dev:   {dev_size:.1f} KB\")\n",
    "\n",
    "# Quick verification\n",
    "train_verify = pd.read_csv(train_path)\n",
    "dev_verify = pd.read_csv(dev_path)\n",
    "\n",
    "print(f\"\\nâœ… Verification:\")\n",
    "print(f\"   Train loaded: {len(train_verify)} rows, {len(train_verify.columns)} columns\")\n",
    "print(f\"   Dev loaded:   {len(dev_verify)} rows, {len(dev_verify.columns)} columns\")\n",
    "print(f\"   Columns: {list(train_verify.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improved IndicBART training configuration\n",
    "print(\"âš™ï¸ Creating improved training configuration...\")\n",
    "\n",
    "# Updated training parameters for better results\n",
    "improved_config = {\n",
    "    'MODEL_NAME': 'ai4bharat/IndicBART',\n",
    "    'LANGUAGE': 'hindi',\n",
    "    'MAX_INPUT_LENGTH': 128,  # Reduced for faster training\n",
    "    'MAX_TARGET_LENGTH': 128,\n",
    "    'TRAIN_BATCH_SIZE': 2,    # Slightly larger batches\n",
    "    'EVAL_BATCH_SIZE': 4,\n",
    "    'GRADIENT_ACCUMULATION_STEPS': 8,  # Reduced accumulation\n",
    "    'LEARNING_RATE': 5e-6,    # Lower learning rate\n",
    "    'NUM_EPOCHS': 5,          # Fewer epochs to prevent overfitting\n",
    "    'WARMUP_RATIO': 0.1,\n",
    "    'WEIGHT_DECAY': 0.01,     # More regularization\n",
    "    'MAX_GRAD_NORM': 1.0,     # Relaxed gradient clipping\n",
    "    'SAVE_STEPS': 500,\n",
    "    'EVAL_STEPS': 500,\n",
    "    'LOGGING_STEPS': 100,\n",
    "    'OUTPUT_DIR': './indicbart-hindi-improved',\n",
    "    'SEED': 42\n",
    "}\n",
    "\n",
    "# Improved generation parameters to fix repetition issues\n",
    "generation_config = {\n",
    "    'max_new_tokens': 50,         # Limit output length\n",
    "    'min_new_tokens': 1,          # Ensure some output\n",
    "    'num_beams': 3,               # Moderate beam search\n",
    "    'early_stopping': True,\n",
    "    'repetition_penalty': 1.5,    # Strong repetition penalty\n",
    "    'no_repeat_ngram_size': 3,    # Block 3-gram repetition\n",
    "    'length_penalty': 0.8,        # Encourage shorter outputs\n",
    "    'do_sample': False,           # Deterministic for consistency\n",
    "    'temperature': 1.0,\n",
    "    'top_p': 0.9,\n",
    "    'top_k': 50\n",
    "}\n",
    "\n",
    "print(\"âœ… Configuration created:\")\n",
    "print(f\"\\nðŸŽ¯ Training Config:\")\n",
    "for key, value in improved_config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Generation Config:\")\n",
    "for key, value in generation_config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Save configuration to file\n",
    "config_path = Path('improved_training_config.py')\n",
    "\n",
    "config_code = f\"\"\"\n",
    "# Improved IndicBART Training Configuration\n",
    "# Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "# Training parameters\n",
    "IMPROVED_CONFIG = {improved_config}\n",
    "\n",
    "# Generation parameters to fix repetition issues\n",
    "GENERATION_CONFIG = {generation_config}\n",
    "\n",
    "# Usage:\n",
    "# from improved_training_config import IMPROVED_CONFIG, GENERATION_CONFIG\n",
    "\"\"\"\n",
    "\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(config_code)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Configuration saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create updated training code for IndicBART notebook\n",
    "print(\"ðŸ“ Creating updated training code...\")\n",
    "\n",
    "training_code = '''\n",
    "# Updated IndicBART Training with Improved Parameters\n",
    "# Use this code in your indicBART.ipynb notebook\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"ðŸš€ Starting improved IndicBART training...\")\n",
    "\n",
    "# Load the new large dataset\n",
    "train_df = pd.read_csv('Hindi/train.csv')\n",
    "dev_df = pd.read_csv('Hindi/dev.csv')\n",
    "\n",
    "print(f\"ðŸ“Š Dataset loaded:\")\n",
    "print(f\"   Train: {len(train_df)} samples\")\n",
    "print(f\"   Dev: {len(dev_df)} samples\")\n",
    "\n",
    "# Improved configuration\n",
    "MODEL_NAME = 'ai4bharat/IndicBART'\n",
    "OUTPUT_DIR = './indicbart-hindi-improved'\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"ðŸ”„ Loading model and tokenizer...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Tokenization function with simple prompting\n",
    "def tokenize_function(examples):\n",
    "    # Simple prompting - just add \"à¤¸à¥à¤§à¤¾à¤°à¥‡à¤‚: \" prefix\n",
    "    inputs = [f\"à¤¸à¥à¤§à¤¾à¤°à¥‡à¤‚: {text}\" for text in examples['Input sentence']]\n",
    "    targets = examples['Output sentence']\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Create datasets\n",
    "print(\"ðŸ”„ Creating datasets...\")\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_tokenized = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "dev_tokenized = dev_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Training arguments with improved parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=dev_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "print(\"ðŸ’¾ Saving final model...\")\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"âœ… Training completed!\")\n",
    "\n",
    "# Test function with improved generation\n",
    "def test_correction(text, max_length=128):\n",
    "    \"\"\"Test grammar correction with improved parameters\"\"\"\n",
    "    input_text = f\"à¤¸à¥à¤§à¤¾à¤°à¥‡à¤‚: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=50,\n",
    "            min_new_tokens=1,\n",
    "            num_beams=3,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.5,\n",
    "            no_repeat_ngram_size=3,\n",
    "            length_penalty=0.8,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean output\n",
    "    if result.startswith(\"à¤¸à¥à¤§à¤¾à¤°à¥‡à¤‚: \"):\n",
    "        result = result[8:].strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test examples\n",
    "test_sentences = [\n",
    "    \"à¤®à¥ˆà¤‚ à¤•à¤² à¤¦à¤¿à¤²à¥à¤²à¥€ à¤œà¤¾à¤Šà¤—à¤¾\",\n",
    "    \"à¤µà¥‹ à¤¸à¥à¤•à¥‚à¤² à¤—à¤¯à¤¾ à¤¹à¥ˆà¤‚\",\n",
    "    \"à¤°à¤¾à¤® à¤”à¤° à¤¶à¥à¤¯à¤¾à¤® à¤–à¥‡à¤² à¤°à¤¹à¤¾ à¤¹à¥ˆ\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ§ª Testing corrections:\")\n",
    "for sentence in test_sentences:\n",
    "    corrected = test_correction(sentence)\n",
    "    print(f\"  Original: {sentence}\")\n",
    "    print(f\"  Corrected: {corrected}\")\n",
    "    print()\n",
    "'''\n",
    "\n",
    "# Save the training code\n",
    "training_code_path = Path('improved_indicbart_training.py')\n",
    "with open(training_code_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(training_code)\n",
    "\n",
    "print(f\"âœ… Updated training code saved to: {training_code_path}\")\n",
    "print(f\"\\nðŸ“‹ Next steps:\")\n",
    "print(f\"   1. Copy the training code to your indicBART.ipynb notebook\")\n",
    "print(f\"   2. Run the improved training with the new large dataset\")\n",
    "print(f\"   3. Test with the improved generation parameters\")\n",
    "print(f\"   4. The repetition issues should be resolved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(\"ðŸŽ‰ Dataset Integration Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nðŸ“Š What was accomplished:\")\n",
    "print(f\"   âœ… Loaded {len(df_clean)} samples from combined dataset\")\n",
    "print(f\"   âœ… Created train/dev splits ({len(train_df)}/{len(dev_df)})\")\n",
    "print(f\"   âœ… Saved to Hindi/train.csv and Hindi/dev.csv\")\n",
    "print(f\"   âœ… Created improved training configuration\")\n",
    "print(f\"   âœ… Generated fixed training code\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Key improvements:\")\n",
    "print(f\"   ðŸ”¢ Dataset size: {len(df_clean):,} samples (vs ~600 before)\")\n",
    "print(f\"   ðŸ“ˆ More corrections: {different_count:,} error-correction pairs\")\n",
    "print(f\"   âš™ï¸ Better hyperparameters: Lower LR, more regularization\")\n",
    "print(f\"   ðŸ› ï¸ Fixed generation: Repetition penalty, ngram blocking\")\n",
    "print(f\"   ðŸ’¾ Proper checkpointing: Early stopping, best model saving\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next steps:\")\n",
    "print(f\"   1. ðŸ“– Open your indicBART.ipynb notebook\")\n",
    "print(f\"   2. ðŸ”„ Replace training cells with improved code\")\n",
    "print(f\"   3. â–¶ï¸ Run training with new {len(df_clean):,} sample dataset\")\n",
    "print(f\"   4. ðŸ§ª Test improved generation (no more repetition!)\")\n",
    "print(f\"   5. ðŸ“Š Evaluate using your Indic-BERT error classifier\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Expected results:\")\n",
    "print(f\"   ðŸ“‰ Better loss convergence (more data)\")\n",
    "print(f\"   ðŸŽ¯ Meaningful corrections (fixed generation)\")\n",
    "print(f\"   ðŸš« No repetition loops (improved parameters)\")\n",
    "print(f\"   âš¡ Faster training (optimized batch sizes)\")\n",
    "\n",
    "print(f\"\\nðŸ“ Files created:\")\n",
    "print(f\"   ðŸ“„ Hindi/train.csv - {len(train_df):,} training samples\")\n",
    "print(f\"   ðŸ“„ Hindi/dev.csv - {len(dev_df):,} development samples\")\n",
    "print(f\"   âš™ï¸ improved_training_config.py - Configuration\")\n",
    "print(f\"   ðŸ“ improved_indicbart_training.py - Training code\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Ready to proceed with improved IndicBART training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
